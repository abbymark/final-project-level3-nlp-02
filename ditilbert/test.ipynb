{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertModel: ['distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at monologg/distilkobert and are newly initialized: ['encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertModel: ['distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at monologg/distilkobert and are newly initialized: ['encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.0.intermediate.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers.models.distilbert.configuration_distilbert import DistilBertConfig\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n",
    "from model_distilbert import DistilBertForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = DistilBertConfig.from_pretrained(\"monologg/distilkobert\")\n",
    "config.type_vocab_size=1\n",
    "config.layer_norm_eps= 1e-05\n",
    "config.hidden_dropout_prob= 0.1\n",
    "config.attention_probs_dropout_prob= 0.1\n",
    "config.intermediate_size= 3072\n",
    "config.hidden_act=\"gelu\"\n",
    "config.decoder_start_token_id = torch.tensor(2)\n",
    "config.use_cache = True\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"monologg/distilkobert\")\n",
    "model = DistilBertForConditionalGeneration(\"monologg/distilkobert\", config).to(device)\n",
    "\n",
    "text = '근대 경제학에서는 행위의 합리성 문제를 비용 대 편익이라는 효율성 측면에서 접근하는데, 이러한 사고방식은 경제학을 넘어 사회과학 전반으로 널리 확산되 었다. 하지만 경제적 논리로는 설명하기 어렵거나 바람직하지 않은 결과를 낳은 현상도 많이 존재한다. 특히 도덕이나 규범은 경제 현상의 작동 방식에도 영향을 미치기 때문에 이를 설명하는 확장된 경제 이론이 필요하다. 도덕경제론은 바로 이러한 문제를 다루는 접근 방식이다. 이에 따르면 경제 활동에서 개인의 효용 극대화와 합리적 선택을 강조하는 경제학의 기본 가정 역시 초시대적으로 통용 되는 가치가 아니라 자본주의 등장 이후에 형성된 역사적 현상에 불과하다. 경제 적 원칙은 당대의 도덕적 규범에 의해 구성되는 상대적인 가치이기 때문에, 경제 활동이나 영역과 관련된 규범적 측면을 고려해 재구성해야 한다. ‘도덕경제 (moral economy)’론은 바로 이처럼 경제 현상에서 규범이나 문화의 역할 문제 를 다루는 접근 방식이다. 이러한 시도는 경제와 관련된 사회 현상을 이해하는 데 에서 경제학적 접근 방식의 편협성과 한계를 해결하는 데에도 풍부한 시사점을 줄수있을것이다. 이논문은기존의도덕경제론에서다룬주요쟁점과개념을 소개하고, 이 논의가 미디어 경제를 이해하는 데 주는 함의, 쟁점 등을 검토한다.'\n",
    "\n",
    "input = tokenizer(text, return_tensors='pt')\n",
    "label = tokenizer('천하통일', return_tensors='pt')\n",
    "\n",
    "output = model(**input.to(device), labels=label['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(8.7542, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.6114, -0.3501,  1.4667,  ...,  0.2065, -0.3554,  0.7851],\n",
       "         [-0.0768,  0.5385,  0.9510,  ...,  0.5405, -1.0165,  0.5869],\n",
       "         [ 0.7476, -0.5816,  0.6950,  ...,  0.4870, -1.3124,  0.6337]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), past_key_values=((tensor([[[[ 0.4898,  0.2701, -0.2470,  ...,  1.1429, -0.2002, -0.3766],\n",
       "          [ 1.0107,  0.3930, -0.7696,  ...,  1.5234, -0.1480, -0.4280],\n",
       "          [ 0.8531,  0.3423, -0.1208,  ..., -0.0831,  0.8597, -0.3816]],\n",
       "\n",
       "         [[-0.2985,  0.8396,  0.4669,  ...,  1.1254,  0.5441,  0.9839],\n",
       "          [-0.1441,  1.1577,  0.1470,  ...,  0.9710, -0.4099,  0.9341],\n",
       "          [-0.5297,  1.7554,  0.1474,  ...,  0.6767, -0.0782,  0.4501]],\n",
       "\n",
       "         [[ 0.0322, -0.8545,  0.4244,  ..., -0.7264, -0.2966, -0.0507],\n",
       "          [ 0.0937, -0.7003,  0.3404,  ..., -0.9574,  0.0372, -0.2609],\n",
       "          [-0.1677, -0.4239,  0.3417,  ..., -0.1577, -0.2771, -0.4421]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3395,  0.1920,  0.3019,  ..., -0.1100, -0.1904,  0.4090],\n",
       "          [ 0.3545, -0.2275,  0.3310,  ..., -0.5166, -0.2473, -0.1240],\n",
       "          [ 0.0306,  0.2209, -0.1855,  ..., -0.9916, -0.2392,  0.2525]],\n",
       "\n",
       "         [[ 0.5294,  0.3396, -0.1500,  ..., -0.3786,  0.0976, -1.0885],\n",
       "          [ 0.4969,  0.6657, -0.3724,  ..., -0.8395,  0.5528, -0.7915],\n",
       "          [ 0.4636, -0.4547, -0.9363,  ..., -0.3585,  0.0660, -0.4702]],\n",
       "\n",
       "         [[ 0.5664,  0.6029,  0.8546,  ..., -0.7265, -0.3032,  0.1475],\n",
       "          [ 0.6026,  0.5611, -0.0255,  ...,  0.1584,  0.0495, -0.5496],\n",
       "          [ 0.9981,  0.4403,  0.2742,  ..., -0.5657,  0.8813, -0.6864]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-0.3753, -0.2617, -0.9536,  ..., -0.3874,  0.2655, -0.1162],\n",
       "          [-0.0996, -0.5872, -0.5841,  ..., -0.4356,  0.8158,  0.0317],\n",
       "          [-0.6229,  0.9675, -0.2028,  ..., -0.4198,  0.3440, -0.3390]],\n",
       "\n",
       "         [[ 0.1210,  0.1419,  0.9572,  ...,  0.8757, -0.5949,  0.6362],\n",
       "          [ 0.5093, -0.0571,  0.5035,  ...,  0.6276,  0.1494,  0.3560],\n",
       "          [ 1.6470,  1.3340,  0.4083,  ...,  0.7046,  0.3476,  0.8410]],\n",
       "\n",
       "         [[ 0.7199,  0.1730, -0.3194,  ..., -0.1452, -0.9080, -0.3052],\n",
       "          [ 0.2742, -0.0364, -0.8066,  ...,  0.1454, -0.0647,  0.3296],\n",
       "          [-0.0439,  1.2092, -0.3011,  ...,  0.7423,  0.0422,  0.1272]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.6493, -0.5173, -0.2445,  ..., -1.3196, -0.6816,  0.8930],\n",
       "          [ 0.2607, -0.0202,  0.2402,  ..., -1.3064, -0.3302,  0.6899],\n",
       "          [ 1.0541, -0.1623,  0.3325,  ..., -1.1356,  0.1425, -0.0576]],\n",
       "\n",
       "         [[ 0.3110, -0.1485, -0.2446,  ..., -0.0310, -0.4342, -0.0249],\n",
       "          [-0.1694, -0.3676, -0.0564,  ..., -0.0977, -0.1349, -0.0889],\n",
       "          [-0.2844,  0.9824, -0.5734,  ..., -0.1008, -0.1886, -0.5099]],\n",
       "\n",
       "         [[-0.6953, -0.8255,  0.3512,  ...,  0.6184, -0.2023,  0.4256],\n",
       "          [-0.4829,  0.0139,  0.4619,  ...,  0.5252,  0.4138,  1.1648],\n",
       "          [-0.6287,  0.3703,  0.8737,  ...,  0.2430,  0.1752,  0.7625]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 0.4486, -0.4551, -0.1606,  ...,  0.5104, -0.3440, -0.0894],\n",
       "          [-0.1755,  0.2282, -0.4177,  ..., -0.4302,  1.1373, -0.7691],\n",
       "          [-0.5306,  0.0088,  0.0124,  ..., -0.3704,  0.5211, -0.7957],\n",
       "          ...,\n",
       "          [-0.5439,  0.0962, -0.1932,  ..., -0.5805,  0.4796, -0.5463],\n",
       "          [ 0.0279, -0.1362, -0.2332,  ..., -0.1007,  0.2471,  0.0505],\n",
       "          [-0.7089, -0.0733, -0.0614,  ..., -0.3369,  0.0660,  0.2297]],\n",
       "\n",
       "         [[-0.1208, -0.4280, -0.5746,  ...,  0.3110, -0.2112,  0.2863],\n",
       "          [ 0.1157,  0.1840, -0.0160,  ...,  1.0100, -0.4365, -0.4841],\n",
       "          [-0.5145, -0.2265, -0.0685,  ...,  0.7958, -0.6006, -1.2568],\n",
       "          ...,\n",
       "          [-0.5840, -0.4086, -0.2027,  ...,  0.9410, -0.1593, -0.6563],\n",
       "          [-0.1731, -0.3433, -0.3119,  ...,  0.3123, -1.0525, -0.1807],\n",
       "          [-0.3545, -0.3367,  0.1784,  ...,  0.1126,  0.0873,  0.1479]],\n",
       "\n",
       "         [[ 0.7428,  0.3082, -0.1951,  ...,  0.5446,  0.2863,  0.2985],\n",
       "          [ 0.7807,  0.5611,  0.0328,  ...,  0.1124,  0.5230,  0.2050],\n",
       "          [ 0.4551,  0.6951, -0.2256,  ...,  0.4218,  0.5570,  0.3492],\n",
       "          ...,\n",
       "          [ 0.4458,  0.2478, -0.0456,  ...,  0.3434,  0.6568, -0.1102],\n",
       "          [-0.0333,  0.1924,  0.2433,  ..., -0.2260,  0.3703,  0.2810],\n",
       "          [ 0.1073,  0.8462, -0.8455,  ...,  0.3667,  0.5447,  0.5670]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2748, -0.2824,  0.4293,  ...,  0.1099,  0.7792,  0.2490],\n",
       "          [-0.0528,  0.3658, -0.3736,  ..., -0.5929,  0.5383,  0.4351],\n",
       "          [ 0.5346,  0.1922, -0.7115,  ..., -0.8911, -0.0085,  0.1285],\n",
       "          ...,\n",
       "          [ 0.2776, -0.0374, -0.3152,  ..., -0.7601,  0.6110, -0.4795],\n",
       "          [ 0.1931, -0.1299, -0.4209,  ...,  0.4511, -0.3304, -0.6701],\n",
       "          [ 0.9766,  0.3741, -0.3663,  ..., -0.3675,  0.7478, -0.1910]],\n",
       "\n",
       "         [[ 0.5257, -0.6533, -0.7088,  ...,  0.3014, -0.2113, -0.5711],\n",
       "          [-0.2071, -0.0339, -0.5377,  ...,  0.0601,  0.0694, -1.1197],\n",
       "          [ 0.0487, -0.1227,  0.0779,  ...,  0.3309,  0.3951, -1.4291],\n",
       "          ...,\n",
       "          [ 0.1598, -0.3298, -0.6057,  ...,  0.5518,  0.4001, -0.8132],\n",
       "          [-0.5493, -1.0838, -0.6882,  ...,  0.4486,  0.0451,  0.2549],\n",
       "          [ 0.4251, -0.0067, -0.7339,  ...,  0.7016, -0.8442, -0.5639]],\n",
       "\n",
       "         [[ 0.3608, -0.1375, -0.4990,  ...,  0.0924, -0.3949, -0.3394],\n",
       "          [ 0.6726,  0.1843, -0.4789,  ..., -0.0749,  0.1796,  0.4922],\n",
       "          [ 0.8133, -0.1612, -0.0162,  ..., -0.0364,  0.6229,  0.2363],\n",
       "          ...,\n",
       "          [ 0.7217, -0.0122, -0.0042,  ...,  0.7111,  0.1263,  0.7782],\n",
       "          [ 0.1118,  0.6872,  0.1279,  ...,  0.3713, -0.5159,  0.6878],\n",
       "          [-0.7213,  0.5163, -0.5362,  ...,  0.3796,  0.3710, -0.1588]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-4.6067e-01,  2.3040e-01, -2.1089e-01,  ...,  3.6072e-01,\n",
       "           -9.3187e-03, -5.0662e-02],\n",
       "          [-2.4900e-01,  8.0967e-01,  6.5591e-01,  ..., -3.5575e-01,\n",
       "           -3.0805e-01, -4.4758e-01],\n",
       "          [ 1.3585e-01,  3.7180e-01, -3.7921e-01,  ...,  1.5069e-01,\n",
       "           -5.3855e-01, -1.1642e+00],\n",
       "          ...,\n",
       "          [ 3.7778e-01,  4.6104e-01,  1.8163e-01,  ...,  3.6835e-01,\n",
       "           -2.0903e-01, -6.2378e-01],\n",
       "          [ 5.3807e-01, -9.2864e-02,  2.5129e-01,  ...,  5.6615e-01,\n",
       "            8.2072e-02, -3.0446e-01],\n",
       "          [-3.4001e-01, -4.7578e-01,  3.0237e-01,  ...,  3.5668e-01,\n",
       "            1.1676e-01, -8.1231e-01]],\n",
       "\n",
       "         [[-3.3181e-01, -6.1967e-01,  5.7304e-01,  ...,  1.3449e-01,\n",
       "           -1.1339e-01, -5.8985e-01],\n",
       "          [ 8.0485e-01, -3.5185e-01,  9.0272e-02,  ..., -3.6964e-01,\n",
       "            6.3910e-01, -5.3886e-01],\n",
       "          [ 6.8266e-01, -2.6570e-01,  5.3884e-01,  ..., -2.9916e-01,\n",
       "            3.1372e-01,  2.6549e-01],\n",
       "          ...,\n",
       "          [ 3.5778e-01,  3.0828e-04,  2.8862e-01,  ..., -7.3929e-01,\n",
       "            1.7320e-01, -9.5925e-02],\n",
       "          [ 5.9234e-01, -9.0829e-01,  6.6213e-01,  ...,  3.5930e-01,\n",
       "            6.4950e-01, -4.4751e-01],\n",
       "          [ 1.4035e-01, -1.9691e-01,  2.4421e-01,  ..., -9.2275e-01,\n",
       "            5.9154e-01, -8.3518e-01]],\n",
       "\n",
       "         [[-6.6474e-02, -6.2960e-01, -5.3819e-02,  ..., -3.4389e-01,\n",
       "            4.3570e-02,  2.1955e-01],\n",
       "          [-9.9182e-01, -4.8012e-01, -7.5631e-01,  ...,  8.3628e-01,\n",
       "            4.8803e-02,  1.1932e+00],\n",
       "          [-1.1924e-01, -4.3689e-01, -1.0688e-01,  ...,  8.5215e-01,\n",
       "            9.0443e-02,  6.7181e-01],\n",
       "          ...,\n",
       "          [-4.6979e-01, -7.1057e-01,  1.8066e-01,  ...,  1.1457e+00,\n",
       "            1.9469e-01,  1.2403e+00],\n",
       "          [ 1.0026e-01, -7.1101e-01,  5.5043e-01,  ...,  3.3358e-01,\n",
       "           -1.8978e-01,  5.9230e-01],\n",
       "          [-5.0161e-02,  5.9230e-01, -7.8124e-01,  ...,  9.9594e-01,\n",
       "           -1.1907e-01,  7.2886e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.7827e-01, -3.3243e-01,  6.9505e-01,  ..., -3.0852e-01,\n",
       "           -1.0674e-01,  1.0022e+00],\n",
       "          [-8.9998e-01, -1.9145e-01,  3.3587e-01,  ..., -1.9508e-01,\n",
       "            2.7232e-01,  6.9078e-01],\n",
       "          [-1.2018e+00, -3.6264e-02, -4.3404e-01,  ..., -5.9760e-01,\n",
       "            4.6897e-02,  7.1004e-01],\n",
       "          ...,\n",
       "          [-6.5310e-01, -6.8796e-02,  4.1809e-01,  ..., -2.9602e-01,\n",
       "            6.0890e-01,  1.3965e-01],\n",
       "          [-1.1157e+00, -6.5643e-01,  1.4621e-01,  ..., -3.5498e-01,\n",
       "            5.0626e-01,  6.8448e-01],\n",
       "          [-6.9928e-01, -5.6207e-02,  4.6986e-01,  ..., -5.1433e-01,\n",
       "            3.4352e-01,  4.8452e-01]],\n",
       "\n",
       "         [[-2.3388e-01, -8.1177e-02,  3.7800e-01,  ...,  2.4804e-01,\n",
       "            5.4394e-01,  9.8538e-01],\n",
       "          [ 1.5734e-01,  3.1000e-01,  6.1219e-01,  ...,  6.4437e-01,\n",
       "           -9.7705e-02,  1.4026e-01],\n",
       "          [ 3.1670e-01,  1.6210e-01,  4.9878e-01,  ...,  4.7363e-01,\n",
       "            5.7253e-01, -6.6566e-02],\n",
       "          ...,\n",
       "          [ 5.4528e-02, -3.3291e-01,  3.0936e-02,  ...,  1.1019e+00,\n",
       "            6.1165e-02, -1.6818e-01],\n",
       "          [ 1.8854e-01, -5.0502e-01, -3.0211e-01,  ...,  1.3725e+00,\n",
       "            5.5589e-01, -2.8804e-02],\n",
       "          [-4.0587e-01, -2.1512e-01, -1.4655e-01,  ...,  6.8246e-01,\n",
       "            3.5526e-01, -4.2704e-02]],\n",
       "\n",
       "         [[ 1.3826e+00,  7.8777e-01, -7.6592e-01,  ...,  3.8189e-02,\n",
       "            1.4255e-01, -1.7168e-01],\n",
       "          [ 1.2662e+00, -2.5468e-01, -6.2212e-01,  ..., -9.5901e-01,\n",
       "            1.8990e-01, -5.1289e-01],\n",
       "          [ 1.5765e+00, -4.3055e-02, -3.9812e-01,  ..., -1.4315e+00,\n",
       "           -1.7583e-01,  7.5687e-01],\n",
       "          ...,\n",
       "          [ 1.0949e+00, -4.2702e-02, -3.3250e-01,  ..., -1.3062e+00,\n",
       "           -9.0213e-02, -3.2631e-01],\n",
       "          [ 1.5487e-01, -6.0395e-02, -3.5084e-01,  ..., -1.0234e+00,\n",
       "           -5.6458e-02, -7.0393e-01],\n",
       "          [ 1.3288e+00, -3.3136e-01, -3.9395e-01,  ..., -6.4267e-01,\n",
       "            1.3808e-02, -6.3963e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.8334e-03, -4.8673e-02, -1.0033e+00,  ...,  1.2865e+00,\n",
       "           -3.4338e-01, -4.7394e-01],\n",
       "          [-5.2188e-01,  2.4415e-01, -8.8744e-01,  ...,  1.0403e+00,\n",
       "            5.8871e-01,  3.2607e-01],\n",
       "          [-9.3482e-02, -2.9824e-03, -6.4796e-01,  ...,  1.3540e+00,\n",
       "            6.7741e-01,  7.3942e-02]],\n",
       "\n",
       "         [[ 1.2411e+00,  5.7824e-02,  1.0018e+00,  ...,  1.7354e-01,\n",
       "           -1.1759e-01,  2.2734e-01],\n",
       "          [ 9.0211e-01, -7.5673e-01,  8.0530e-01,  ...,  2.3635e-01,\n",
       "           -2.4638e-01,  1.4086e-01],\n",
       "          [-2.6254e-01,  3.2087e-01,  9.3872e-01,  ...,  1.4186e-01,\n",
       "            1.7226e+00, -1.0431e-01]],\n",
       "\n",
       "         [[ 8.7353e-03,  4.5556e-02, -1.0467e-01,  ..., -9.7777e-01,\n",
       "            2.8331e-01,  3.1592e-02],\n",
       "          [ 7.6415e-02, -3.2629e-01,  6.7022e-02,  ..., -2.8693e-01,\n",
       "            2.1605e-01, -1.8406e-01],\n",
       "          [ 7.6492e-01,  2.1888e-01,  8.7461e-02,  ...,  3.9084e-01,\n",
       "            2.2346e-01, -8.5062e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.6416e-01, -1.0894e-01, -3.2044e-01,  ..., -2.9487e-01,\n",
       "            3.3598e-01, -2.7970e-01],\n",
       "          [-1.0108e+00, -3.3825e-01, -1.2933e-01,  ..., -6.6889e-01,\n",
       "            1.2645e-01, -5.8553e-01],\n",
       "          [-7.1577e-01, -3.9619e-01, -9.3913e-01,  ..., -6.2240e-01,\n",
       "           -2.3224e-01, -4.2422e-01]],\n",
       "\n",
       "         [[-1.1098e-03, -1.0787e+00,  5.4411e-01,  ..., -8.4982e-02,\n",
       "           -1.1317e+00,  3.4343e-01],\n",
       "          [-1.8194e-01, -1.0629e-01,  4.7336e-01,  ...,  8.5746e-01,\n",
       "           -6.1617e-01, -4.5101e-03],\n",
       "          [ 1.5294e-01, -5.2354e-01,  1.1891e+00,  ...,  5.5409e-02,\n",
       "           -7.5623e-01,  1.5153e+00]],\n",
       "\n",
       "         [[ 7.3473e-01,  2.6367e-01,  7.6743e-01,  ...,  9.0665e-01,\n",
       "           -2.3845e-01, -2.1405e-01],\n",
       "          [ 2.4872e-01,  3.5306e-01,  4.6105e-01,  ...,  1.5096e+00,\n",
       "            1.5995e-01, -4.7461e-01],\n",
       "          [-1.4041e-01, -1.9690e-01,  2.6554e-01,  ...,  1.3978e-01,\n",
       "           -6.6326e-03,  6.5479e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-3.7989e-01, -2.6762e-01, -9.9714e-01,  ...,  1.6598e-01,\n",
       "            3.8357e-01, -8.2313e-01],\n",
       "          [-8.6165e-01, -3.3882e-02, -5.5419e-01,  ..., -1.7101e-01,\n",
       "           -2.9414e-01, -1.2952e+00],\n",
       "          [-1.8704e-01,  4.2668e-01, -8.7790e-01,  ...,  1.4227e-01,\n",
       "           -5.7282e-01,  2.8152e-01]],\n",
       "\n",
       "         [[-6.1861e-01,  1.0043e-01, -1.2132e+00,  ..., -9.1159e-02,\n",
       "           -4.6750e-01,  3.0833e-01],\n",
       "          [-2.4730e-01,  8.7241e-01, -1.1909e+00,  ...,  1.2081e-01,\n",
       "           -3.0298e-01, -3.1691e-03],\n",
       "          [-1.9622e+00, -4.0888e-01, -1.0355e+00,  ...,  1.1546e-03,\n",
       "            6.6331e-01, -2.4980e-02]],\n",
       "\n",
       "         [[-2.0833e-01,  3.0676e-01,  6.0111e-01,  ...,  7.2191e-02,\n",
       "           -1.2011e+00,  1.0656e-01],\n",
       "          [-1.9078e-01,  9.7686e-01,  2.4402e-01,  ..., -3.9983e-01,\n",
       "           -8.0373e-01, -1.1571e-01],\n",
       "          [-9.6180e-02,  2.1800e-01,  1.7504e-01,  ..., -2.6949e-01,\n",
       "            4.3672e-01,  8.0878e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.5353e-01, -1.6416e-01, -6.8283e-01,  ...,  6.7732e-01,\n",
       "            3.3339e-01, -2.6322e-01],\n",
       "          [ 2.4436e-01, -1.3781e-01,  1.2173e-01,  ...,  9.9798e-01,\n",
       "           -4.2289e-03, -1.3894e-02],\n",
       "          [-3.2740e-01, -7.0770e-03, -3.7870e-01,  ...,  8.4323e-01,\n",
       "           -2.5095e-01,  5.2668e-01]],\n",
       "\n",
       "         [[ 8.5593e-01, -6.1307e-01,  8.6856e-01,  ..., -8.4985e-01,\n",
       "            3.0385e-01, -4.3115e-01],\n",
       "          [ 5.9763e-01, -1.0102e+00,  9.7639e-01,  ..., -4.0759e-01,\n",
       "            2.4565e-01, -6.9945e-01],\n",
       "          [-1.7136e-02, -2.2578e-01,  8.7648e-01,  ..., -1.0342e+00,\n",
       "            3.7880e-01, -2.5212e-01]],\n",
       "\n",
       "         [[-1.7163e-01, -4.9616e-01,  4.4939e-01,  ..., -3.5531e-02,\n",
       "           -3.0218e-01,  2.9646e-01],\n",
       "          [-2.0085e-03, -4.2902e-01,  4.5147e-01,  ..., -2.8788e-01,\n",
       "            5.3109e-02,  5.7091e-01],\n",
       "          [-3.8652e-01, -5.1289e-01, -1.1424e+00,  ...,  1.1473e+00,\n",
       "           -9.4097e-02, -3.9227e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-7.6210e-01, -9.9910e-02, -5.5067e-01,  ..., -5.1391e-01,\n",
       "           -1.5215e+00, -4.2943e-01],\n",
       "          [-3.7609e-01, -3.7069e-01, -1.4200e+00,  ..., -1.5829e+00,\n",
       "           -5.7960e-01, -3.7139e-01],\n",
       "          [-5.8814e-01,  7.6519e-01, -9.0781e-01,  ..., -8.9208e-01,\n",
       "           -7.7175e-01,  2.8815e-01],\n",
       "          ...,\n",
       "          [-5.3378e-01,  4.7636e-01, -7.0564e-01,  ..., -1.1119e+00,\n",
       "           -6.3453e-01,  5.7490e-01],\n",
       "          [-2.9245e-01,  2.1592e-01, -2.9114e-01,  ..., -5.0603e-01,\n",
       "           -5.9836e-01,  1.6948e-01],\n",
       "          [ 2.3620e-01,  7.0545e-01, -1.3213e+00,  ..., -5.5253e-01,\n",
       "           -3.8419e-01,  4.0121e-02]],\n",
       "\n",
       "         [[-1.3287e-01,  4.0313e-01, -1.0245e+00,  ..., -6.3272e-01,\n",
       "            5.3053e-01,  9.9271e-01],\n",
       "          [-1.7793e-01,  1.4378e-01, -5.0130e-02,  ..., -4.8431e-01,\n",
       "           -2.6783e-02,  9.3247e-02],\n",
       "          [ 1.5032e-01,  2.0856e-01, -6.2129e-01,  ..., -1.1402e+00,\n",
       "            6.7176e-02,  1.4289e-02],\n",
       "          ...,\n",
       "          [ 1.8478e-01,  1.0054e-01,  1.0056e-01,  ..., -1.3134e+00,\n",
       "            2.7250e-01, -2.3885e-01],\n",
       "          [ 1.3895e-01,  4.8266e-01, -8.9948e-01,  ..., -7.8576e-01,\n",
       "            8.7147e-01,  1.4778e-01],\n",
       "          [ 5.5454e-01,  9.8824e-01, -2.8733e-01,  ..., -1.5557e-01,\n",
       "            4.3569e-02,  2.0823e-01]],\n",
       "\n",
       "         [[ 4.1704e-01,  1.8196e-01,  1.1851e+00,  ..., -3.1602e-01,\n",
       "           -5.3758e-02, -5.9258e-02],\n",
       "          [ 1.1732e-01, -2.1572e-01,  4.9149e-01,  ...,  1.4322e-01,\n",
       "            1.0379e+00,  7.8577e-01],\n",
       "          [ 9.0853e-01, -5.5550e-01,  4.3265e-01,  ..., -2.0680e-01,\n",
       "            9.3610e-01,  6.2615e-01],\n",
       "          ...,\n",
       "          [ 3.8197e-01, -2.1661e-01,  7.3746e-01,  ...,  3.3129e-01,\n",
       "            1.0267e+00,  1.7318e-01],\n",
       "          [ 4.0937e-01,  1.4171e-01, -2.4464e-01,  ...,  1.1263e-01,\n",
       "            4.4019e-01,  1.3691e-01],\n",
       "          [-1.1572e+00, -2.4690e-01, -1.8519e-01,  ..., -3.5300e-01,\n",
       "            1.2892e-01,  4.6324e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.2834e-01,  4.4334e-01, -3.2248e-04,  ...,  1.1952e-01,\n",
       "            8.0608e-01,  1.0802e+00],\n",
       "          [ 1.3363e+00,  1.7957e-01, -2.2962e-01,  ...,  3.2919e-01,\n",
       "           -5.1042e-03,  1.2572e-01],\n",
       "          [ 5.7982e-01,  2.7964e-01, -3.9870e-01,  ...,  1.1006e+00,\n",
       "            6.0640e-01, -2.1058e-04],\n",
       "          ...,\n",
       "          [ 6.9823e-01,  3.3630e-01,  4.1934e-01,  ...,  1.0626e+00,\n",
       "           -3.5639e-01,  4.0834e-01],\n",
       "          [-2.2523e-02,  2.1407e-01, -3.1098e-01,  ..., -9.6945e-01,\n",
       "           -2.6998e-01,  3.6235e-01],\n",
       "          [ 2.7670e-01,  6.9456e-01,  8.2008e-01,  ...,  5.6431e-01,\n",
       "            3.3132e-01,  1.4714e-01]],\n",
       "\n",
       "         [[ 4.3172e-01,  3.9055e-01, -2.4205e-01,  ...,  8.5365e-02,\n",
       "            5.5054e-01,  4.4716e-01],\n",
       "          [ 8.6550e-01,  1.2445e-01, -7.8864e-01,  ...,  2.3142e-01,\n",
       "            1.9815e-01,  5.4508e-02],\n",
       "          [ 4.7698e-01,  2.8310e-01, -3.2462e-01,  ..., -8.0772e-02,\n",
       "            4.7183e-01,  5.1113e-01],\n",
       "          ...,\n",
       "          [ 4.1868e-01,  6.3986e-02, -5.7414e-01,  ...,  6.4918e-01,\n",
       "            4.2020e-01, -2.3399e-01],\n",
       "          [ 3.3823e-01,  6.4628e-02, -4.7400e-02,  ...,  3.0870e-01,\n",
       "            2.7926e-01,  2.5019e-01],\n",
       "          [ 9.3623e-01, -1.0609e-02, -4.8246e-01,  ...,  8.5834e-02,\n",
       "           -2.5621e-01, -3.1084e-01]],\n",
       "\n",
       "         [[ 2.1784e-01, -2.2417e-01, -3.9091e-02,  ...,  4.4013e-01,\n",
       "           -5.8125e-02,  1.4773e+00],\n",
       "          [ 5.9331e-01, -3.8123e-01, -6.0104e-01,  ..., -1.8909e-01,\n",
       "           -2.3977e-01,  4.7991e-01],\n",
       "          [ 3.1736e-01, -7.9562e-01,  7.0387e-02,  ..., -1.5517e-01,\n",
       "            5.0648e-01,  6.9816e-01],\n",
       "          ...,\n",
       "          [ 1.9127e-01,  4.5651e-01, -4.6715e-01,  ...,  4.8035e-01,\n",
       "            2.4959e-01,  9.3313e-01],\n",
       "          [ 3.1659e-02,  6.1326e-02, -4.4395e-01,  ...,  6.6271e-01,\n",
       "            9.3093e-01,  8.3182e-01],\n",
       "          [ 2.6616e-01,  1.0094e+00, -8.9910e-01,  ..., -5.7874e-01,\n",
       "            8.3117e-02,  9.1531e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.7661, -0.4966,  0.1313,  ..., -1.1839,  0.2573,  1.2410],\n",
       "          [-0.1038, -0.4474,  0.1769,  ..., -1.4013,  0.8362,  0.5482],\n",
       "          [-0.1380,  0.1078, -0.7202,  ..., -0.4319,  1.5649,  0.8584],\n",
       "          ...,\n",
       "          [ 0.0298, -0.1983,  0.2574,  ..., -0.4678,  0.5814,  0.2677],\n",
       "          [ 0.2682,  0.0222, -0.4809,  ..., -0.3867,  0.7100,  0.9127],\n",
       "          [ 0.5180, -0.2687, -0.4606,  ..., -0.4961,  0.9757,  0.7976]],\n",
       "\n",
       "         [[-0.1441,  0.2070, -0.1765,  ...,  0.3828,  0.4022, -0.1744],\n",
       "          [-0.1167,  0.3157,  0.2130,  ...,  0.5469,  0.9752, -0.0793],\n",
       "          [ 0.3376,  0.8208, -0.0365,  ...,  0.1545,  0.6992, -0.1262],\n",
       "          ...,\n",
       "          [-0.0098, -0.3095, -0.2742,  ...,  0.6421,  0.7541, -0.5259],\n",
       "          [-0.1396,  1.3988,  0.6442,  ...,  0.4745,  0.0546, -0.6678],\n",
       "          [-0.9355, -0.2025,  0.5662,  ...,  0.8202,  0.0967, -0.1015]],\n",
       "\n",
       "         [[ 0.0607, -0.6088,  0.4175,  ...,  0.3579,  0.3371,  0.4577],\n",
       "          [-1.2524, -0.2477, -0.2284,  ..., -0.1349,  1.6549, -0.0194],\n",
       "          [-0.7842, -0.1965, -0.2051,  ..., -0.4877,  1.2533,  0.0811],\n",
       "          ...,\n",
       "          [-1.1051, -0.2202, -0.5728,  ..., -0.2271,  0.9761,  0.4880],\n",
       "          [ 0.0344, -0.3896,  0.3734,  ..., -0.9796,  0.9251, -0.2898],\n",
       "          [-0.9230,  0.2652, -0.0292,  ..., -0.9591,  1.0086, -0.2556]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0422, -0.1308, -0.5191,  ..., -0.3341,  0.1875,  0.7083],\n",
       "          [ 0.3429,  0.1728, -0.1589,  ..., -0.3002, -0.7385, -0.2355],\n",
       "          [-0.1083,  0.2157,  0.2962,  ...,  0.0241, -0.5834, -0.2059],\n",
       "          ...,\n",
       "          [ 0.0108,  0.8726,  0.1262,  ..., -0.0842, -0.2267,  0.0323],\n",
       "          [ 0.3676, -0.4208, -0.5348,  ..., -0.7299,  0.5025,  0.4097],\n",
       "          [-0.9161, -0.2086, -0.5260,  ...,  0.0907,  0.1242,  0.5864]],\n",
       "\n",
       "         [[-0.9078, -0.8469,  0.2581,  ...,  0.3803, -0.9057,  0.4755],\n",
       "          [-0.6326, -0.1207, -0.1080,  ..., -0.4718,  0.3269, -0.5018],\n",
       "          [-0.4112, -0.2005, -0.7973,  ...,  0.1840,  0.4670,  0.2253],\n",
       "          ...,\n",
       "          [-0.3314,  0.2920, -0.2675,  ...,  0.1191,  0.9357,  0.4030],\n",
       "          [-0.2199, -0.6206, -0.2110,  ...,  0.5230,  0.4274,  0.9568],\n",
       "          [ 0.1733, -0.7614, -0.1810,  ..., -0.2605,  0.2127,  0.5711]],\n",
       "\n",
       "         [[ 0.5740,  1.1099, -0.0708,  ...,  0.4402, -0.3346, -0.1867],\n",
       "          [ 0.1181, -0.1091, -0.2947,  ..., -0.2237, -0.8491, -0.0194],\n",
       "          [ 0.1337,  0.0465, -0.5964,  ..., -0.9249, -0.7230,  0.4918],\n",
       "          ...,\n",
       "          [ 0.2361,  0.3297, -0.6654,  ..., -0.4955, -0.6459,  0.0902],\n",
       "          [ 0.0420, -0.1132, -0.6973,  ...,  0.5761, -0.1223, -0.2776],\n",
       "          [ 0.5661,  0.2494, -1.3268,  ...,  0.1687, -0.5931, -0.6428]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[-0.2896, -1.1382,  0.0179,  ...,  0.0611,  0.1800, -0.8549],\n",
       "          [-0.0671, -1.0510, -0.2649,  ...,  0.3673, -0.0699, -0.4492],\n",
       "          [-0.5084, -0.6351,  0.2181,  ..., -0.4846, -0.2099, -0.1678]],\n",
       "\n",
       "         [[-0.0459,  0.1086, -0.0907,  ..., -1.0024, -0.2328,  0.8414],\n",
       "          [-0.1308,  0.1722, -0.1455,  ..., -1.1723, -0.4024,  1.0417],\n",
       "          [-0.4029, -0.8565,  0.4864,  ..., -0.1061, -0.1515,  0.1676]],\n",
       "\n",
       "         [[ 0.8700,  0.3492,  0.8469,  ...,  0.0856, -0.0104, -0.8975],\n",
       "          [ 0.7512,  0.5802,  0.6109,  ...,  0.6235, -0.5897, -1.3183],\n",
       "          [ 0.6296,  0.6379,  1.1350,  ..., -0.3530, -0.3014,  0.1000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0683,  1.0174, -0.6225,  ..., -0.4864, -0.8116, -1.0917],\n",
       "          [ 0.6193,  0.3972, -0.4864,  ..., -0.3878, -0.9734, -0.5896],\n",
       "          [ 0.0040,  0.1189, -0.4901,  ..., -0.5932, -0.4791, -0.6868]],\n",
       "\n",
       "         [[ 0.0056, -0.2868, -0.2808,  ...,  0.5619, -0.4135,  0.6523],\n",
       "          [ 0.6369, -0.0712,  0.7797,  ...,  1.1798,  0.0641, -0.0730],\n",
       "          [ 0.5460,  0.0691,  0.1940,  ...,  0.2378, -0.0334,  0.3919]],\n",
       "\n",
       "         [[ 1.2196,  0.6845,  0.8757,  ...,  0.7754,  0.0024,  0.6092],\n",
       "          [ 1.0569,  0.3331,  0.3974,  ...,  0.5627,  0.0181,  0.2958],\n",
       "          [ 1.1556, -0.2660,  0.8558,  ...,  0.0380,  0.1011,  0.3746]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-4.8152e-01,  8.3230e-01, -4.0822e-01,  ..., -2.7923e-01,\n",
       "           -3.2959e-02,  2.1123e-01],\n",
       "          [-9.5307e-01,  1.1236e+00,  1.7681e-01,  ..., -4.2835e-01,\n",
       "            9.7150e-02,  3.9121e-01],\n",
       "          [-5.0365e-01,  1.2930e-01,  3.4358e-01,  ...,  2.6874e-02,\n",
       "           -1.0896e+00, -3.5762e-01]],\n",
       "\n",
       "         [[-2.0155e-01, -8.4829e-01, -2.9899e-01,  ..., -4.5593e-02,\n",
       "            3.4328e-01,  3.6134e-01],\n",
       "          [-2.5188e-01, -6.2263e-01, -1.5328e-01,  ...,  2.6098e-01,\n",
       "            8.5732e-01,  3.9571e-01],\n",
       "          [ 1.3387e-01, -9.0679e-01, -2.3392e-01,  ..., -1.1350e-01,\n",
       "            1.0112e+00,  9.0082e-01]],\n",
       "\n",
       "         [[ 3.3113e-01,  1.0247e+00, -3.3259e-02,  ...,  3.7565e-02,\n",
       "           -1.9536e-01, -4.8909e-02],\n",
       "          [-3.8841e-01,  9.3855e-01,  7.7543e-01,  ..., -2.6270e-01,\n",
       "           -1.6522e-01, -3.7343e-02],\n",
       "          [-6.4935e-01,  7.3921e-01, -2.6797e-01,  ..., -2.9975e-01,\n",
       "           -2.4022e-01, -6.9630e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.9407e-02, -3.3343e-02, -4.1187e-01,  ..., -6.1515e-01,\n",
       "            4.4727e-01, -6.7827e-01],\n",
       "          [-1.5412e-01,  3.4664e-01, -3.8198e-01,  ..., -9.3495e-01,\n",
       "            7.5422e-01, -5.5618e-01],\n",
       "          [-1.1132e+00, -5.1866e-02, -3.7976e-01,  ..., -1.8173e-01,\n",
       "            1.7336e-01, -2.1482e-01]],\n",
       "\n",
       "         [[ 1.4318e-01, -3.1719e-01,  2.5768e-01,  ...,  7.7645e-02,\n",
       "            3.2756e-01, -7.0458e-01],\n",
       "          [ 2.5239e-01,  6.8046e-05,  5.4124e-01,  ...,  1.5489e-01,\n",
       "            4.9694e-01, -4.7109e-01],\n",
       "          [ 4.2181e-02,  3.5454e-01,  8.7687e-01,  ...,  1.0815e-01,\n",
       "           -1.1165e-01, -3.2066e-01]],\n",
       "\n",
       "         [[ 5.2875e-01,  4.0646e-01,  4.3312e-01,  ...,  1.2771e+00,\n",
       "           -4.0538e-03,  1.8771e-01],\n",
       "          [ 8.5430e-01,  6.7771e-01,  5.2693e-01,  ...,  9.2863e-01,\n",
       "            5.0970e-01,  1.5751e-02],\n",
       "          [ 7.3147e-01,  8.8187e-01,  7.9129e-01,  ...,  1.4019e+00,\n",
       "            1.1857e-01,  1.6370e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 8.0192e-01, -3.8634e-01,  1.5740e-01,  ...,  6.0013e-01,\n",
       "           -2.1548e-01, -2.7251e-01],\n",
       "          [ 4.6000e-01, -8.7375e-01, -1.1036e-01,  ...,  5.2242e-01,\n",
       "            6.5972e-01,  4.4402e-01],\n",
       "          [-2.1304e-01,  1.2385e-02, -6.4767e-01,  ...,  4.8922e-01,\n",
       "            7.9337e-02,  2.6693e-01],\n",
       "          ...,\n",
       "          [-1.6654e-01, -4.9199e-01,  2.2930e-01,  ...,  5.6447e-01,\n",
       "            5.7143e-01,  5.5209e-01],\n",
       "          [ 6.2510e-02, -5.6321e-01, -4.2400e-01,  ...,  3.7597e-01,\n",
       "           -4.9908e-01, -3.1149e-01],\n",
       "          [-3.6121e-01,  3.2918e-01, -5.3392e-02,  ...,  5.3036e-01,\n",
       "           -9.8377e-02, -1.0933e-01]],\n",
       "\n",
       "         [[ 4.8736e-01, -2.7148e-01, -1.8539e-01,  ...,  5.6421e-01,\n",
       "            1.5353e-02,  2.8568e-01],\n",
       "          [ 3.8892e-01, -8.4262e-01, -1.3434e-01,  ..., -3.4637e-01,\n",
       "            3.7235e-04,  1.3601e+00],\n",
       "          [ 1.3005e-01, -7.0621e-01,  1.0241e-01,  ..., -6.3660e-01,\n",
       "            1.0723e+00,  1.3711e+00],\n",
       "          ...,\n",
       "          [ 5.5820e-01, -1.1980e+00, -1.9947e-01,  ..., -1.8377e-02,\n",
       "            2.0586e-01,  1.2569e+00],\n",
       "          [ 9.6307e-01, -1.7226e-01,  2.1494e-02,  ...,  3.7570e-01,\n",
       "            6.3798e-02, -3.9121e-01],\n",
       "          [ 3.9129e-01, -1.1605e+00, -5.1278e-02,  ..., -6.4504e-01,\n",
       "            1.2133e-02,  9.0330e-01]],\n",
       "\n",
       "         [[ 7.4931e-02,  7.4615e-02,  5.1947e-02,  ...,  2.6210e-01,\n",
       "            5.3902e-01, -1.2584e-01],\n",
       "          [ 2.5272e-02,  1.2409e-01,  1.6007e-01,  ...,  1.0011e+00,\n",
       "           -5.5656e-01, -2.4022e-01],\n",
       "          [-2.1503e-01,  5.5897e-02, -1.0184e-01,  ...,  3.6958e-01,\n",
       "            3.3017e-01, -1.8178e-01],\n",
       "          ...,\n",
       "          [ 5.3028e-01,  3.1156e-01,  3.7979e-02,  ...,  1.3037e-01,\n",
       "            3.2040e-01, -5.8493e-02],\n",
       "          [-1.2748e-01,  2.7474e-01,  1.3143e-01,  ...,  3.2810e-01,\n",
       "            3.6325e-01, -1.1268e-02],\n",
       "          [ 1.4717e-01,  1.4356e+00, -5.2067e-01,  ...,  1.9788e-01,\n",
       "            1.4530e+00,  2.0054e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0101e-01,  3.7273e-02,  2.0641e-01,  ...,  5.5692e-01,\n",
       "            1.1969e-01,  3.1588e-01],\n",
       "          [-7.8164e-02, -2.7747e-01,  4.0550e-01,  ...,  9.2000e-01,\n",
       "            1.8770e-01,  1.1621e+00],\n",
       "          [-3.0704e-01,  1.4993e-02,  3.3074e-01,  ...,  3.8467e-01,\n",
       "            2.7433e-03,  6.1909e-01],\n",
       "          ...,\n",
       "          [-1.9432e-01, -8.2184e-02,  1.8464e-01,  ...,  8.9028e-01,\n",
       "            4.8346e-01,  1.3082e+00],\n",
       "          [ 5.3616e-01,  2.8451e-01,  2.6654e-01,  ...,  7.2487e-01,\n",
       "           -5.0086e-01,  1.3055e+00],\n",
       "          [ 3.4711e-01, -6.1315e-02,  4.3032e-01,  ...,  1.1306e+00,\n",
       "           -2.8065e-02,  5.9149e-01]],\n",
       "\n",
       "         [[ 5.4838e-01, -3.0393e-01,  4.6085e-02,  ..., -4.3367e-01,\n",
       "           -5.4891e-01,  3.4277e-01],\n",
       "          [-5.6172e-01,  6.0934e-02,  2.1782e-01,  ...,  2.7854e-01,\n",
       "           -2.1602e-01, -1.2717e+00],\n",
       "          [ 3.5203e-01,  1.9817e-01,  3.5736e-01,  ...,  4.6168e-01,\n",
       "           -3.1056e-01, -9.9751e-01],\n",
       "          ...,\n",
       "          [-1.9425e-01,  1.1116e-01,  6.3304e-01,  ...,  4.3191e-01,\n",
       "           -7.3362e-02, -6.1984e-01],\n",
       "          [-5.3779e-01,  5.1553e-01,  9.2505e-01,  ...,  3.3239e-01,\n",
       "            3.4474e-01, -2.9204e-01],\n",
       "          [ 6.4752e-01, -1.7802e-01,  1.2399e+00,  ..., -4.9546e-01,\n",
       "           -2.8492e-02, -1.7624e-01]],\n",
       "\n",
       "         [[-4.7532e-01,  8.2309e-01,  1.9761e-01,  ..., -3.4901e-01,\n",
       "            2.6865e-01, -2.6842e-01],\n",
       "          [-9.3911e-03,  1.0350e-01, -5.6358e-01,  ..., -1.2004e+00,\n",
       "           -6.8571e-01,  3.8448e-01],\n",
       "          [ 1.1675e-01,  6.5974e-01,  2.4964e-01,  ..., -4.0554e-01,\n",
       "            3.1441e-01,  5.5046e-01],\n",
       "          ...,\n",
       "          [ 3.3688e-01, -1.7440e-01, -4.5193e-02,  ..., -8.8234e-01,\n",
       "           -3.9747e-01,  5.0435e-01],\n",
       "          [ 1.0414e-01,  1.1388e-01,  9.5580e-01,  ...,  2.9996e-01,\n",
       "            6.0184e-02,  7.2517e-01],\n",
       "          [-1.0112e+00,  3.9746e-01,  6.4593e-01,  ..., -5.7948e-01,\n",
       "            3.0310e-02,  8.2935e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.5259,  0.2485,  1.1778,  ..., -0.0381, -0.4454,  0.4780],\n",
       "          [-0.5622, -0.3466,  0.3590,  ..., -0.0865, -0.2962,  0.7546],\n",
       "          [-0.3204, -0.3947,  0.8566,  ...,  0.0275, -0.4151,  1.2537],\n",
       "          ...,\n",
       "          [-0.7387, -0.4333,  0.6615,  ...,  0.1834, -0.6696,  0.3629],\n",
       "          [-0.7515,  0.2842,  0.8283,  ...,  0.4187,  0.3681,  0.8612],\n",
       "          [-1.0848,  0.4876,  0.7531,  ..., -0.5645,  0.3963,  0.5041]],\n",
       "\n",
       "         [[ 0.0619,  0.4824, -0.3028,  ..., -0.9715,  0.2686, -0.7226],\n",
       "          [-0.0182,  0.2105, -0.4236,  ..., -0.4766, -0.9514, -0.6379],\n",
       "          [ 0.1952,  0.3053, -0.2495,  ...,  0.5091, -0.9174, -0.5519],\n",
       "          ...,\n",
       "          [ 0.4488,  0.1380, -0.0651,  ..., -0.3811, -0.6810, -0.2406],\n",
       "          [ 0.6404, -0.0116, -0.3467,  ...,  0.5241, -0.0283, -0.6605],\n",
       "          [ 0.4046, -0.5452, -0.9102,  ..., -0.3085, -0.9038, -0.2954]],\n",
       "\n",
       "         [[-0.5530,  0.0864,  0.0151,  ...,  0.1068, -0.4391, -0.5449],\n",
       "          [-0.2955,  0.6368, -0.2568,  ..., -0.1302,  0.5110, -0.1682],\n",
       "          [-0.5218,  0.4264, -0.2110,  ..., -0.5285,  0.3739,  0.1876],\n",
       "          ...,\n",
       "          [-0.3623, -0.0339, -1.1004,  ..., -0.6807,  0.1763, -0.0104],\n",
       "          [-0.3313, -0.3540, -0.4483,  ..., -0.8012, -0.2177, -0.7418],\n",
       "          [-0.0197, -0.1623, -0.2122,  ..., -0.6929, -0.3495, -0.7008]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2182,  0.2182, -0.4928,  ...,  0.3113,  0.5728, -1.1895],\n",
       "          [-0.0054,  0.2797, -0.1795,  ...,  0.9595,  0.6644,  0.0817],\n",
       "          [-0.5790,  0.5973, -0.1671,  ...,  0.8943,  0.1931, -0.0447],\n",
       "          ...,\n",
       "          [-0.7265,  0.4006,  0.5095,  ...,  1.1810,  0.5822, -0.3538],\n",
       "          [-0.3318,  0.1387, -0.3580,  ...,  1.0402,  0.1226, -0.6473],\n",
       "          [ 0.0225,  0.3198,  0.2026,  ...,  0.9438,  0.3966, -0.3961]],\n",
       "\n",
       "         [[ 0.4414, -0.0814, -0.3337,  ...,  0.3554,  0.3243,  0.1761],\n",
       "          [ 0.7415, -0.0067, -0.7335,  ...,  0.1410,  0.5930,  0.2539],\n",
       "          [ 1.0188,  0.0409, -1.1035,  ..., -0.6412, -0.0183, -0.0346],\n",
       "          ...,\n",
       "          [ 1.2128,  0.1214, -1.0330,  ..., -0.3670,  0.6161,  0.2034],\n",
       "          [ 0.6620, -0.4616, -0.0367,  ...,  0.2465,  0.4793,  0.4745],\n",
       "          [ 0.9796, -0.2549, -0.6967,  ..., -0.1701,  0.9033, -0.0569]],\n",
       "\n",
       "         [[-0.4523, -1.0534,  1.0023,  ...,  0.4841,  0.1479,  0.9267],\n",
       "          [ 0.5094, -1.6607,  0.9940,  ..., -0.1059,  0.1102,  0.1352],\n",
       "          [-0.0274, -1.0653,  0.5775,  ...,  0.3531, -0.2039, -0.0611],\n",
       "          ...,\n",
       "          [ 0.0026, -1.0908,  1.4530,  ..., -0.1294, -0.2918,  0.2066],\n",
       "          [ 0.0419,  0.1524, -0.0875,  ..., -0.7174,  0.7005,  0.3984],\n",
       "          [-0.3215, -0.9929,  0.0390,  ...,  0.1915, -0.2245,  0.5568]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.6616, -0.6444, -2.3440,  ...,  0.2163,  0.2950, -1.5357],\n",
       "         [ 0.3336,  0.4374, -1.5776,  ..., -0.1850, -1.0965, -1.9951],\n",
       "         [ 1.0703,  0.5164, -1.6595,  ..., -1.0678, -0.0201, -0.9745],\n",
       "         ...,\n",
       "         [ 0.6899,  0.6390, -1.1909,  ..., -0.9485, -0.5129, -1.0858],\n",
       "         [ 2.4962, -0.4073, -1.3643,  ..., -0.1722, -0.3522, -1.2120],\n",
       "         [ 2.1471,  1.7575, -2.6914,  ...,  0.2267, -1.1126, -0.3012]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " summary_ids = model.generate(torch.tensor([input_ids]), num_beams=num_beams, **generation_args.__dict__)\n",
    "        print('** text: ', text)\n",
    "        print('** title: ', title)\n",
    "        if len(summary_ids.shape) == 1  or summary_ids.shape[0] == 1:\n",
    "            ## 출력 1개\n",
    "            title = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "            print('Gen title 0', title)\n",
    "        else :\n",
    "            ## 출력 여러개\n",
    "            titles = tokenizer.batch_decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "            for idx, title in enumerate(titles) :\n",
    "                print('Gen title', idx, title)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "981f108a204f421f158e0977940335d851edffa6dd3586828a3e1aec045160e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('final': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
