{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from datasets import (load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Value,\n",
    "    Features\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "paper_dataset = load_dataset('metamong1/summarization_paper', \n",
    "    use_auth_token='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')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset paper_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/2.2.0/46d835d4e22daa3a5a46d13de39e3d75f6c2eaef5ead153d48cbe8d7cd3bec9c)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2f96d0e82d24e4d98de3ff4891f02f6"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "sys.path.append('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from preprocessor import PaperPreprocessor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "preprocessor = PaperPreprocessor()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "paper_dataset = paper_dataset.map(preprocessor.for_train)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/73640 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be74360ad7f24448af49f05dba5342e8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/18411 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03fa16b6ff6f448f832ac01cb581c612"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "train_data = paper_dataset['train']\n",
    "train_docs = [data['text'] for data in train_data]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "train_docs[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'역시간 구조보정은 음원영역 파동장 외삽과 수진기영역 파동장 외삽의 상호상관으로 지층구조를 영상화하는 방법으로 복잡한 등방성 매질 층서구조를 영상화하는데 주로 이용된다. 그러나 일반적으로 지구내부 지층구조는 이방성 특성을 지니고 있으므로 이방성을 고려한 구조보정 기술이 필요하다. 여기에서는 편미분 파동장과 음원모음의 내적에 의한 알고리즘과 가상음원과 역전파 파동장과의 내적에 의한 알고리즘을 이용하여 횡적등방성 매질에서 역시간 구조보정 기술을 개발하고자 하였다. 단순 이방성 지층모델에 대한 수치모형실험 결과, 두 가지 방법에 의한 지층단면도 영상은 거의 차이가 없어 가상음원과 역전파 파동장과의 내적으로 구조보정을 실시하는 것이 효과적임을 알 수 있었다. 수평적으로 속도가 변하는 이방성 매질 지층구조에서 편미분 파동장을 구하지 않고 영상화 할 수 있음을 알 수 있었다.'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select UNK Token"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "sent_tokenize(train_docs[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['역시간 구조보정은 음원영역 파동장 외삽과 수진기영역 파동장 외삽의 상호상관으로 지층구조를 영상화하는 방법으로 복잡한 등방성 매질 층서구조를 영상화하는데 주로 이용된다.',\n",
       " '그러나 일반적으로 지구내부 지층구조는 이방성 특성을 지니고 있으므로 이방성을 고려한 구조보정 기술이 필요하다.',\n",
       " '여기에서는 편미분 파동장과 음원모음의 내적에 의한 알고리즘과 가상음원과 역전파 파동장과의 내적에 의한 알고리즘을 이용하여 횡적등방성 매질에서 역시간 구조보정 기술을 개발하고자 하였다.',\n",
       " '단순 이방성 지층모델에 대한 수치모형실험 결과, 두 가지 방법에 의한 지층단면도 영상은 거의 차이가 없어 가상음원과 역전파 파동장과의 내적으로 구조보정을 실시하는 것이 효과적임을 알 수 있었다.',\n",
       " '수평적으로 속도가 변하는 이방성 매질 지층구조에서 편미분 파동장을 구하지 않고 영상화 할 수 있음을 알 수 있었다.']"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "unk_words = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "for doc in tqdm(train_docs) :\n",
    "    sen_list = sent_tokenize(doc)\n",
    "\n",
    "    for sen in sen_list :\n",
    "        word_list = word_tokenize(sen)\n",
    "\n",
    "        for word in word_list :\n",
    "            if tokenizer.unk_token_id in tokenizer.encode(word) :\n",
    "                unk_words.append(word)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/73640 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dda96a7bd2d0402a968ae05d5babee9c"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "unk_word_counter = collections.Counter()\n",
    "unk_word_counter.update(unk_words)\n",
    "\n",
    "unk_word_counter = dict(unk_word_counter)\n",
    "print('Unk Word Size : %d' %len(unk_word_counter))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unk Word Size : 26921\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "unk_word_item = sorted(unk_word_counter.items(), key=lambda x : x[1], reverse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "unk_word_item[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('휨', 141),\n",
       " ('살핌으로써', 53),\n",
       " ('앎과', 45),\n",
       " ('앎', 41),\n",
       " ('앎의', 37),\n",
       " ('휨강도', 37),\n",
       " ('시퀀스를', 34),\n",
       " ('앎을', 33),\n",
       " ('시퀀스', 32),\n",
       " ('朱子의', 30)]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "words = [item[0] for item in unk_word_item]\n",
    "counts = [item[1] for item in unk_word_item]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "unk_word_df = pd.DataFrame({'Word' : words, 'Counts' : counts})\n",
    "unk_word_df.to_csv('../paper_vocabs.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "unk_word_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Word  Counts\n",
       "0      휨     141\n",
       "1  살핌으로써      53\n",
       "2     앎과      45\n",
       "3      앎      41\n",
       "4     앎의      37"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>휨</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>살핌으로써</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>앎과</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>앎</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>앎의</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large', use_fast=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "print('Index : 31500 \\t Token : %s' %tokenizer.convert_ids_to_tokens(31500))\n",
    "print('Index : 31599 \\t Token : %s' %tokenizer.convert_ids_to_tokens(31599))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 31500 \t Token : [unused0]\n",
      "Index : 31599 \t Token : [unused99]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "tokenizer.save_pretrained('../Tokenizer')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('../Tokenizer/tokenizer_config.json',\n",
       " '../Tokenizer/special_tokens_map.json',\n",
       " '../Tokenizer/vocab.txt',\n",
       " '../Tokenizer/added_tokens.json',\n",
       " '../Tokenizer/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "words[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['휨', '살핌으로써', '앎과', '앎', '앎의']"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "class TokenizerOptimization :\n",
    "    def __init__(self, dir_path, word_list) :\n",
    "        self.txt_path = os.path.join(dir_path, 'vocab.txt')\n",
    "        self.json_path = os.path.join(dir_path, 'tokenizer.json')\n",
    "        self.vocab_map = {i: word for i, word in enumerate(word_list)}\n",
    "\n",
    "    def write_vocab(self, vocab_map) :\n",
    "        data_size = len(vocab_map)\n",
    "        vocab_list = list(vocab_map.values())\n",
    "        f = open(self.txt_path, 'w')\n",
    "        for i in range(data_size):\n",
    "            f.write(vocab_list[i]+'\\n')\n",
    "        f.close()\n",
    "\n",
    "    def add_unused(self, vocab_map, tokenizer) :\n",
    "        unused_start = tokenizer.convert_tokens_to_ids('[unused0]')\n",
    "        unused_end = tokenizer.convert_tokens_to_ids('[unused99]') + 1\n",
    "\n",
    "        unused_size = unused_end - unused_start \n",
    "        for i in range(unused_size) :\n",
    "            unused_idx = unused_start + i\n",
    "            word = vocab_map[i]\n",
    "            vocab_map[unused_idx] = word\n",
    "\n",
    "    def load_tokenizer_json(self) :\n",
    "        with open(self.json_path, \"r\") as json_data:\n",
    "            tokenizer_data = json.load(json_data)\n",
    "        return tokenizer_data\n",
    "\n",
    "    def write_tokenizer_json(self, tokenizer_data, vocab_data) :\n",
    "        inverse_vocab_data = {vocab_data[key] : key for key in vocab_data.keys()}\n",
    "        tokenizer_data['model']['vocab'] = inverse_vocab_data\n",
    "        with open(self.json_path, 'w') as json_file:\n",
    "            json.dump(tokenizer_data, json_file)\n",
    "\n",
    "    def optimize(self, tokenizer) :\n",
    "        assert isinstance(tokenizer, PreTrainedTokenizerFast)\n",
    "        self.add_unused(self.vocab_map, tokenizer)\n",
    "        self.write_vocab(self.vocab_map)\n",
    "\n",
    "        tokenizer_data = self.load_tokenizer_json()\n",
    "        self.write_tokenizer_json(tokenizer_data, self.vocab_map)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "optimizer = TokenizerOptimization('./', words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "optimizer.optimize(tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained('../Tokenizer', use_fast=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "print('Index : 31500 \\t Token : %s' %new_tokenizer.convert_ids_to_tokens(31500))\n",
    "print('Index : 31599 \\t Token : %s' %new_tokenizer.convert_ids_to_tokens(31599))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 31500 \t Token : 휨\n",
      "Index : 31599 \t Token : 茶\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('final': conda)"
  },
  "interpreter": {
   "hash": "4d949d837314f0bd127b310a251a9c5e144fbaeaec32e29e415d9adebd0cb2d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}