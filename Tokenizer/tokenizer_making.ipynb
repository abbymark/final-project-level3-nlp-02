{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from datasets import (load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Value,\n",
    "    Features\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bert Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large', use_fast=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "tokenizer.save_pretrained('/opt/ml/project/final-project-level3-nlp-02/Tokenizer')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/opt/ml/project/final-project-level3-nlp-02/Tokenizer/tokenizer_config.json',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/special_tokens_map.json',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/vocab.txt',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/added_tokens.json',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "print('Index : 31500 \\t Token : %s' %tokenizer.convert_ids_to_tokens(31500))\n",
    "print('Index : 31999 \\t Token : %s' %tokenizer.convert_ids_to_tokens(31999))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 31500 \t Token : [unused0]\n",
      "Index : 31999 \t Token : [unused499]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizaing : Target Bert, Roberta"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def load_tokenizer(path) :\n",
    "    with open(path, \"r\") as f:\n",
    "       data = json.load(f)\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "tokenizer_data = load_tokenizer('../Tokenizer/tokenizer.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def load_vocab(path) : \n",
    "    with open(path, \"r\") as f :\n",
    "        data = f.read()\n",
    "    idx2vocab = {i:vocab for i, vocab in enumerate(data.split('\\n')[:-1])}\n",
    "    return idx2vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "idx2vocab = load_vocab('../Tokenizer/vocab.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Updating"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def update_unused(idx2vocab, ch_df) :\n",
    "    unused_start = 31500\n",
    "    \n",
    "    size = 0\n",
    "    map_id = 0\n",
    "    while(size < 500) :\n",
    "        ch, flag = ch_df.iloc[map_id][['Character', 'KoreanFlag']]\n",
    "\n",
    "        target_id = unused_start + size\n",
    "        if flag == False :\n",
    "            idx2vocab[target_id] = ch\n",
    "            size += 1\n",
    "        else :\n",
    "            if size + 2 >= 500 :\n",
    "                map_id += 1\n",
    "                continue\n",
    "\n",
    "            idx2vocab[target_id] = ch\n",
    "            idx2vocab[target_id+1] = '##' + ch\n",
    "            size += 2\n",
    "            \n",
    "        map_id += 1\n",
    "\n",
    "    return idx2vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "extra_df = pd.read_csv('../Tokenizer/extra_characters.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "idx2vocab_after = update_unused(idx2vocab, extra_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Writing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "def update_vocab(idx2vocab, path) :\n",
    "    data_size = len(idx2vocab)\n",
    "    vocab_list = list(idx2vocab.values())\n",
    "    f = open(path, 'w')\n",
    "    for i in range(data_size):\n",
    "        f.write(vocab_list[i]+'\\n')\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "update_vocab(idx2vocab_after, '../Tokenizer/vocab.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "def update_tokenizer(idx2vocab, tokenizer_data, path) :\n",
    "    vocab2idx = {idx2vocab[key] : key for key in idx2vocab.keys()}\n",
    "    tokenizer_data['model']['vocab'] = vocab2idx\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(tokenizer_data, f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "update_tokenizer(idx2vocab_after, tokenizer_data, '../Tokenizer/tokenizer.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained('../Tokenizer', use_fast=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "print('Index : 31500 \\t Token : %s' %new_tokenizer.convert_ids_to_tokens(31500))\n",
    "print('Index : 31599 \\t Token : %s' %new_tokenizer.convert_ids_to_tokens(31999))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 31500 \t Token : 乙\n",
      "Index : 31599 \t Token : 舞\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bart Tokniezr"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-summarization', use_fast=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "print('Index : 7 \\t Token : %s' %tokenizer.convert_ids_to_tokens(7))\n",
    "print('Index : 106 \\t Token : %s' %tokenizer.convert_ids_to_tokens(106))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 7 \t Token : <unused0>\n",
      "Index : 106 \t Token : <unused99>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "tokenizer.save_pretrained('/opt/ml/project/final-project-level3-nlp-02/Tokenizer')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/opt/ml/project/final-project-level3-nlp-02/Tokenizer/tokenizer_config.json',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/special_tokens_map.json',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/vocab.json',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/merges.txt',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/added_tokens.json',\n",
       " '/opt/ml/project/final-project-level3-nlp-02/Tokenizer/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "def load_json(path) :\n",
    "   with open(path, \"r\") as f:\n",
    "      data = json.load(f)\n",
    "   return data\n",
    "\n",
    "def write_json(tokenizer_data, path) :\n",
    "   with open(path, 'w') as f:\n",
    "      json.dump(tokenizer_data, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "dir_path = '../Tokenizer'\n",
    "tokenizer_path = '../Tokenizer/tokenizer.json'\n",
    "vocab_path = '../Tokenizer/vocab.json'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "def update_unused(idx2vocab, ch_df) :\n",
    "    unused_start = 7\n",
    "    unused_size = 100\n",
    "    \n",
    "    size = 0\n",
    "    map_id = 0\n",
    "    while(size < unused_size) :\n",
    "        ch, flag = ch_df.iloc[map_id][['Character', 'KoreanFlag']]\n",
    "\n",
    "        if ch in idx2vocab.values() :\n",
    "            map_id += 1\n",
    "            continue\n",
    "\n",
    "        target_id = unused_start + size\n",
    "        if flag == False :\n",
    "            idx2vocab[target_id] = ch\n",
    "            size += 1\n",
    "        else :\n",
    "            if size + 2 >= unused_size :\n",
    "                map_id += 1\n",
    "                continue\n",
    "\n",
    "            idx2vocab[target_id] = ch\n",
    "            idx2vocab[target_id+1] = '▁' + ch\n",
    "            size += 2\n",
    "            \n",
    "        map_id += 1\n",
    "\n",
    "    return idx2vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "# optimizing tokenizer\n",
    "def optimize(extra_ch_path) :\n",
    "    ch_df = pd.read_csv(extra_ch_path)\n",
    "\n",
    "    # load tokenizer data\n",
    "    tokenizer_data = load_json(tokenizer_path)\n",
    "    vocab2idx = load_json(vocab_path)\n",
    "    idx2vocab = {idx:vocab for vocab, idx in vocab2idx.items()}\n",
    "    \n",
    "    # update tokenizer\n",
    "    idx2vocab = update_unused(idx2vocab, ch_df)\n",
    "    vocab_list = list(idx2vocab.values())\n",
    "\n",
    "    vocab2idx = {idx2vocab[key] : key for key in idx2vocab.keys()}\n",
    "\n",
    "    tokenizer_data['model']['vocab'] = vocab2idx\n",
    "    tokenizer_added_tokens = tokenizer_data['added_tokens']\n",
    "\n",
    "    for i in range(7,106+1) :\n",
    "        tokenizer_added_tokens[i]['content'] = vocab_list[i]\n",
    "\n",
    "    # write tokenizer data\n",
    "    write_json(vocab2idx, vocab_path)\n",
    "    write_json(tokenizer_data, tokenizer_path)\n",
    "\n",
    "    # load updated tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(dir_path, use_fast=True)\n",
    "    return tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "new_tokenizer = optimize('../Tokenizer/extra_characters.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "print('Index : 7 \\t Token : %s' %new_tokenizer.convert_ids_to_tokens(7))\n",
    "print('Index : 106 \\t Token : %s' %new_tokenizer.convert_ids_to_tokens(106))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 7 \t Token : 룖\n",
      "Index : 106 \t Token : 弇\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('final': conda)"
  },
  "interpreter": {
   "hash": "4d949d837314f0bd127b310a251a9c5e144fbaeaec32e29e415d9adebd0cb2d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}