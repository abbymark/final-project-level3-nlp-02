{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from datasets import (load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Value,\n",
    "    Features\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "sys.path.append('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2', use_fast=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print('Size of tokenizer : %d' %len(tokenizer))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of tokenizer : 30000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "paper_dataset = load_dataset('metamong1/summarization_paper', \n",
    "    use_auth_token='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')\n",
    "\n",
    "law_dataset = load_dataset('metamong1/summarization_law', \n",
    "    use_auth_token='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')\n",
    "\n",
    "magazine_dataset = load_dataset('metamong1/summarization_magazine', \n",
    "    use_auth_token='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')\n",
    "\n",
    "news_dataset = load_dataset('metamong1/summarization_news', \n",
    "    use_auth_token='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')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset paper_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b056903e52144fdd8ce7b5308c5951bd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset law_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___law_summarization/Paper Summarization/1.2.0/b422baca30e481895dd2b572a7ff9f6c6428725e575fdafb73c0aa1d62356973)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5a07ac9025c41c49f6e68d58e1a8d8a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset magazine_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___magazine_summarization/Magizine Summarization/1.0.0/506cb41eb0b96b084eafa5dd5fe3b51ff0d1061256700adf1aa92d3b19762c36)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0af44d8ebac5428db2f346aeea4b9f19"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset news_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___news_summarization/News Summarization/1.0.0/ae25c3215dc878e979d01f1157dbfb014c0a6985fc959ae45eaf10847db75600)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9e95beb4c9c4eecb250e87b56a3fc82"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Documents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "news_docs = [data['text'] for data in news_dataset['train']]\n",
    "law_docs = [data['text'] for data in law_dataset['train']]\n",
    "magazine_docs = [data['text'] for data in magazine_dataset['train']]\n",
    "paper_docs = [data['text'] for data in paper_dataset['train']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "total_docs = news_docs + law_docs + magazine_docs + paper_docs\n",
    "random.shuffle(total_docs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from preprocessor import DocsPreprocessor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bracket_comp = re.compile(r\"\\([^)]+\\)\")\n",
    "data_preprocessor = DocsPreprocessor()\n",
    "\n",
    "def preprocess(doc) :\n",
    "    doc = bracket_comp.sub(' ', doc)\n",
    "    data_preprocessor.doc_preprocess(doc)\n",
    "    doc = re.sub('\\s+', ' ', doc)\n",
    "    return doc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "total_docs = [preprocess(doc) for doc in tqdm(total_docs)]"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/390689 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5333bc7d55544200b98f12bc5e8653df"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "total_docs[11]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'본 연구에서는 붉은장목수수 MeOH 추출물과 분획물들의 다양한 생리활성을 비교하기 위하여항산화 , 항당뇨 , 항암 활성을 관찰 하였다. 항산화활성의 경우 EtOAc fraction이 높은 총 페놀과 플라보노이드 함량을 나타내었으며, DPPH assay와 reducing power 결과 역시 EtOAc fraction과 MeOH fraction이 positive control로 사용한 α-tocopherol보다 우수한 활성을 나타냈다. α-Glucosidase, α-amlyase 저해능 평가 결과 D.W. fraction이 가장 높은 활성을 보였으며 n-BuOH fraction 과 MeOH extract도 활성을 나타내었다. 암세포인 AGS,HT29, HCT116세포주에 대한 세포독성 연구인 MTT assay 에서는 EtOAc, n-BuOH, D.W. fraction을 처리한 처리구에서 독성을 보였으며 이중 n-BuOH fraction이 모든세포주에서 농도 의존적으로 세포독성을 가지는 것을 확인할 수 있었다. 연구결과 붉은장목수수의 추출물 EtOAc fraction의 항산화활성, D.W. fraction의 항당뇨활성, n-BuOH fraction의 암세포에 대한 독성과 관련된 화합물의분리 및 구조규명에 관한 연구가 필요 할 것으로 사료되어지며, normal cell에 대한 MTT assay를 진행하여 항암활성에 관한 연구를 진행하여 붉은장목수수 추출물을 이용한 다양한 건강기능식품과 의약품 개발을 통해 건강 증진과질병으로부터 보호 받을 수 있을 것으로 기대한다.'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract Unk Tokens"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "target_docs = {}\n",
    "\n",
    "for i,doc in enumerate(tqdm(total_docs)) :\n",
    "    if tokenizer.unk_token_id in tokenizer.encode(doc) :\n",
    "        target_docs[i] = doc"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/390689 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "607ab709189840ada1da935084263c81"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print('Size of Documents which has UNK Tokens : %d' %len(target_docs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of Documents which has UNK Tokens : 231\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "unk_ids = list(target_docs.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "unk_chars = []\n",
    "\n",
    "for idx, doc in tqdm(target_docs.items()) :\n",
    "    doc = re.sub('\\s+', '', doc)\n",
    "    for ch in doc :\n",
    "        if tokenizer.convert_tokens_to_ids(ch) == tokenizer.unk_token_id :\n",
    "            unk_chars.append(ch)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e6923a3d5094958a09544e063acccab"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "unk_char_counter = collections.Counter()\n",
    "unk_char_counter.update(unk_chars)\n",
    "unk_char_counter = dict(unk_char_counter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "unk_char_counter = sorted(unk_char_counter.items(), key=lambda x : x[1], reverse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "target_chars = []\n",
    "\n",
    "private_range = re.compile('[^\\u0000-\\u007f\\ue000-\\uf8ff\\uac00-\\ud7af]')\n",
    "for char, count in unk_char_counter :\n",
    "    if private_range.match(char) != None :\n",
    "        if count >= 1 :\n",
    "            target_chars.append(char)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "target_chars = target_chars[:100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimize Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "tokenizer.save_pretrained('./tmp')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./tmp/tokenizer_config.json',\n",
       " './tmp/special_tokens_map.json',\n",
       " './tmp/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "print('Index : 7 \\t Token : %s' %tokenizer.convert_ids_to_tokens(7))\n",
    "print('Index : 106 \\t Token : %s' %tokenizer.convert_ids_to_tokens(106))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 7 \t Token : <unused0>\n",
      "Index : 106 \t Token : <unused99>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "def load_tokenizer(path) :\n",
    "    with open(path, \"r\") as f:\n",
    "       data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def write_json(tokenizer_data, path) :\n",
    "   with open(path, 'w') as f:\n",
    "      json.dump(tokenizer_data, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# optimizing tokenizer\n",
    "def optimize(dir_path, vocab_list) :\n",
    "    file_path = os.path.join(dir_path, 'tokenizer.json')\n",
    "    assert os.path.isfile(file_path) \n",
    "\n",
    "    if len(vocab_list) > 100 :\n",
    "        vocab_list = vocab_list[:100]\n",
    "    \n",
    "    tokenizer_data = load_tokenizer(file_path)\n",
    "\n",
    "    # part1 added_tokens\n",
    "    unused_start = 7\n",
    "    unused_size = 100\n",
    "    vocab_idx = 0\n",
    "    for i in range(unused_start, unused_start+unused_size) :\n",
    "        tokenizer_data['added_tokens'][i]['content'] = vocab_list[vocab_idx]\n",
    "        if vocab_idx >= len(vocab_list) :\n",
    "            break\n",
    "        vocab_idx += 1\n",
    "\n",
    "    # part2 model\n",
    "    tokenizer_vocab_data = tokenizer_data['model']['vocab']\n",
    "    idx2vocab = {idx:vocab for vocab, idx in tokenizer_vocab_data.items()}\n",
    "\n",
    "    vocab_idx = 0\n",
    "    for i in range(unused_start, unused_start+unused_size) :\n",
    "        idx2vocab[i] = vocab_list[vocab_idx]\n",
    "        if vocab_idx >= len(vocab_list) :\n",
    "            break\n",
    "        vocab_idx += 1\n",
    "\n",
    "    vocab2idx = {vocab:idx for idx,vocab in idx2vocab.items()}\n",
    "    tokenizer_data['model']['vocab'] = vocab2idx\n",
    "\n",
    "    write_json(tokenizer_data, file_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(dir_path, use_fast=True)\n",
    "    return tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "optimized_tokenizer = optimize('./kobart', target_chars)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "print('Index : 7 \\t Token : %s' %optimized_tokenizer.convert_ids_to_tokens(7))\n",
    "print('Index : 7 \\t Token : %s' %optimized_tokenizer.convert_ids_to_tokens(50))\n",
    "print('Index : 106 \\t Token : %s' %optimized_tokenizer.convert_ids_to_tokens(106))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 7 \t Token : ㄹ\n",
      "Index : 7 \t Token : 匿\n",
      "Index : 106 \t Token : ㅠ\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "metamong1_tokenizer = AutoTokenizer.from_pretrained('metamong1/kobart')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "print('Index : 7 \\t Token : %s' %metamong1_tokenizer.convert_ids_to_tokens(7))\n",
    "print('Index : 7 \\t Token : %s' %metamong1_tokenizer.convert_ids_to_tokens(50))\n",
    "print('Index : 106 \\t Token : %s' %metamong1_tokenizer.convert_ids_to_tokens(106))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index : 7 \t Token : ㄹ\n",
      "Index : 7 \t Token : 匿\n",
      "Index : 106 \t Token : ㅠ\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('final': conda)"
  },
  "interpreter": {
   "hash": "4d949d837314f0bd127b310a251a9c5e144fbaeaec32e29e415d9adebd0cb2d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}