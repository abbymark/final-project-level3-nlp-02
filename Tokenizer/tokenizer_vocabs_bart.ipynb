{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from datasets import (load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Value,\n",
    "    Features\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sys.path.append('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-12-02 01:55:54.830098: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-summarization', use_fast=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "paper_dataset = load_dataset('metamong1/summarization_paper', \n",
    "    use_auth_token='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')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset paper_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/2.2.0/46d835d4e22daa3a5a46d13de39e3d75f6c2eaef5ead153d48cbe8d7cd3bec9c)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c96c2aab11d4522b6c55f64f44b6fde"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "law_dataset = load_dataset('metamong1/summarization_law', \n",
    "    use_auth_token='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')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset law_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___law_summarization/Paper Summarization/1.2.0/b422baca30e481895dd2b572a7ff9f6c6428725e575fdafb73c0aa1d62356973)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9b69d2e3f914889afbe6a1a1b54d84f"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "magazine_dataset = load_dataset('metamong1/summarization_magazine', \n",
    "    use_auth_token='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')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset magazine_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___magazine_summarization/Magizine Summarization/1.0.0/506cb41eb0b96b084eafa5dd5fe3b51ff0d1061256700adf1aa92d3b19762c36)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfdfec631487415fa6cbda4488da0542"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "news_dataset = load_dataset('metamong1/summarization_news', \n",
    "    use_auth_token='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')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset news_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___news_summarization/News Summarization/1.0.0/ae25c3215dc878e979d01f1157dbfb014c0a6985fc959ae45eaf10847db75600)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de21f43c9f5f4399bdea83e471adc23d"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from preprocessor import PaperPreprocessor, DocsPreprocessor, Filter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "paper_preprocessor = PaperPreprocessor()\n",
    "docs_preprocessor = DocsPreprocessor()\n",
    "data_filter = Filter(title_size=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Paper Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "paper_dataset.cleanup_cache_files()\n",
    "paper_dataset = paper_dataset.map(paper_preprocessor.for_train)\n",
    "paper_dataset = paper_dataset.filter(data_filter)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/73640 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f2ca019b8e44b7f8e6afad7966584b8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/18411 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "707f7305da404819b34eba37e462a74b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7f43be78eca42c5b1fdec039d66350a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38efea6e131e4ab2a6addc2c71fc486a"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "paper_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "        num_rows: 69950\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "        num_rows: 17493\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Law"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "law_dataset.cleanup_cache_files()\n",
    "law_dataset = law_dataset.map(docs_preprocessor.for_train)\n",
    "law_dataset = law_dataset.filter(data_filter)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/23730 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4182d8437a54447992c9f0ca560ab2e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/5933 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70e07ba8089c4d66af72a7f022b895a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d089a1cb62b841e787d8d1b8732a96df"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f054f38058f74dc49d2d9cf131fc826e"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "law_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "        num_rows: 20921\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "        num_rows: 5223\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Magazine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "magazine_dataset.cleanup_cache_files()\n",
    "magazine_dataset = magazine_dataset.map(paper_preprocessor.for_train)\n",
    "magazine_dataset = magazine_dataset.filter(data_filter)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/52691 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e775ff0fe16042478bcedc3885166ccc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/13173 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "861313fd0acc4b6cb6b386d34e5e8515"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca2498752864419a99af25cda576afca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0edbca9f83b64635a569dce3e6e3a54d"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "magazine_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "        num_rows: 51211\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "        num_rows: 12823\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### News"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "news_dataset.cleanup_cache_files()\n",
    "news_dataset = news_dataset.map(docs_preprocessor.for_train)\n",
    "news_dataset = news_dataset.filter(data_filter)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/240628 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b4e18b0b6254d55908173381cbe6814"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/60157 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c67d941d33334a43931e433136546599"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/241 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2373f75b3f29420aaf2d9578713c8d70"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88c8458cffaa41b9b1a1b39a4f27e1ca"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "news_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "        num_rows: 214237\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "        num_rows: 53572\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Tokens"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "paper_train = paper_dataset['train']\n",
    "paper_docs = [doc['text'] for doc in paper_train]\n",
    "\n",
    "magazine_train = magazine_dataset['train']\n",
    "magazine_docs = [doc['text'] for doc in magazine_train]\n",
    "\n",
    "law_train = law_dataset['train']\n",
    "law_docs = [doc['text'] for doc in law_train]\n",
    "\n",
    "news_train = news_dataset['train']\n",
    "news_docs = [doc['text'] for doc in news_train]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "total_docs = paper_docs + magazine_docs + law_docs + news_docs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "random.shuffle(total_docs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print('Data Size : %d' %len(total_docs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Size : 356319\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "total_sens = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "for doc in tqdm(total_docs) :\n",
    "    sen_list = sent_tokenize(doc)\n",
    "    total_sens.extend(sen_list)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/356319 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7302844a2b6a4f4580ad2e67ecce5e64"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print('Sentence Size : %d' %len(total_sens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence Size : 4573278\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "total_sens[13]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"하지만 대법원은 '모욕적 인신공격 발언'이 아니라며 2심 재판을 다시하라고 결정했다.\""
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "sen_tokenized = [tokenizer.tokenize(sen) for sen in tqdm(total_sens)]"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/4573278 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03bad894ae3d4fed834c8d5b70844f1f"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "sen_unk = {i:vec for i,vec in enumerate(tqdm(sen_tokenized)) if tokenizer.unk_token in vec}\n",
    "sen_unk_ids = list(sen_unk.keys())\n",
    "print('Size : %d' %len(sen_unk))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/4573278 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a493a26ec47f45b79f0360a25012b3e3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size : 229\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "print('Original Sentence')\n",
    "print(total_sens[sen_unk_ids[121]])\n",
    "print('')\n",
    "print('Tokenized Sentence')\n",
    "print(tokenizer.convert_tokens_to_string(sen_unk[sen_unk_ids[121]]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Sentence\n",
      "둘째, 유흠은 이처럼 왕제 의 ‘천자칠묘’를 혈연적 친소에 따라 입묘되는 迭毁廟의 常數로 해석한 후, 功德에 따라 세우는 ‘宗’은 變數라고 하여 그 설치에 제한을 두지 않는다는 ‘宗變’論을 주장하였다.\n",
      "\n",
      "Tokenized Sentence\n",
      "둘째, 유흠은 이처럼 왕제 의 ‘천자칠묘’를 혈연적 친소에 따라 입묘되는 <unk>毁廟의 常數로 해석한 후, 功德에 따라 세우는 ‘宗’은 變數라고 하여 그 설치에 제한을 두지 않는다는 ‘宗變’論을 주장하였다.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "print('Original Sentence')\n",
    "print(total_sens[sen_unk_ids[13]])\n",
    "print('')\n",
    "print('Tokenized Sentence')\n",
    "print(tokenizer.convert_tokens_to_string(sen_unk[sen_unk_ids[13]]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Sentence\n",
      "장에서는 \u0002全唐詩\u0003에 수록된 詩歌 중 西王母의 이미지를‘美’ ?\n",
      "\n",
      "Tokenized Sentence\n",
      "장에서는 <unk>全唐詩<unk>에 수록된 詩歌 중 西王母의 이미지를‘美’ ?\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizer Add Tokens"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "print('Original Sentence')\n",
    "print(total_sens[sen_unk_ids[125]])\n",
    "print('')\n",
    "print('Tokenized Sentence')\n",
    "print(tokenizer.convert_tokens_to_string(sen_unk[sen_unk_ids[125]]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Sentence\n",
      "당시 정준영은 “동영상 찍어서 보내준 거 걸려가지곸 ”라는 말을 했고, 용준형은 “그 여자애한테 걸렸다고?”라며 반문했다.\n",
      "\n",
      "Tokenized Sentence\n",
      "당시 정준영은 “동영상 찍어서 보내준 거 걸려가지<unk> ”라는 말을 했고, 용준형은 “그 여자애한테 걸렸다고?”라며 반문했다.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-summarization', use_fast=True)\n",
    "tokenizer.add_tokens(['곸'])\n",
    "tokenizer.tokenize('걸려가지곸')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['걸', '려', '가지', '곸']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-summarization', use_fast=True)\n",
    "tokenizer.add_tokens(['▁곸'])\n",
    "tokenizer.tokenize('걸려가지곸')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['걸', '려', '가지', '<unk>']"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-summarization', use_fast=True)\n",
    "vocab_set = tokenizer.vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "start_id = ord('가')\n",
    "end_id = ord('힣')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "size = 0\n",
    "\n",
    "for i in range(start_id, end_id+1) :\n",
    "    ch1 = chr(i)\n",
    "    ch2 = '▁' + ch1\n",
    "    flag = ch1 in vocab_set and ch2 not in vocab_set\n",
    "\n",
    "    if flag == True :\n",
    "        size += 1 \n",
    "\n",
    "print('원래 글자는 있고 _붙인 글자는 없는 경우') # 굣 - O,  ##굣 - X\n",
    "print('Size : %d' %size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "원래 글자는 있고 _붙인 글자는 없는 경우\n",
      "Size : 3989\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "size = 0\n",
    "for i in range(start_id, end_id+1) :\n",
    "    ch1 = chr(i)\n",
    "    ch2 = '▁' + ch1\n",
    "    flag = ch1 not in vocab_set and ch2 in vocab_set\n",
    "    \n",
    "    if flag == True :\n",
    "        size += 1\n",
    "\n",
    "print('원래 글자는 없고 _붙인 글자는 있는 경우') # 굣 - X,  ##굣 - O\n",
    "print('Size : %d' %size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "원래 글자는 없고 _붙인 글자는 있는 경우\n",
      "Size : 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "size = 0 \n",
    "\n",
    "for i in range(start_id, end_id+1) :\n",
    "    ch1 = chr(i)\n",
    "    ch2 = '▁' + ch1\n",
    "    flag = ch1 in vocab_set and ch2 in vocab_set\n",
    "    \n",
    "    if flag == True :\n",
    "        size += 1\n",
    "\n",
    "print('원래 글자는 있고 _붙인 글자는 있는 경우') # 굣 - O,  ##굣 - O\n",
    "print('Size : %d' %size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "원래 글자는 있고 _붙인 글자는 있는 경우\n",
      "Size : 888\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select Characters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "unk_words = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "for idx in tqdm(sen_unk_ids) :\n",
    "    sen = total_sens[idx]\n",
    "    tok_list = mecab.morphs(sen)\n",
    "\n",
    "    for tok in tok_list :\n",
    "        if tokenizer.unk_token_id in tokenizer.encode(tok) :\n",
    "            unk_words.append(tok)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3dbef16ab494771ae0f4520dac63d1d"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "unk_words[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\\x01',\n",
       " '\\x02',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x01',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x01']"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "unk_chars = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "for word in tqdm(unk_words) :\n",
    "    for ch in word :\n",
    "        if tokenizer.convert_tokens_to_ids(ch) == tokenizer.unk_token_id :\n",
    "            unk_chars.append(ch)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/331 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34c387ed7e604a488076e4fc7be98aec"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "unk_chars[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\\x01',\n",
       " '\\x02',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x01',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x01']"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "unk_ch_counter = collections.Counter()\n",
    "unk_ch_counter.update(unk_chars)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "unk_char_items = sorted(unk_ch_counter.items(), key=lambda x : x[1], reverse=True)\n",
    "unk_char_items = [item for item in unk_char_items if re.search('[\\x00-\\x20]', item[0]) == None]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "unk_char_items[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('룖', 54),\n",
       " ('윾', 34),\n",
       " ('뢾', 11),\n",
       " ('뢿', 11),\n",
       " ('拗', 7),\n",
       " ('剡', 7),\n",
       " ('嘔', 5),\n",
       " ('‒', 5),\n",
       " ('줼', 4),\n",
       " ('귱', 3)]"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "unk_ch_list = [item[0] for item in unk_char_items]\n",
    "unk_ch_count = [item[1] for item in unk_char_items]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "extra_unk_chars = pd.DataFrame({'Character' : unk_ch_list, 'Count' : unk_ch_count})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "def check_korean(ch) :\n",
    "    if ord(ch) in range(ord('가'), ord('힣')+1) :\n",
    "        return True\n",
    "    else :\n",
    "        return False "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "extra_unk_chars['KoreanFlag'] = extra_unk_chars['Character'].map(check_korean)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "extra_unk_chars.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Character  Count  KoreanFlag\n",
       "0         룖     54        True\n",
       "1         윾     34        True\n",
       "2         뢾     11        True\n",
       "3         뢿     11        True\n",
       "4         拗      7       False"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Count</th>\n",
       "      <th>KoreanFlag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>룖</td>\n",
       "      <td>54</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>윾</td>\n",
       "      <td>34</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>뢾</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>뢿</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>拗</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "extra_unk_chars.to_csv('./Tokenizer/extra_for_bart.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('final': conda)"
  },
  "interpreter": {
   "hash": "4d949d837314f0bd127b310a251a9c5e144fbaeaec32e29e415d9adebd0cb2d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}