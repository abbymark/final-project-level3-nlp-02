{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabbymark\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/final_project/final-project-level3-nlp-02-amc_/runs/pfhky4fy\" target=\"_blank\">edpl3_tiny_25ep</a></strong> to <a href=\"https://wandb.ai/final_project/final-project-level3-nlp-02-amc_\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/final_project/final-project-level3-nlp-02-amc_/runs/pfhky4fy?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f253bd0a4f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "from knowledge_distillation import DistillationTrainingArguments, DistillationTrainer, TinyTrainer\n",
    "from huggingface_hub import notebook_login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(verbose=True)\n",
    "import torch\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append('/opt/ml/final-project-level3-nlp-02')\n",
    "from rouge import compute\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "wandb.init(\n",
    "    entity=\"final_project\",\n",
    "    project='optimization',\n",
    "    name='edpl3_tiny_25ep'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "train_size = 5000\n",
    "eval_size = 500\n",
    "check_point = 'encoder_decoder_pruned_last_5' #\"gogamza/kobart-summarization\"\n",
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 25\n",
    "learning_rate=5.6e-5\n",
    "weight_decay = 0.01\n",
    "logging_steps = 500\n",
    "model_name = check_point.split(\"/\")[-1]\n",
    "\n",
    "distillation_type = 'tiny'\n",
    "if distillation_type:\n",
    "    student_check_point = \"encoder_decoder_pruned_last_3\"\n",
    "    teacher_check_point = \"kobart-summarization-finetuned-5000/checkpoint-1000\"\n",
    "    alpha=0.5\n",
    "    temperature = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset paper_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653e8329e9f249689a3888a9044e8877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api_token = os.getenv('HF_DATASET_API_TOKEN')\n",
    "dataset = datasets.load_dataset('metamong1/summarization_paper', use_auth_token=api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f/cache-93a7be41ce043bd9.arrow\n"
     ]
    }
   ],
   "source": [
    "training_dataset = dataset['train'].shuffle(seed=seed).select(range(train_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distillation_type:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(student_check_point)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['text'], max_length=max_input_length, truncation = True, #padding=True\n",
    "    )\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['title'], max_length=max_target_length, truncation=True, #padding=True\n",
    "        )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f/cache-0083b6d5c7c91f98.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = training_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f/cache-d9529bd184d76b26.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_eval_dataset = dataset['validation'].select(range(500)).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if distillation_type:\n",
    "    student_model = AutoModelForSeq2SeqLM.from_pretrained(student_check_point, torch_dtype='auto').to(device)\n",
    "    teacher_model = AutoModelForSeq2SeqLM.from_pretrained(teacher_check_point, torch_dtype='auto').to(device)\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(check_point, torch_dtype='auto').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distillation_type:\n",
    "    args =  DistillationTrainingArguments(\n",
    "        output_dir=f'{student_check_point}-distilled-{train_size}',\n",
    "        evaluation_strategy = 'steps',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay = weight_decay,\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        predict_with_generate=True,\n",
    "        logging_steps=logging_steps,\n",
    "        alpha=alpha,\n",
    "        temperature=temperature,\n",
    "        report_to='wandb'\n",
    "    )\n",
    "else:\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'{model_name}-finetuned-{train_size}',\n",
    "        evaluation_strategy='steps', #'epoch',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay = weight_decay,\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        predict_with_generate=True,\n",
    "        logging_steps=logging_steps,\n",
    "        report_to='wandb'\n",
    "        # push_to_hub=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distillation_type:\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=student_model)\n",
    "else:\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distillation_type == 'distil':\n",
    "    trainer = DistillationTrainer(\n",
    "        model=student_model,\n",
    "        args=args,\n",
    "        teacher_model = teacher_model,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "elif distillation_type == 'tiny':\n",
    "    trainer = TinyTrainer(\n",
    "        model=student_model,\n",
    "        args=args,\n",
    "        teacher_model = teacher_model,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "else:\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15625\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8997' max='15625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8997/15625 26:12 < 19:18, 5.72 it/s, Epoch 14.39/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>18.237900</td>\n",
       "      <td>6.121234</td>\n",
       "      <td>5.675700</td>\n",
       "      <td>0.556100</td>\n",
       "      <td>5.203200</td>\n",
       "      <td>5.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>12.976100</td>\n",
       "      <td>5.468546</td>\n",
       "      <td>7.258400</td>\n",
       "      <td>0.796900</td>\n",
       "      <td>6.486100</td>\n",
       "      <td>6.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>11.283900</td>\n",
       "      <td>4.995747</td>\n",
       "      <td>8.367400</td>\n",
       "      <td>1.126200</td>\n",
       "      <td>7.582600</td>\n",
       "      <td>7.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>10.315500</td>\n",
       "      <td>4.619568</td>\n",
       "      <td>10.087900</td>\n",
       "      <td>1.865300</td>\n",
       "      <td>8.902800</td>\n",
       "      <td>8.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>9.569400</td>\n",
       "      <td>4.279552</td>\n",
       "      <td>12.936100</td>\n",
       "      <td>3.434900</td>\n",
       "      <td>11.557800</td>\n",
       "      <td>11.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>8.836500</td>\n",
       "      <td>3.957933</td>\n",
       "      <td>15.409200</td>\n",
       "      <td>5.014300</td>\n",
       "      <td>13.835600</td>\n",
       "      <td>13.800700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>8.207200</td>\n",
       "      <td>3.671987</td>\n",
       "      <td>19.279800</td>\n",
       "      <td>7.896200</td>\n",
       "      <td>17.405700</td>\n",
       "      <td>17.412900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>7.847700</td>\n",
       "      <td>3.427720</td>\n",
       "      <td>21.823600</td>\n",
       "      <td>9.859300</td>\n",
       "      <td>19.892000</td>\n",
       "      <td>19.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>7.380900</td>\n",
       "      <td>3.231940</td>\n",
       "      <td>24.637900</td>\n",
       "      <td>11.965300</td>\n",
       "      <td>22.385900</td>\n",
       "      <td>22.339400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>7.007700</td>\n",
       "      <td>3.111772</td>\n",
       "      <td>26.233000</td>\n",
       "      <td>14.074700</td>\n",
       "      <td>24.101500</td>\n",
       "      <td>24.100900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>6.676500</td>\n",
       "      <td>2.992422</td>\n",
       "      <td>28.356000</td>\n",
       "      <td>15.290500</td>\n",
       "      <td>25.658700</td>\n",
       "      <td>25.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.439300</td>\n",
       "      <td>2.905635</td>\n",
       "      <td>29.807600</td>\n",
       "      <td>16.508800</td>\n",
       "      <td>27.184900</td>\n",
       "      <td>27.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>6.214500</td>\n",
       "      <td>2.859203</td>\n",
       "      <td>29.743600</td>\n",
       "      <td>16.582800</td>\n",
       "      <td>27.146500</td>\n",
       "      <td>27.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>6.052800</td>\n",
       "      <td>2.793307</td>\n",
       "      <td>31.436200</td>\n",
       "      <td>17.955800</td>\n",
       "      <td>28.547900</td>\n",
       "      <td>28.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.848100</td>\n",
       "      <td>2.752372</td>\n",
       "      <td>32.827200</td>\n",
       "      <td>19.022600</td>\n",
       "      <td>29.992400</td>\n",
       "      <td>29.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.711800</td>\n",
       "      <td>2.728337</td>\n",
       "      <td>32.009100</td>\n",
       "      <td>18.373500</td>\n",
       "      <td>28.924000</td>\n",
       "      <td>28.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.530300</td>\n",
       "      <td>2.691850</td>\n",
       "      <td>32.467900</td>\n",
       "      <td>19.098300</td>\n",
       "      <td>29.585100</td>\n",
       "      <td>29.598100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-500\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-30500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-31000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, title, file, doc_type.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_3-distilled-5000/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_3-distilled-5000/checkpoint-7500] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1310\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1839\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final-project-level3-nlp-02/amc_/knowledge_distillation.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Extract logits from teacher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutputs_teacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mlogits_teacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_teacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 )\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1300\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m         attention_mask = self._prepare_decoder_attention_mask(\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36m_prepare_decoder_attention_mask\u001b[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mcombined_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             combined_attention_mask = _make_causal_mask(\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             ).to(self.device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: title, file, doc_type, text, doc_id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.3818862438201904,\n",
       " 'eval_rouge1': 31.6557,\n",
       " 'eval_rouge2': 18.708,\n",
       " 'eval_rougeL': 28.8796,\n",
       " 'eval_rougeLsum': 28.8763,\n",
       " 'eval_runtime': 8.5031,\n",
       " 'eval_samples_per_second': 58.802,\n",
       " 'eval_steps_per_second': 7.409,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "981f108a204f421f158e0977940335d851edffa6dd3586828a3e1aec045160e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('final': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
