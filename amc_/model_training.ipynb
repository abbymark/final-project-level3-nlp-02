{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabbymark\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/final_project/final-project-level3-nlp-02-amc_/runs/3g9cl7lw\" target=\"_blank\">wobbly-night-22</a></strong> to <a href=\"https://wandb.ai/final_project/final-project-level3-nlp-02-amc_\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/final_project/final-project-level3-nlp-02-amc_/runs/3g9cl7lw?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fca833abbb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "from knowledge_distillation import DistillationTrainingArguments, DistillationTrainer\n",
    "from huggingface_hub import notebook_login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(verbose=True)\n",
    "import torch\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append('/opt/ml/final-project-level3-nlp-02')\n",
    "from rouge import compute\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "wandb.init(\n",
    "    entity=\"final_project\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "train_size = 5000\n",
    "eval_size = 500\n",
    "check_point = 'encoder_decoder_pruned_last_5' #\"gogamza/kobart-summarization\"\n",
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 20\n",
    "learning_rate=5.6e-5\n",
    "weight_decay = 0.01\n",
    "logging_steps = 100\n",
    "model_name = check_point.split(\"/\")[-1]\n",
    "\n",
    "is_distillation = True\n",
    "if is_distillation:\n",
    "    student_check_point = \"encoder_decoder_pruned_last_5\"\n",
    "    teacher_check_point = \"kobart-summarization-finetuned-5000/checkpoint-1000\"\n",
    "    alpha=0.5\n",
    "    temperature = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset paper_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8104015b844df2887e8c87fa9e6b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api_token = os.getenv('HF_DATASET_API_TOKEN')\n",
    "dataset = datasets.load_dataset('metamong1/summarization_paper', use_auth_token=api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f/cache-93a7be41ce043bd9.arrow\n"
     ]
    }
   ],
   "source": [
    "training_dataset = dataset['train'].shuffle(seed=seed).select(range(train_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file encoder_decoder_pruned_last_5/added_tokens.json. We won't load it.\n",
      "loading file encoder_decoder_pruned_last_5/vocab.json\n",
      "loading file encoder_decoder_pruned_last_5/merges.txt\n",
      "loading file encoder_decoder_pruned_last_5/tokenizer.json\n",
      "loading file None\n",
      "loading file encoder_decoder_pruned_last_5/special_tokens_map.json\n",
      "loading file encoder_decoder_pruned_last_5/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "if is_distillation:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(student_check_point)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['text'], max_length=max_input_length, truncation = True, #padding=True\n",
    "    )\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['title'], max_length=max_target_length, truncation=True, #padding=True\n",
    "        )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f/cache-7e2fedc45ccdd79e.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = training_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /opt/ml/.cache/huggingface/datasets/metamong1___paper_summarization/Paper Summarization/1.4.0/24bb09528ebb04fdc6aafb6e110202e52fbb818c0f204839bc833d8ce1e86a5f/cache-6ade4e4681b6569a.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_eval_dataset = dataset['validation'].select(range(500)).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file encoder_decoder_pruned_last_5/config.json\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 1,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file encoder_decoder_pruned_last_5/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at encoder_decoder_pruned_last_5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file kobart-summarization-finetuned-5000/checkpoint-1000/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-summarization\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file kobart-summarization-finetuned-5000/checkpoint-1000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at kobart-summarization-finetuned-5000/checkpoint-1000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "if is_distillation:\n",
    "    student_model = AutoModelForSeq2SeqLM.from_pretrained(student_check_point, torch_dtype='auto').to(device)\n",
    "    teacher_model = AutoModelForSeq2SeqLM.from_pretrained(teacher_check_point, torch_dtype='auto').to(device)\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(check_point, torch_dtype='auto').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 100\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "if is_distillation:\n",
    "    args =  DistillationTrainingArguments(\n",
    "        output_dir=f'{student_check_point}-distilled-{train_size}',\n",
    "        evaluation_strategy = 'steps',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay = weight_decay,\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        predict_with_generate=True,\n",
    "        logging_steps=logging_steps,\n",
    "        alpha=alpha,\n",
    "        temperature=temperature,\n",
    "        report_to='wandb'\n",
    "    )\n",
    "else:\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'{model_name}-finetuned-{train_size}',\n",
    "        evaluation_strategy='steps', #'epoch',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay = weight_decay,\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        predict_with_generate=True,\n",
    "        logging_steps=logging_steps,\n",
    "        report_to='wandb'\n",
    "        # push_to_hub=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_distillation:\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=student_model)\n",
    "else:\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_distillation:\n",
    "    trainer = DistillationTrainer(\n",
    "        model=student_model,\n",
    "        args=args,\n",
    "        teacher_model = teacher_model,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "else:\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 28:42, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>27.458600</td>\n",
       "      <td>21.147156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>20.446500</td>\n",
       "      <td>18.383450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>18.593100</td>\n",
       "      <td>17.028332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>17.195700</td>\n",
       "      <td>16.156927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>16.396100</td>\n",
       "      <td>15.512665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>15.748700</td>\n",
       "      <td>15.080248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>15.266100</td>\n",
       "      <td>14.607510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>15.012600</td>\n",
       "      <td>14.232364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>14.725300</td>\n",
       "      <td>13.945302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>14.160100</td>\n",
       "      <td>13.730156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>13.783900</td>\n",
       "      <td>13.494742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>13.770800</td>\n",
       "      <td>13.288455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>13.480300</td>\n",
       "      <td>13.092805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>13.190300</td>\n",
       "      <td>12.917725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>12.925900</td>\n",
       "      <td>12.746019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>12.886100</td>\n",
       "      <td>12.586760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>12.838700</td>\n",
       "      <td>12.450572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>12.856100</td>\n",
       "      <td>12.322559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>12.491300</td>\n",
       "      <td>12.233555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>11.932100</td>\n",
       "      <td>12.093904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>12.006800</td>\n",
       "      <td>11.972324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>12.141100</td>\n",
       "      <td>11.894492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>12.130600</td>\n",
       "      <td>11.831212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>11.825800</td>\n",
       "      <td>11.695910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>11.945500</td>\n",
       "      <td>11.623077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>11.433300</td>\n",
       "      <td>11.516655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>11.434000</td>\n",
       "      <td>11.487365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>11.512100</td>\n",
       "      <td>11.383319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>11.359400</td>\n",
       "      <td>11.300793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>11.436600</td>\n",
       "      <td>11.260904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>11.263100</td>\n",
       "      <td>11.185894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>10.837100</td>\n",
       "      <td>11.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>10.741500</td>\n",
       "      <td>11.064891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>10.921300</td>\n",
       "      <td>11.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>10.906000</td>\n",
       "      <td>10.954356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>10.960100</td>\n",
       "      <td>10.895183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>10.889600</td>\n",
       "      <td>10.852366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>10.700800</td>\n",
       "      <td>10.827466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>10.564800</td>\n",
       "      <td>10.739545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>10.554200</td>\n",
       "      <td>10.691217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>10.317100</td>\n",
       "      <td>10.644283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>10.362600</td>\n",
       "      <td>10.623623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>10.508700</td>\n",
       "      <td>10.592093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>10.307800</td>\n",
       "      <td>10.510928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>10.286700</td>\n",
       "      <td>10.508284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>10.306800</td>\n",
       "      <td>10.472764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>10.054700</td>\n",
       "      <td>10.395716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>10.177500</td>\n",
       "      <td>10.362282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>9.916200</td>\n",
       "      <td>10.329776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>9.889200</td>\n",
       "      <td>10.290428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>9.787500</td>\n",
       "      <td>10.257422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>9.760400</td>\n",
       "      <td>10.230254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>9.882000</td>\n",
       "      <td>10.208309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>9.725700</td>\n",
       "      <td>10.163062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>9.840600</td>\n",
       "      <td>10.154012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>9.739500</td>\n",
       "      <td>10.141841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>9.651000</td>\n",
       "      <td>10.071937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>9.523000</td>\n",
       "      <td>10.038485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>9.639100</td>\n",
       "      <td>10.034967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>9.475600</td>\n",
       "      <td>10.006693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>9.690600</td>\n",
       "      <td>9.985101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>9.412200</td>\n",
       "      <td>9.956758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>9.340700</td>\n",
       "      <td>9.921002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>9.545800</td>\n",
       "      <td>9.897778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>8.986200</td>\n",
       "      <td>9.878127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>9.392200</td>\n",
       "      <td>9.862430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>9.316400</td>\n",
       "      <td>9.822002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>9.418600</td>\n",
       "      <td>9.815782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>9.072200</td>\n",
       "      <td>9.787528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>9.271800</td>\n",
       "      <td>9.766077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>9.104900</td>\n",
       "      <td>9.761782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>9.217600</td>\n",
       "      <td>9.731568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>9.128600</td>\n",
       "      <td>9.711179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>8.984900</td>\n",
       "      <td>9.716489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>8.995900</td>\n",
       "      <td>9.680938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>8.829400</td>\n",
       "      <td>9.666289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>8.978400</td>\n",
       "      <td>9.654094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>9.037000</td>\n",
       "      <td>9.621057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>9.033300</td>\n",
       "      <td>9.600580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>9.051700</td>\n",
       "      <td>9.609648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>8.726300</td>\n",
       "      <td>9.580028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>8.712300</td>\n",
       "      <td>9.566638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>8.776400</td>\n",
       "      <td>9.554599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>9.004100</td>\n",
       "      <td>9.541144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>8.811600</td>\n",
       "      <td>9.554483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>8.717400</td>\n",
       "      <td>9.521845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>8.735000</td>\n",
       "      <td>9.511415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>8.808900</td>\n",
       "      <td>9.485847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>8.740600</td>\n",
       "      <td>9.479703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>8.483500</td>\n",
       "      <td>9.461726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>8.724300</td>\n",
       "      <td>9.463690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>8.549300</td>\n",
       "      <td>9.447681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>8.690200</td>\n",
       "      <td>9.435905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>8.827300</td>\n",
       "      <td>9.430110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>8.529400</td>\n",
       "      <td>9.424263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>8.697300</td>\n",
       "      <td>9.414330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>8.443100</td>\n",
       "      <td>9.402142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>8.502600</td>\n",
       "      <td>9.390679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>8.691000</td>\n",
       "      <td>9.392873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>8.527100</td>\n",
       "      <td>9.378171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>8.508900</td>\n",
       "      <td>9.376945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>8.515600</td>\n",
       "      <td>9.374534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>8.393900</td>\n",
       "      <td>9.372554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>8.514000</td>\n",
       "      <td>9.345571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>8.448000</td>\n",
       "      <td>9.352119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>8.563400</td>\n",
       "      <td>9.349710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>8.350600</td>\n",
       "      <td>9.337098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>8.509400</td>\n",
       "      <td>9.325266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>8.420900</td>\n",
       "      <td>9.321308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>8.373000</td>\n",
       "      <td>9.323749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>8.394300</td>\n",
       "      <td>9.310096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>8.363200</td>\n",
       "      <td>9.308508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>8.557000</td>\n",
       "      <td>9.301717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>8.352400</td>\n",
       "      <td>9.302183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>8.328000</td>\n",
       "      <td>9.300239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>8.421900</td>\n",
       "      <td>9.287810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>8.423000</td>\n",
       "      <td>9.289732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>8.352400</td>\n",
       "      <td>9.293480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>8.394800</td>\n",
       "      <td>9.287724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>8.413900</td>\n",
       "      <td>9.290432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>8.351700</td>\n",
       "      <td>9.288575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>8.464800</td>\n",
       "      <td>9.281410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>8.374200</td>\n",
       "      <td>9.287661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>8.266900</td>\n",
       "      <td>9.282754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>8.329500</td>\n",
       "      <td>9.280494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-500\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-9500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-10500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12000\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12000/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: doc_id, text, doc_type, file, title.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12500\n",
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12500/config.json\n",
      "Model weights saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in encoder_decoder_pruned_last_5-distilled-5000/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [encoder_decoder_pruned_last_5-distilled-5000/checkpoint-11500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=10.382124545898437, metrics={'train_runtime': 1722.2156, 'train_samples_per_second': 58.065, 'train_steps_per_second': 7.258, 'total_flos': 4752394983751680.0, 'train_loss': 10.382124545898437, 'epoch': 20.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: title, file, doc_type, text, doc_id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.3818862438201904,\n",
       " 'eval_rouge1': 31.6557,\n",
       " 'eval_rouge2': 18.708,\n",
       " 'eval_rougeL': 28.8796,\n",
       " 'eval_rougeLsum': 28.8763,\n",
       " 'eval_runtime': 8.5031,\n",
       " 'eval_samples_per_second': 58.802,\n",
       " 'eval_steps_per_second': 7.409,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "981f108a204f421f158e0977940335d851edffa6dd3586828a3e1aec045160e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('final': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
