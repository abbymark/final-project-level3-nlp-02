{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.configuration_distilbert import DistilBertConfig\n",
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, RobertaTokenizerFast\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n",
    "from model_distilbert import DistilBertForSeq2Seq, DistilBertForConditionalGeneration\n",
    "\n",
    "from roberta_model import RobertaModelForSeq2Seq, RobertaForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_pretrained(\"monologg/distilkobert\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"monologg/distilkobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"하이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK] [CLS]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.decoder_start_token_id = torch.tensor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForConditionalGeneration(\"monologg/distilkobert\", config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '근대 경제학에서는 행위의 합리성 문제를 비용 대 편익이라는 효율성 측면에서 접근하는데, 이러한 사고방식은 경제학을 넘어 사회과학 전반으로 널리 확산되 었다. 하지만 경제적 논리로는 설명하기 어렵거나 바람직하지 않은 결과를 낳은 현상도 많이 존재한다. 특히 도덕이나 규범은 경제 현상의 작동 방식에도 영향을 미치기 때문에 이를 설명하는 확장된 경제 이론이 필요하다. 도덕경제론은 바로 이러한 문제를 다루는 접근 방식이다. 이에 따르면 경제 활동에서 개인의 효용 극대화와 합리적 선택을 강조하는 경제학의 기본 가정 역시 초시대적으로 통용 되는 가치가 아니라 자본주의 등장 이후에 형성된 역사적 현상에 불과하다. 경제 적 원칙은 당대의 도덕적 규범에 의해 구성되는 상대적인 가치이기 때문에, 경제 활동이나 영역과 관련된 규범적 측면을 고려해 재구성해야 한다. ‘도덕경제 (moral economy)’론은 바로 이처럼 경제 현상에서 규범이나 문화의 역할 문제 를 다루는 접근 방식이다. 이러한 시도는 경제와 관련된 사회 현상을 이해하는 데 에서 경제학적 접근 방식의 편협성과 한계를 해결하는 데에도 풍부한 시사점을 줄수있을것이다. 이논문은기존의도덕경제론에서다룬주요쟁점과개념을 소개하고, 이 논의가 미디어 경제를 이해하는 데 주는 함의, 쟁점 등을 검토한다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(text, return_tensors='pt')\n",
    "label = tokenizer('천하통일', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2af94bde9f88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final-project-level3-nlp-02/amc_/model/model_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         dlbrt_output = self.distilbert(\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         return self.transformer(\n\u001b[1;32m    552\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/opt/conda/envs/final/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "output = model(**input.to(device), labels=label['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n",
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(\"klue/roberta-small\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"klue/roberta-small\")\n",
    "\n",
    "model = RobertaForConditionalGeneration(\"klue/roberta-small\", config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-small\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '근대 경제학에서는 행위의 합리성 문제를 비용 대 편익이라는 효율성 측면에서 접근하는데, 이러한 사고방식은 경제학을 넘어 사회과학 전반으로 널리 확산되 었다. 하지만 경제적 논리로는 설명하기 어렵거나 바람직하지 않은 결과를 낳은 현상도 많이 존재한다. 특히 도덕이나 규범은 경제 현상의 작동 방식에도 영향을 미치기 때문에 이를 설명하는 확장된 경제 이론이 필요하다. 도덕경제론은 바로 이러한 문제를 다루는 접근 방식이다. 이에 따르면 경제 활동에서 개인의 효용 극대화와 합리적 선택을 강조하는 경제학의 기본 가정 역시 초시대적으로 통용 되는 가치가 아니라 자본주의 등장 이후에 형성된 역사적 현상에 불과하다. 경제 적 원칙은 당대의 도덕적 규범에 의해 구성되는 상대적인 가치이기 때문에, 경제 활동이나 영역과 관련된 규범적 측면을 고려해 재구성해야 한다. ‘도덕경제 (moral economy)’론은 바로 이처럼 경제 현상에서 규범이나 문화의 역할 문제 를 다루는 접근 방식이다. 이러한 시도는 경제와 관련된 사회 현상을 이해하는 데 에서 경제학적 접근 방식의 편협성과 한계를 해결하는 데에도 풍부한 시사점을 줄수있을것이다. 이논문은기존의도덕경제론에서다룬주요쟁점과개념을 소개하고, 이 논의가 미디어 경제를 이해하는 데 주는 함의, 쟁점 등을 검토한다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(text, return_tensors='pt')\n",
    "label = tokenizer('천하통일', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**input.to(device), labels=label['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(11.1898, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-1.1316, -0.6741, -0.1176,  ...,  0.0252,  0.4225, -0.0943],\n",
       "         [-0.7986, -0.1373, -0.0905,  ...,  0.1245, -0.1410, -0.3085],\n",
       "         [-0.7354,  0.0161, -0.1842,  ...,  0.0168, -0.3928, -0.2734],\n",
       "         [-0.7605, -0.0490, -0.2677,  ...,  0.0638, -0.3127, -0.2499]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), past_key_values=((tensor([[[[ 1.2309,  0.2302,  0.3045,  ..., -1.6170, -1.3731,  1.7152],\n",
       "          [-0.0186,  0.4162,  0.2413,  ..., -1.3862, -0.3282,  1.0099],\n",
       "          [ 1.6610,  0.2384,  0.3050,  ...,  0.7553, -0.0918, -1.1111],\n",
       "          [ 0.9052, -0.9485, -0.3875,  ...,  0.2217,  0.3591, -0.6301]],\n",
       "\n",
       "         [[-1.9201, -0.6689,  1.2949,  ..., -3.3846,  0.0695,  4.0560],\n",
       "          [-2.5114, -0.7484,  0.5283,  ..., -1.2289,  2.0754,  1.6583],\n",
       "          [-0.3675, -1.0694,  1.2907,  ...,  0.2696,  1.8481,  0.8961],\n",
       "          [-1.1344, -0.9662, -0.6251,  ...,  1.1545,  2.0255,  1.7402]],\n",
       "\n",
       "         [[ 0.4102,  0.6262, -0.7856,  ..., -1.5897,  0.5552, -0.1272],\n",
       "          [-0.2777,  0.2524, -0.8994,  ..., -1.4645, -0.7536, -0.1674],\n",
       "          [-0.4956, -0.5203,  0.0926,  ..., -0.1172,  0.6466,  0.6774],\n",
       "          [-0.0372, -0.9630, -0.5630,  ...,  0.2896,  1.4144,  0.8724]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.8427, -1.7080,  5.7872,  ...,  1.5550,  5.1110, -0.6619],\n",
       "          [-2.1709,  0.1253,  1.8930,  ...,  2.6690,  0.8371, -2.4501],\n",
       "          [ 0.1273,  0.1880,  2.2418,  ...,  2.2846,  0.9756,  0.1567],\n",
       "          [ 1.0017,  2.6379,  1.9884,  ...,  1.7095,  1.7142,  0.6892]],\n",
       "\n",
       "         [[ 2.0980, -0.1006, -0.4153,  ...,  1.0739, -3.1043, -3.4297],\n",
       "          [ 1.0317,  1.1332, -2.1129,  ...,  1.7547, -0.7274, -5.8334],\n",
       "          [-1.8438,  0.3181, -4.7733,  ...,  1.9169, -0.7116, -2.9982],\n",
       "          [ 0.4891,  0.9667, -0.5309,  ..., -1.0244, -1.3261, -1.8802]],\n",
       "\n",
       "         [[-1.5945, -0.7819, -2.3943,  ...,  2.0263,  0.0797, -2.5990],\n",
       "          [-1.6982, -1.0190, -1.7548,  ..., -0.0761,  1.2698, -0.0868],\n",
       "          [-1.2898,  0.2428, -0.3339,  ..., -1.8124,  0.1692, -0.5685],\n",
       "          [-1.8051, -0.0086, -1.1126,  ..., -1.5723, -0.9536, -1.0575]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 1.5375, -0.1907,  0.4014,  ...,  0.5156,  1.2765, -1.0430],\n",
       "          [ 0.5268, -0.5939, -1.2363,  ...,  0.1746,  0.4647,  0.5322],\n",
       "          [-0.2908, -0.9832, -1.9985,  ..., -0.4475,  0.8271, -1.1460],\n",
       "          [-0.0772, -0.0796, -1.4734,  ...,  0.9517,  0.3957,  0.6727]],\n",
       "\n",
       "         [[ 0.0687,  0.1613, -0.1163,  ..., -0.1444,  0.0366,  0.0123],\n",
       "          [ 0.1246,  0.1963, -0.2238,  ...,  0.1707,  0.1359,  0.6152],\n",
       "          [-0.1434,  1.1003, -0.5151,  ...,  0.0716,  0.1257, -0.4790],\n",
       "          [ 0.6748,  0.0907, -0.0101,  ..., -0.4767,  0.1186, -0.5226]],\n",
       "\n",
       "         [[ 0.1290, -1.1605, -0.3138,  ..., -1.2174,  0.4121,  1.2980],\n",
       "          [ 0.2026,  0.5878,  0.1431,  ..., -0.4159, -0.9954,  0.2933],\n",
       "          [-2.2716, -0.2602, -0.3064,  ...,  0.1609,  0.0323,  0.6253],\n",
       "          [-0.7747, -0.8216, -0.1294,  ..., -0.5896, -1.1208, -0.7013]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0215,  0.2335, -0.2334,  ...,  0.0264, -0.2246, -0.0906],\n",
       "          [ 0.1171,  0.1341,  0.2773,  ...,  0.5320, -0.4147, -0.3077],\n",
       "          [-0.6864,  0.2954,  0.0520,  ...,  0.1074, -0.9105, -0.6044],\n",
       "          [ 0.3207, -0.4225, -0.5003,  ..., -0.1414, -0.2520, -0.7920]],\n",
       "\n",
       "         [[ 0.0146,  0.0898,  0.1580,  ...,  0.0915, -0.5505,  0.8879],\n",
       "          [-0.0259, -0.0962, -0.0415,  ..., -0.4467,  0.1055,  0.7989],\n",
       "          [-0.1886, -0.0797, -0.2053,  ..., -0.0159, -0.5989,  0.3078],\n",
       "          [-0.5415, -0.4575, -0.4193,  ...,  0.0756,  0.2127,  0.1423]],\n",
       "\n",
       "         [[-0.1945, -0.8494,  0.1626,  ..., -0.2643,  0.3490, -0.4096],\n",
       "          [ 0.6024, -0.2630, -0.4052,  ...,  0.6781,  0.4265,  0.5632],\n",
       "          [ 0.4456,  0.6349, -0.5397,  ...,  0.3512,  0.8115,  0.2770],\n",
       "          [-0.8788,  0.5644, -0.3316,  ...,  0.4073,  0.2289,  0.3722]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 0.4416,  0.0241, -0.1985,  ...,  0.0699,  0.0775, -0.2306],\n",
       "          [ 0.1566, -0.1396, -0.0403,  ..., -0.0415,  0.0101, -0.3622],\n",
       "          [ 0.0959, -0.3277, -0.0288,  ...,  0.0322, -0.3565, -0.6410],\n",
       "          ...,\n",
       "          [ 0.1088, -0.1025, -0.1824,  ...,  0.1028, -0.1436, -0.2043],\n",
       "          [ 0.3366, -0.1987, -0.1642,  ...,  0.0527, -0.1943, -0.2740],\n",
       "          [ 0.4421,  0.0287, -0.1979,  ...,  0.0710,  0.0764, -0.2278]],\n",
       "\n",
       "         [[-0.1764,  0.1040,  0.1761,  ...,  0.2088,  0.1342,  0.0655],\n",
       "          [ 0.0764,  0.2651,  0.2096,  ...,  0.1133, -0.1177,  0.0815],\n",
       "          [-0.3373, -0.1116,  0.0969,  ..., -0.0478,  0.1688,  0.3739],\n",
       "          ...,\n",
       "          [-0.0798, -0.1321,  0.2153,  ..., -0.3666, -0.0707,  0.0703],\n",
       "          [ 0.1520,  0.0882, -0.0601,  ...,  0.2839, -0.1175,  0.2505],\n",
       "          [-0.1748,  0.1076,  0.1759,  ...,  0.2094,  0.1332,  0.0675]],\n",
       "\n",
       "         [[-0.2085, -0.4309,  0.2080,  ...,  0.0156,  0.0010,  0.0391],\n",
       "          [-0.2200, -0.1957, -0.1615,  ..., -0.0898, -0.1470, -0.0902],\n",
       "          [-0.2568, -0.1444, -0.0917,  ...,  0.1718, -0.4736,  0.1146],\n",
       "          ...,\n",
       "          [ 0.0414,  0.1345,  0.4938,  ...,  0.2586, -0.1265,  0.2233],\n",
       "          [-0.3572, -0.0822,  0.3733,  ...,  0.2573, -0.1931, -0.1371],\n",
       "          [-0.2064, -0.4315,  0.2078,  ...,  0.0165,  0.0046,  0.0385]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3179, -0.0375,  0.4808,  ..., -0.2057, -0.0768,  0.2209],\n",
       "          [-0.3145,  0.0707,  0.1835,  ...,  0.0270, -0.1682, -0.0973],\n",
       "          [ 0.0679,  0.0569,  0.1939,  ..., -0.0953, -0.0647,  0.3440],\n",
       "          ...,\n",
       "          [-0.1673, -0.0607, -0.0124,  ...,  0.1370, -0.2070,  0.1142],\n",
       "          [-0.4369, -0.0402,  0.2856,  ..., -0.1350, -0.0223,  0.0718],\n",
       "          [-0.3192, -0.0391,  0.4807,  ..., -0.2062, -0.0722,  0.2184]],\n",
       "\n",
       "         [[-0.0968,  0.0229,  0.2737,  ..., -0.2302,  0.0287, -0.2533],\n",
       "          [-0.1719, -0.2156,  0.4812,  ..., -0.0353,  0.1086,  0.0054],\n",
       "          [-0.1856,  0.2445,  0.0980,  ...,  0.3693, -0.1578,  0.2236],\n",
       "          ...,\n",
       "          [ 0.1732, -0.0763,  0.1274,  ..., -0.2008, -0.1729,  0.2097],\n",
       "          [ 0.0393,  0.0732,  0.1679,  ...,  0.2006, -0.2699,  0.1673],\n",
       "          [-0.0961,  0.0212,  0.2706,  ..., -0.2296,  0.0278, -0.2502]],\n",
       "\n",
       "         [[-0.0755, -0.0207,  0.1061,  ...,  0.1320, -0.1449,  0.4551],\n",
       "          [-0.1330, -0.1662,  0.1322,  ...,  0.2096, -0.0725,  0.2797],\n",
       "          [-0.2382,  0.0443,  0.0247,  ...,  0.0425, -0.2513,  0.2744],\n",
       "          ...,\n",
       "          [-0.0756,  0.4951,  0.0770,  ...,  0.2133, -0.3127,  0.4277],\n",
       "          [ 0.3368, -0.1178,  0.0933,  ...,  0.0796, -0.0810,  0.4774],\n",
       "          [-0.0749, -0.0176,  0.1085,  ...,  0.1290, -0.1461,  0.4538]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-0.3164, -0.1340,  0.0863,  ..., -0.1149, -0.0355,  0.1126],\n",
       "          [-0.4070, -0.0138, -0.2405,  ..., -0.0824,  0.3083, -0.1885],\n",
       "          [-0.6661,  0.1359,  0.2435,  ..., -0.2335, -0.0705,  0.0687],\n",
       "          ...,\n",
       "          [-0.5689, -0.1250,  0.1713,  ..., -0.3050,  0.1190,  0.4640],\n",
       "          [-0.3848,  0.0049, -0.0121,  ...,  0.0375,  0.1409,  0.3193],\n",
       "          [-0.3157, -0.1306,  0.0878,  ..., -0.1127, -0.0359,  0.1118]],\n",
       "\n",
       "         [[-0.1137,  0.2543,  0.0540,  ...,  0.0965, -0.1501, -0.1394],\n",
       "          [ 0.0623,  0.0875, -0.1315,  ...,  0.1079, -0.0241, -0.2936],\n",
       "          [-0.4007,  0.2783,  0.2709,  ..., -0.0527,  0.1177, -0.1078],\n",
       "          ...,\n",
       "          [-0.6098, -0.0727, -0.0472,  ...,  0.0499, -0.0424, -0.0451],\n",
       "          [-0.4142,  0.4525,  0.0016,  ...,  0.2230,  0.0827,  0.0480],\n",
       "          [-0.1155,  0.2572,  0.0528,  ...,  0.0954, -0.1482, -0.1395]],\n",
       "\n",
       "         [[ 0.0066,  0.0532,  0.5804,  ...,  0.3303, -0.0533, -0.0580],\n",
       "          [-0.0436, -0.0537,  0.2768,  ...,  0.1107, -0.0118, -0.0019],\n",
       "          [-0.3071,  0.1595,  0.4335,  ...,  0.2787, -0.2439,  0.0343],\n",
       "          ...,\n",
       "          [-0.0512, -0.5686,  0.3996,  ..., -0.2176, -0.3550,  0.0377],\n",
       "          [-0.1778,  0.0543,  0.2828,  ...,  0.2627, -0.2408, -0.3004],\n",
       "          [ 0.0091,  0.0565,  0.5780,  ...,  0.3294, -0.0539, -0.0595]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1299,  0.3052,  0.4750,  ..., -0.0309,  0.2075, -0.1468],\n",
       "          [ 0.0747,  0.2868,  0.2164,  ..., -0.1409,  0.3106,  0.1490],\n",
       "          [-0.0812, -0.0806,  0.0758,  ..., -0.5665,  0.5469,  0.1781],\n",
       "          ...,\n",
       "          [-0.1333, -0.0049,  0.3281,  ...,  0.2686, -0.3194, -0.4926],\n",
       "          [-0.0826,  0.0098,  0.3642,  ...,  0.3577, -0.0351, -0.4316],\n",
       "          [-0.1289,  0.3043,  0.4745,  ..., -0.0313,  0.2060, -0.1480]],\n",
       "\n",
       "         [[ 0.3996,  0.0017, -0.0766,  ...,  0.2565, -0.2265, -0.1487],\n",
       "          [ 0.2090, -0.0980,  0.1523,  ...,  0.0406, -0.1613, -0.2125],\n",
       "          [ 0.3544, -0.1835,  0.1673,  ..., -0.2182,  0.0051, -0.3363],\n",
       "          ...,\n",
       "          [ 0.2123,  0.2146, -0.0378,  ..., -0.0341,  0.2304,  0.0975],\n",
       "          [-0.0016, -0.1516,  0.2403,  ..., -0.0705, -0.3385, -0.0851],\n",
       "          [ 0.3989,  0.0020, -0.0771,  ...,  0.2566, -0.2268, -0.1467]],\n",
       "\n",
       "         [[ 0.3002,  0.0018, -0.1651,  ...,  0.1039,  0.1661,  0.0465],\n",
       "          [ 0.5737,  0.4238,  0.0706,  ...,  0.1732,  0.4654, -0.0026],\n",
       "          [-0.0531,  0.0933, -0.3054,  ..., -0.1153,  0.0138, -0.0691],\n",
       "          ...,\n",
       "          [-0.0821,  0.2380,  0.2554,  ..., -0.0466, -0.0569, -0.0362],\n",
       "          [ 0.3625,  0.2382,  0.0397,  ..., -0.1586,  0.1062,  0.1737],\n",
       "          [ 0.2967,  0.0050, -0.1652,  ...,  0.1045,  0.1650,  0.0469]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.9135, -0.9387, -3.5475,  ..., -1.5277,  0.5890,  0.3007],\n",
       "          [-1.3208, -0.9288,  0.7618,  ..., -0.1063,  0.7479,  0.2327],\n",
       "          [-0.3287, -2.2094,  0.7664,  ...,  0.5652, -0.1067, -0.0704],\n",
       "          [-2.3675, -1.2022,  0.9788,  ..., -1.0945,  0.6536, -0.5532]],\n",
       "\n",
       "         [[-1.8041,  1.8558,  1.6656,  ...,  3.2071, -1.3294, -1.6843],\n",
       "          [ 0.0513,  0.4140,  0.7021,  ...,  1.7871,  1.3927, -0.6297],\n",
       "          [ 1.6492,  0.3005,  0.2224,  ...,  0.3939,  1.7524,  0.9173],\n",
       "          [ 1.2244, -0.0554, -0.4323,  ...,  0.6693,  0.2737,  0.2454]],\n",
       "\n",
       "         [[ 0.9837, -0.9797,  0.7473,  ...,  0.3407, -1.4541,  0.3033],\n",
       "          [-0.1562, -0.4158,  0.3731,  ...,  0.8560,  0.9706,  1.3073],\n",
       "          [-0.5626,  0.3587,  0.0364,  ...,  0.5571,  0.4626,  1.3475],\n",
       "          [-1.1398, -0.4687, -1.1190,  ..., -0.7836,  0.2712,  0.9117]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.8684, -1.8220,  1.4319,  ...,  2.5796,  1.4176, -0.7923],\n",
       "          [ 0.3416,  0.1505,  0.5849,  ..., -0.6385, -1.4689, -1.4476],\n",
       "          [ 0.0590, -0.0473,  2.1855,  ...,  0.5551,  0.4661, -2.0958],\n",
       "          [ 0.6807,  1.0321,  2.7010,  ...,  0.4802,  0.2377,  0.2182]],\n",
       "\n",
       "         [[ 1.1435, -1.4363,  0.6490,  ..., -3.1455,  1.0874, -0.9354],\n",
       "          [ 1.0994,  1.1123, -0.7119,  ...,  0.7490, -0.2219,  1.5165],\n",
       "          [ 0.5716,  1.7438, -0.7642,  ..., -0.7236,  0.1859,  0.8057],\n",
       "          [-0.5313,  1.9112, -0.9014,  ..., -0.0653,  0.1941,  1.3694]],\n",
       "\n",
       "         [[ 0.3824,  0.4528, -1.4294,  ...,  0.3979,  1.2264,  0.7688],\n",
       "          [ 0.5528,  2.2548, -0.9279,  ..., -1.1200,  0.8428,  0.3795],\n",
       "          [ 1.6117,  1.9354, -1.6346,  ...,  0.2489, -0.1728,  0.2040],\n",
       "          [ 0.9570,  0.2378, -3.3023,  ...,  0.8251, -1.3805,  0.5018]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-0.1038, -0.8705, -0.2985,  ..., -0.6875,  0.5023, -0.2492],\n",
       "          [-0.6587,  0.4977,  0.0306,  ...,  0.8538, -0.2236, -0.5632],\n",
       "          [-0.8623, -0.8527, -0.2775,  ...,  1.7940,  0.9258, -0.1678],\n",
       "          [-0.4672, -0.2760, -0.7857,  ...,  0.9194,  0.2255,  0.6751]],\n",
       "\n",
       "         [[-0.0098,  0.0147, -0.0188,  ...,  0.0475,  0.0799,  0.1322],\n",
       "          [ 0.0066, -0.0323,  0.7784,  ..., -0.3305,  0.7576, -0.3452],\n",
       "          [-0.3871,  0.0502, -0.2708,  ..., -0.4348,  1.7230,  0.0045],\n",
       "          [-0.4199, -0.1652, -0.6024,  ...,  0.7824,  0.7673, -0.9120]],\n",
       "\n",
       "         [[-0.0536,  0.2020, -0.1260,  ...,  0.1401, -0.0832, -0.2096],\n",
       "          [ 0.4988,  0.1816,  0.2637,  ..., -1.0311, -1.0298, -0.9298],\n",
       "          [-0.2245, -0.2589,  0.1066,  ...,  0.0746, -2.0236, -0.8985],\n",
       "          [-0.3824, -0.6225, -1.0047,  ...,  2.1376, -0.6896, -1.4397]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2938, -0.8143,  0.4292,  ...,  0.0771,  0.5914,  0.0209],\n",
       "          [ 1.0696,  1.3649,  0.3508,  ..., -0.7210, -0.4266,  0.5878],\n",
       "          [ 1.3387,  0.1295, -0.5348,  ..., -0.2907, -0.1323,  1.2690],\n",
       "          [ 0.6642,  0.3434, -0.1754,  ..., -0.2204, -0.5991,  0.5086]],\n",
       "\n",
       "         [[-0.2530,  0.0876, -0.0700,  ..., -0.2548,  0.3052,  0.1755],\n",
       "          [ 0.4588,  1.4319,  0.4319,  ..., -0.9378, -0.9571, -0.5153],\n",
       "          [ 0.0612,  1.4597,  0.5017,  ..., -0.2866, -0.2665, -0.4952],\n",
       "          [-0.5396,  1.3088,  0.5107,  ..., -0.1309, -0.7301,  0.6640]],\n",
       "\n",
       "         [[-0.1127,  0.0061, -0.1323,  ..., -0.0744, -0.1386,  0.0113],\n",
       "          [-1.5894, -0.5290, -0.8056,  ..., -0.6629,  0.8923, -0.5959],\n",
       "          [-1.2633, -0.4712, -0.1232,  ..., -0.2963,  1.4044, -0.9607],\n",
       "          [-1.2485, -0.5319, -1.0015,  ..., -0.5990,  1.1378, -0.4951]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 0.2909,  0.2014,  0.0599,  ...,  0.2818, -0.3956,  0.0382],\n",
       "          [ 0.5136,  0.3866, -0.7401,  ...,  0.1868, -0.5505, -0.1411],\n",
       "          [ 0.0456, -0.1855, -0.1120,  ..., -0.0746, -0.5092,  0.2182],\n",
       "          ...,\n",
       "          [ 0.1663,  0.1499,  0.0345,  ..., -0.3081, -0.5019,  0.1476],\n",
       "          [ 0.3383,  0.2046, -0.2155,  ...,  0.4138, -0.5414,  0.0351],\n",
       "          [ 0.2900,  0.2015,  0.0596,  ...,  0.2809, -0.3964,  0.0404]],\n",
       "\n",
       "         [[ 0.1559,  0.2348,  0.2502,  ..., -0.2397,  0.1605,  0.1703],\n",
       "          [ 0.0246,  0.4304,  0.1027,  ..., -0.1172,  0.1049,  0.1989],\n",
       "          [-0.1631,  0.3683, -0.2503,  ..., -0.3109,  0.2542,  0.1800],\n",
       "          ...,\n",
       "          [-0.0338,  0.3005,  0.0983,  ..., -0.3462,  0.2315, -0.1676],\n",
       "          [ 0.0662,  0.1203,  0.4320,  ..., -0.1439, -0.0302,  0.0613],\n",
       "          [ 0.1541,  0.2332,  0.2443,  ..., -0.2409,  0.1603,  0.1684]],\n",
       "\n",
       "         [[-0.4284, -0.0535, -0.2064,  ..., -0.0321,  0.0880,  0.5086],\n",
       "          [ 0.0696, -0.0756,  0.1734,  ..., -0.2145, -0.0238,  0.5205],\n",
       "          [-0.3162, -0.2767, -0.0514,  ..., -0.2428,  0.3798,  0.2504],\n",
       "          ...,\n",
       "          [-0.4212, -0.2324, -0.3558,  ..., -0.2865,  0.1322,  0.3945],\n",
       "          [-0.0056,  0.2336, -0.0995,  ..., -0.1583,  0.0710,  0.4140],\n",
       "          [-0.4283, -0.0552, -0.2093,  ..., -0.0348,  0.0890,  0.5080]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1803, -0.2244, -0.2544,  ..., -0.0560, -0.2097, -0.1481],\n",
       "          [ 0.1186, -0.0018, -0.3258,  ..., -0.0548, -0.0431, -0.1251],\n",
       "          [ 0.0189, -0.2261, -0.3482,  ..., -0.0484,  0.1962, -0.1484],\n",
       "          ...,\n",
       "          [-0.3955, -0.3662, -0.1621,  ..., -0.0138, -0.1602,  0.0772],\n",
       "          [ 0.2221, -0.1779, -0.2884,  ..., -0.1546, -0.2488, -0.0688],\n",
       "          [-0.1807, -0.2247, -0.2548,  ..., -0.0555, -0.2094, -0.1501]],\n",
       "\n",
       "         [[-0.4453,  0.1571,  0.1067,  ...,  0.2782,  0.6039, -0.5677],\n",
       "          [-0.0928, -0.0303,  0.1325,  ...,  0.2224,  0.4102, -0.1842],\n",
       "          [-0.0305, -0.0562, -0.1937,  ...,  0.3499,  0.3550, -0.3911],\n",
       "          ...,\n",
       "          [-0.6434,  0.0269, -0.1706,  ..., -0.1472,  0.5323, -0.0286],\n",
       "          [-0.6039, -0.0782,  0.0923,  ...,  0.2764,  0.4780, -0.3577],\n",
       "          [-0.4441,  0.1582,  0.1046,  ...,  0.2798,  0.6049, -0.5665]],\n",
       "\n",
       "         [[-0.0651, -0.4388, -0.3252,  ..., -0.1100, -0.2744,  0.1069],\n",
       "          [ 0.1957, -0.6594, -0.6039,  ..., -0.1181, -0.1568, -0.3439],\n",
       "          [ 0.1534, -0.5041, -0.2880,  ..., -0.3621,  0.1346, -0.2816],\n",
       "          ...,\n",
       "          [ 0.0178,  0.1469,  0.0551,  ...,  0.3298,  0.0982,  0.0239],\n",
       "          [ 0.0438, -0.2356, -0.4569,  ...,  0.0801, -0.2436,  0.2000],\n",
       "          [-0.0647, -0.4376, -0.3260,  ..., -0.1100, -0.2733,  0.1060]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-0.2614, -0.2996, -0.2860,  ..., -0.0342, -0.0706,  0.0450],\n",
       "          [-0.1001,  0.1356, -0.3032,  ...,  0.0864,  0.0388,  0.2266],\n",
       "          [ 0.0016,  0.2221, -0.0949,  ...,  0.3797,  0.0399,  0.2676],\n",
       "          ...,\n",
       "          [ 0.1001, -0.3631,  0.2597,  ..., -0.4770,  0.0164, -0.1750],\n",
       "          [-0.3871, -0.0236, -0.3936,  ..., -0.1525,  0.0312, -0.0551],\n",
       "          [-0.2607, -0.3009, -0.2864,  ..., -0.0347, -0.0699,  0.0438]],\n",
       "\n",
       "         [[-0.2869, -0.6127, -0.0424,  ...,  0.0677,  0.1045,  0.0644],\n",
       "          [-0.1508, -0.2449,  0.0602,  ...,  0.0490, -0.3072, -0.1668],\n",
       "          [-0.1980,  0.0907, -0.0012,  ...,  0.5732,  0.2752, -0.0676],\n",
       "          ...,\n",
       "          [ 0.0477, -0.3787,  0.0129,  ...,  0.3357, -0.1016,  0.2447],\n",
       "          [ 0.2831, -0.3579,  0.2002,  ..., -0.0195, -0.0964,  0.0011],\n",
       "          [-0.2876, -0.6110, -0.0434,  ...,  0.0667,  0.1040,  0.0638]],\n",
       "\n",
       "         [[ 0.3105,  0.2010, -0.4071,  ..., -0.1992, -0.0487,  0.3049],\n",
       "          [ 0.3579,  0.3667, -0.2605,  ..., -0.1254,  0.1912,  0.5332],\n",
       "          [-0.0840,  0.0990, -0.3351,  ...,  0.0617,  0.0526,  0.3152],\n",
       "          ...,\n",
       "          [-0.0866,  0.1327, -0.2876,  ..., -0.0875, -0.1738,  0.2044],\n",
       "          [-0.1121,  0.2827, -0.0921,  ..., -0.1540, -0.0725,  0.2388],\n",
       "          [ 0.3116,  0.2019, -0.4066,  ..., -0.1996, -0.0463,  0.3028]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1587,  0.4253, -0.2345,  ...,  0.4982, -0.2013, -0.3595],\n",
       "          [ 0.0447,  0.2708,  0.0014,  ...,  0.3561,  0.1563, -0.1917],\n",
       "          [ 0.1060,  0.4449, -0.0818,  ...,  0.0247,  0.1228, -0.1155],\n",
       "          ...,\n",
       "          [-0.0746,  0.2025, -0.2935,  ..., -0.0448, -0.0049, -0.5002],\n",
       "          [-0.1282,  0.0423, -0.1728,  ...,  0.3546, -0.0653, -0.0854],\n",
       "          [-0.1583,  0.4238, -0.2361,  ...,  0.4959, -0.1998, -0.3612]],\n",
       "\n",
       "         [[ 0.1649,  0.0367, -0.0539,  ...,  0.1034,  0.2256,  0.2042],\n",
       "          [-0.0553, -0.0893,  0.0601,  ...,  0.1244,  0.4881,  0.1613],\n",
       "          [-0.2180,  0.2302, -0.3171,  ..., -0.1274, -0.0463,  0.2921],\n",
       "          ...,\n",
       "          [ 0.2759,  0.1786, -0.2233,  ...,  0.3960, -0.0515,  0.0527],\n",
       "          [ 0.2525,  0.0662, -0.1193,  ...,  0.2122,  0.4196,  0.1535],\n",
       "          [ 0.1664,  0.0363, -0.0539,  ...,  0.1053,  0.2259,  0.2058]],\n",
       "\n",
       "         [[ 0.1362, -0.2365,  0.0302,  ...,  0.4669,  0.3575, -0.0579],\n",
       "          [ 0.0889,  0.1015,  0.1020,  ...,  0.2862, -0.0717, -0.2186],\n",
       "          [-0.1530, -0.0142,  0.1777,  ...,  0.0676,  0.0937, -0.3290],\n",
       "          ...,\n",
       "          [-0.1390, -0.3891, -0.2375,  ...,  0.0636, -0.1232, -0.1189],\n",
       "          [ 0.2254, -0.3498, -0.0236,  ...,  0.5136,  0.1648, -0.1052],\n",
       "          [ 0.1346, -0.2356,  0.0304,  ...,  0.4674,  0.3566, -0.0580]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[-0.6842,  1.1355, -0.4053,  ..., -0.0226,  1.3675, -0.1622],\n",
       "          [-0.6445, -0.1534,  0.3737,  ...,  0.2813,  0.0856,  0.5842],\n",
       "          [-0.0336, -0.0538,  1.0467,  ...,  0.3514,  1.3699,  0.8544],\n",
       "          [ 0.2521, -0.3982,  0.5389,  ..., -0.5178,  0.3747,  0.8285]],\n",
       "\n",
       "         [[-0.2132,  0.8500, -0.1335,  ...,  0.4316, -0.6800,  1.2208],\n",
       "          [-1.3068, -0.0436,  0.9236,  ..., -0.0851, -1.8448,  1.6811],\n",
       "          [-1.3418,  0.3961,  0.3714,  ..., -1.1578, -0.6680,  1.1057],\n",
       "          [-0.7311,  0.8942, -0.3020,  ..., -1.8204, -0.5151,  0.2336]],\n",
       "\n",
       "         [[ 0.4262, -0.7863, -0.8597,  ...,  0.2706, -1.1632,  2.4274],\n",
       "          [ 0.0829, -0.3592, -0.8823,  ..., -0.1388, -0.7638,  0.2762],\n",
       "          [ 1.3319,  0.1000, -0.0747,  ..., -0.2859, -0.8690,  0.3369],\n",
       "          [ 0.3256,  0.3173,  0.1178,  ...,  0.4021,  0.2487, -0.3924]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.4736, -0.2812, -1.3835,  ..., -0.3430,  0.4464,  1.2933],\n",
       "          [-0.3674, -0.9237, -0.3952,  ..., -0.8213,  0.2772,  1.3229],\n",
       "          [-0.0172, -0.5662,  0.3885,  ...,  0.0992,  0.3894,  0.7277],\n",
       "          [-1.1646, -0.7250, -0.4518,  ..., -0.6690,  0.3092,  0.6748]],\n",
       "\n",
       "         [[ 1.4200, -0.9949, -0.7186,  ..., -0.5833,  2.3505, -2.8260],\n",
       "          [ 0.7946,  0.2002, -0.3508,  ..., -0.1213, -0.2674, -1.1993],\n",
       "          [ 0.2454,  0.0857, -1.1009,  ..., -1.3397,  0.0790,  0.6791],\n",
       "          [-0.6901,  1.3349, -0.0133,  ..., -0.6664,  1.4046,  0.4424]],\n",
       "\n",
       "         [[-0.1895, -0.0402, -0.8574,  ...,  0.9264, -0.4332,  0.1490],\n",
       "          [ 0.5864,  0.2489, -1.5822,  ...,  0.7712,  1.3689,  0.5265],\n",
       "          [-0.4425, -0.1388, -1.0447,  ...,  0.6341,  1.8458,  1.0633],\n",
       "          [ 0.2633, -0.9001, -1.8305,  ...,  0.9203, -0.2887,  0.9531]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 0.0611, -0.1060,  0.0412,  ..., -0.0614,  0.2130,  0.1193],\n",
       "          [-1.3925, -0.4103,  0.6239,  ..., -0.1365,  0.4306,  0.2483],\n",
       "          [-0.0866, -0.6266, -1.0326,  ..., -0.3059,  0.8442, -0.8418],\n",
       "          [-0.1482, -0.8953, -1.5727,  ..., -1.3455,  1.0076, -0.5048]],\n",
       "\n",
       "         [[ 0.0028,  0.0902, -0.0741,  ..., -0.1389,  0.2183, -0.1873],\n",
       "          [ 0.0291,  0.0761,  0.0717,  ...,  0.1526, -0.4152,  0.2806],\n",
       "          [ 0.5268, -0.3049,  0.0129,  ..., -0.3693,  0.6131,  0.6242],\n",
       "          [ 0.1848, -0.1303, -0.7384,  ..., -0.7316,  1.3282, -0.2311]],\n",
       "\n",
       "         [[ 0.1052, -0.0420, -0.0955,  ...,  0.2988,  0.1381, -0.0812],\n",
       "          [ 0.4827, -0.2018, -0.4724,  ...,  0.2779,  0.1514, -0.0921],\n",
       "          [ 0.1244,  0.1978, -0.6971,  ...,  0.8247,  1.1339, -0.7124],\n",
       "          [-0.6860, -0.0679,  0.1683,  ..., -0.1925,  1.1747,  0.8071]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0067, -0.0161,  0.1990,  ...,  0.1243,  0.0263, -0.1292],\n",
       "          [ 0.0478, -0.2325,  0.8253,  ..., -0.3150,  0.2252,  0.2246],\n",
       "          [-0.7771, -0.1477,  0.1774,  ..., -0.5733, -0.3900, -0.6244],\n",
       "          [-0.2401, -0.5452,  0.5236,  ...,  0.1761, -0.3792, -0.1833]],\n",
       "\n",
       "         [[-0.3011, -0.1175, -0.3897,  ...,  0.0763,  0.1168, -0.2778],\n",
       "          [-0.0493, -0.1082, -1.3709,  ..., -0.6771,  0.6097, -0.2601],\n",
       "          [-0.1409,  0.3575,  0.2633,  ..., -0.8816, -0.0148,  0.2051],\n",
       "          [ 0.3354, -0.8372, -0.1079,  ..., -0.4995,  0.6180,  0.5538]],\n",
       "\n",
       "         [[ 0.6387,  0.1587,  0.3682,  ..., -0.0413, -1.7764, -0.3121],\n",
       "          [ 0.2101, -0.8050,  0.6828,  ...,  0.1364, -0.3823,  0.4603],\n",
       "          [-0.5988, -1.0858,  0.9387,  ...,  0.3549,  0.2934,  0.3608],\n",
       "          [ 0.1703, -1.1930,  1.0519,  ...,  0.2757,  0.0339,  0.3572]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-5.4640e-01,  5.7458e-02,  9.7053e-03,  ..., -2.9330e-01,\n",
       "            5.2341e-02,  2.7899e-01],\n",
       "          [-4.1534e-01,  1.4503e-01,  2.0998e-01,  ..., -6.0117e-02,\n",
       "            1.8679e-01, -3.1907e-02],\n",
       "          [-6.3770e-01,  3.5045e-01,  8.9739e-02,  ..., -1.1954e-02,\n",
       "            2.6550e-01, -5.8803e-02],\n",
       "          ...,\n",
       "          [-3.3424e-01,  2.3924e-03,  1.7207e-01,  ...,  2.5368e-01,\n",
       "           -6.1713e-02, -4.3023e-01],\n",
       "          [-6.3606e-01, -2.1572e-01, -2.6450e-01,  ..., -3.8562e-02,\n",
       "            1.1104e-01, -5.3788e-02],\n",
       "          [-5.4738e-01,  5.7806e-02,  1.3128e-02,  ..., -2.9129e-01,\n",
       "            5.3328e-02,  2.7782e-01]],\n",
       "\n",
       "         [[-4.6422e-01, -2.6551e-01,  3.4714e-01,  ..., -7.1971e-02,\n",
       "           -2.2842e-01,  5.5929e-02],\n",
       "          [-2.8380e-01, -4.1038e-01,  1.6159e-01,  ..., -1.2708e-01,\n",
       "           -1.2186e-01, -5.1213e-02],\n",
       "          [-2.7690e-01, -3.9022e-01,  2.5114e-01,  ...,  2.4963e-01,\n",
       "           -1.6246e-01,  1.6736e-02],\n",
       "          ...,\n",
       "          [-4.6844e-02,  1.1108e-01, -1.3063e-01,  ..., -4.5754e-02,\n",
       "           -3.9031e-01, -1.7315e-01],\n",
       "          [-2.1383e-01, -1.3591e-01,  1.7251e-01,  ..., -3.5565e-03,\n",
       "           -2.7359e-01,  1.3613e-01],\n",
       "          [-4.6284e-01, -2.6353e-01,  3.4759e-01,  ..., -7.3032e-02,\n",
       "           -2.3027e-01,  5.4185e-02]],\n",
       "\n",
       "         [[-2.0027e-01, -1.4910e-01,  1.8674e-01,  ...,  3.1614e-01,\n",
       "           -2.8166e-01,  2.6101e-01],\n",
       "          [-2.4631e-01, -5.5419e-02,  3.9552e-01,  ...,  1.9062e-01,\n",
       "           -3.2413e-01, -7.6773e-02],\n",
       "          [-3.3174e-01, -1.4927e-01,  3.0269e-01,  ..., -3.1954e-01,\n",
       "           -2.2575e-01, -5.9873e-04],\n",
       "          ...,\n",
       "          [ 8.5798e-02,  1.0344e-01,  4.0986e-01,  ..., -1.2341e-01,\n",
       "           -6.5966e-01,  2.3982e-01],\n",
       "          [-8.9061e-02, -3.4044e-01,  2.5776e-01,  ..., -9.3641e-02,\n",
       "           -3.9347e-01,  2.6379e-01],\n",
       "          [-1.9794e-01, -1.4975e-01,  1.8717e-01,  ...,  3.1565e-01,\n",
       "           -2.8488e-01,  2.6046e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.5117e-02, -1.3928e-01,  7.6014e-03,  ..., -3.8481e-01,\n",
       "            1.2669e-01, -2.2993e-01],\n",
       "          [-8.0262e-02, -4.9735e-02,  1.5532e-01,  ..., -4.8707e-01,\n",
       "            3.9387e-01, -4.6682e-02],\n",
       "          [ 2.2370e-01,  1.7691e-01, -3.5919e-01,  ..., -5.1865e-01,\n",
       "            1.6920e-02,  2.1310e-01],\n",
       "          ...,\n",
       "          [-1.1332e-01, -6.6876e-02,  1.8200e-01,  ...,  3.7903e-02,\n",
       "            1.7610e-01, -2.3323e-01],\n",
       "          [ 3.3128e-02, -3.4952e-02,  2.5311e-01,  ..., -9.7530e-02,\n",
       "            3.1803e-01, -3.4675e-01],\n",
       "          [ 1.5287e-02, -1.3809e-01,  9.2525e-03,  ..., -3.8411e-01,\n",
       "            1.2447e-01, -2.3136e-01]],\n",
       "\n",
       "         [[-1.2392e-01,  3.3861e-01, -1.0459e-01,  ..., -1.2595e-02,\n",
       "           -1.0849e-01,  2.5836e-01],\n",
       "          [-7.7411e-02,  2.4782e-01, -9.5613e-02,  ...,  1.3220e-01,\n",
       "           -2.1522e-01,  1.1187e-02],\n",
       "          [-6.7891e-02,  5.3134e-01, -3.8289e-03,  ...,  2.0252e-01,\n",
       "            2.3143e-02, -2.4148e-01],\n",
       "          ...,\n",
       "          [ 5.6724e-02,  1.0345e-01, -3.9804e-01,  ..., -3.2694e-02,\n",
       "           -7.3376e-02, -3.7074e-02],\n",
       "          [ 2.4157e-01,  5.0918e-01, -4.7326e-01,  ...,  1.7951e-01,\n",
       "           -1.1676e-01,  9.0994e-02],\n",
       "          [-1.2722e-01,  3.3912e-01, -1.0523e-01,  ..., -9.9084e-03,\n",
       "           -1.0996e-01,  2.5831e-01]],\n",
       "\n",
       "         [[ 4.8222e-01, -1.5834e-01,  2.9193e-01,  ..., -6.9083e-02,\n",
       "           -1.0538e-01,  6.2097e-02],\n",
       "          [ 2.3103e-01, -1.1861e-01,  2.7825e-01,  ..., -2.0382e-01,\n",
       "           -1.9261e-01, -3.5816e-01],\n",
       "          [ 4.5501e-01, -5.9815e-03,  1.0476e-01,  ...,  3.9940e-02,\n",
       "           -7.6687e-03, -9.3440e-02],\n",
       "          ...,\n",
       "          [ 1.1879e-01, -3.7505e-02,  1.1185e-01,  ..., -1.8232e-01,\n",
       "            1.3342e-01,  2.6131e-01],\n",
       "          [ 3.9491e-01, -3.0028e-01,  2.8387e-01,  ..., -1.7151e-01,\n",
       "            6.8828e-02, -5.5362e-02],\n",
       "          [ 4.8176e-01, -1.5798e-01,  2.9241e-01,  ..., -6.7229e-02,\n",
       "           -1.0522e-01,  6.3745e-02]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.1929,  0.0191, -0.0045,  ...,  0.2403,  0.3247,  0.6009],\n",
       "          [ 0.0480,  0.3388, -0.1645,  ...,  0.0352, -0.1056,  0.3946],\n",
       "          [ 0.0712,  0.2280,  0.1433,  ...,  0.2622, -0.0674,  0.0638],\n",
       "          ...,\n",
       "          [ 0.0328,  0.1273,  0.4276,  ...,  0.2144,  0.3279,  0.3169],\n",
       "          [-0.2077, -0.0936, -0.0286,  ...,  0.1816,  0.4233,  0.6167],\n",
       "          [-0.1917,  0.0202, -0.0032,  ...,  0.2380,  0.3258,  0.6030]],\n",
       "\n",
       "         [[-0.4478,  0.0663, -0.0102,  ..., -0.1682, -0.1892, -0.2215],\n",
       "          [-0.4355,  0.4787,  0.1166,  ..., -0.0485,  0.1458, -0.1535],\n",
       "          [-0.9091,  0.5704, -0.2356,  ...,  0.0576,  0.1437,  0.0957],\n",
       "          ...,\n",
       "          [-0.0635,  0.1425, -0.0586,  ...,  0.0240, -0.2070, -0.0054],\n",
       "          [-0.2037,  0.2418,  0.0022,  ..., -0.0285, -0.2522, -0.1647],\n",
       "          [-0.4486,  0.0645, -0.0105,  ..., -0.1673, -0.1916, -0.2202]],\n",
       "\n",
       "         [[-0.1024, -0.1234, -0.1138,  ..., -0.0375, -0.0827,  0.1250],\n",
       "          [ 0.0970, -0.1406,  0.2871,  ...,  0.2715, -0.1082,  0.0563],\n",
       "          [-0.1270, -0.3348,  0.0622,  ...,  0.0188, -0.0683,  0.2763],\n",
       "          ...,\n",
       "          [-0.1568, -0.0906,  0.5358,  ...,  0.0252,  0.0506,  0.4314],\n",
       "          [-0.0299, -0.3289,  0.1691,  ..., -0.0790, -0.2549,  0.2087],\n",
       "          [-0.1020, -0.1249, -0.1123,  ..., -0.0343, -0.0816,  0.1289]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2085,  0.1011,  0.1767,  ..., -0.4826,  0.2875,  0.1351],\n",
       "          [-0.0034,  0.0369,  0.3246,  ..., -0.0635, -0.2081,  0.3475],\n",
       "          [-0.1529,  0.0340, -0.0923,  ..., -0.4186,  0.2741,  0.2553],\n",
       "          ...,\n",
       "          [ 0.0049,  0.0968, -0.2360,  ..., -0.2942,  0.1307, -0.0489],\n",
       "          [-0.1598,  0.0750, -0.0669,  ..., -0.6609,  0.0541,  0.1042],\n",
       "          [-0.2075,  0.0994,  0.1778,  ..., -0.4820,  0.2849,  0.1370]],\n",
       "\n",
       "         [[ 0.3971, -0.0207,  0.3312,  ...,  0.1542,  0.3236,  0.2965],\n",
       "          [ 0.1854, -0.0016,  0.3453,  ..., -0.2970,  0.1512,  0.0708],\n",
       "          [-0.0770, -0.0912,  0.2327,  ...,  0.0871,  0.2249,  0.3303],\n",
       "          ...,\n",
       "          [-0.0351,  0.0605, -0.1010,  ...,  0.4318,  0.2035,  0.3181],\n",
       "          [ 0.5057,  0.0621,  0.1521,  ...,  0.2848,  0.3969,  0.5452],\n",
       "          [ 0.3980, -0.0215,  0.3315,  ...,  0.1540,  0.3259,  0.2954]],\n",
       "\n",
       "         [[-0.0029,  0.0189, -0.3331,  ...,  0.3541,  0.1962, -0.2419],\n",
       "          [ 0.0102, -0.0617, -0.2004,  ...,  0.5232,  0.4553, -0.5429],\n",
       "          [-0.1040,  0.2300, -0.5065,  ...,  0.6311,  0.2619, -0.5405],\n",
       "          ...,\n",
       "          [ 0.1004,  0.0930, -0.2594,  ...,  0.1182, -0.0066, -0.0211],\n",
       "          [-0.0772,  0.1616, -0.1775,  ...,  0.4308,  0.0035, -0.2565],\n",
       "          [-0.0036,  0.0155, -0.3349,  ...,  0.3524,  0.1945, -0.2413]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.5462,  0.4702, -0.8295,  ...,  0.1459,  0.7012, -0.8567],\n",
       "          [ 0.6113,  0.9477, -0.6951,  ..., -0.3385,  1.0725,  0.0581],\n",
       "          [ 0.2606,  0.8738, -0.8056,  ..., -0.3084,  0.5441,  0.4741],\n",
       "          [ 0.0612,  1.1786, -0.3345,  ..., -0.6544,  1.0746,  0.4991]],\n",
       "\n",
       "         [[ 1.6311, -0.3108,  0.2580,  ..., -0.1879,  0.9756,  0.5364],\n",
       "          [ 0.2612,  0.5009,  0.3092,  ..., -0.7568,  1.5260,  0.9270],\n",
       "          [ 0.0250,  1.0439, -0.2636,  ..., -2.3590,  2.2003,  0.9124],\n",
       "          [-0.4929, -0.0237,  0.0897,  ..., -2.3381,  1.1398,  0.9901]],\n",
       "\n",
       "         [[ 0.1134,  0.7159, -0.1347,  ...,  1.0972, -0.6505,  0.1363],\n",
       "          [ 0.8273,  1.8243, -0.4018,  ...,  0.0690, -0.8647,  0.7764],\n",
       "          [ 0.4877,  1.3700, -0.4699,  ..., -0.5280, -0.3722,  0.3920],\n",
       "          [ 1.0546,  0.7911, -0.4395,  ..., -0.0999, -0.9421,  0.1921]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0619,  0.1592,  0.3242,  ..., -0.6599, -0.1150, -0.5818],\n",
       "          [ 1.9026, -0.6222, -0.0490,  ..., -0.3804,  0.7402,  0.6334],\n",
       "          [ 1.9025, -0.6317,  0.1677,  ..., -0.9332,  0.6462,  0.5048],\n",
       "          [ 1.7607, -0.5465, -0.0048,  ..., -0.3195,  0.7220,  0.4318]],\n",
       "\n",
       "         [[-0.0849, -0.9011,  0.1407,  ..., -1.1158, -0.3079,  0.0936],\n",
       "          [-0.0062, -0.4526, -0.5597,  ..., -0.7874, -0.3499,  0.0342],\n",
       "          [-0.2642, -1.3733, -0.9757,  ..., -0.6372, -0.1164,  0.5968],\n",
       "          [-0.4768, -1.3766, -1.0083,  ..., -0.8652,  0.0310,  0.1703]],\n",
       "\n",
       "         [[ 1.0717,  0.1727, -0.7254,  ...,  1.2686, -0.1320, -0.2041],\n",
       "          [ 0.2100, -0.1317, -1.4119,  ...,  1.6712, -0.5138, -0.1527],\n",
       "          [-0.0618,  0.6459, -0.6642,  ...,  1.2599, -0.7395, -0.7357],\n",
       "          [ 0.8146, -0.7241, -0.1433,  ...,  1.4562, -1.6107, -0.9719]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-0.1205,  0.6442, -0.2235,  ...,  0.1362,  0.3981,  0.1348],\n",
       "          [ 0.4209,  1.3090, -0.1859,  ...,  0.5325,  0.8006, -0.0271],\n",
       "          [ 0.3964,  1.6707,  0.2482,  ...,  0.6993,  0.6065, -0.0727],\n",
       "          [ 0.4362,  1.2736,  0.5792,  ...,  0.4135,  0.7490, -0.2544]],\n",
       "\n",
       "         [[ 0.1208,  0.3111, -0.1036,  ...,  0.0761, -0.3220,  0.6560],\n",
       "          [ 0.2760,  0.5874, -0.0058,  ...,  0.0685, -0.7774,  0.5514],\n",
       "          [ 0.2206,  0.7074, -0.0511,  ...,  0.1115, -1.2267,  0.8018],\n",
       "          [ 0.6167,  0.5161, -0.4319,  ..., -0.0518, -0.8680,  0.7876]],\n",
       "\n",
       "         [[-0.2464,  0.4196, -0.9445,  ...,  0.2840,  0.0853, -0.3203],\n",
       "          [-0.3768,  0.6673, -1.2296,  ..., -0.0748,  0.2532, -0.0803],\n",
       "          [-0.4081,  1.1277, -1.3697,  ..., -0.3518,  0.5689, -0.5356],\n",
       "          [-0.2126,  0.8991, -1.2929,  ..., -0.2013,  0.1839, -0.0865]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3755,  0.0827,  0.5771,  ...,  0.2579,  0.1919,  0.5814],\n",
       "          [-0.2647,  0.2200,  0.6630,  ...,  0.2463,  0.4284,  1.1629],\n",
       "          [-0.3366,  0.4808,  0.6741,  ...,  0.7635,  0.4397,  0.9553],\n",
       "          [ 0.2376,  0.3287,  0.7679,  ...,  0.7693,  0.4022,  0.8291]],\n",
       "\n",
       "         [[ 0.1894,  0.3319,  0.4220,  ..., -0.2848, -0.0812, -0.6589],\n",
       "          [-0.5217,  1.9806, -0.7248,  ..., -0.7370,  0.2517, -1.5336],\n",
       "          [-0.0291,  0.8833, -0.3533,  ...,  0.2592,  0.2248, -1.0237],\n",
       "          [ 0.1906,  0.3620, -0.3130,  ...,  0.0443,  0.2858, -0.2069]],\n",
       "\n",
       "         [[ 0.0825,  0.3650,  0.1934,  ...,  0.1508, -0.1861, -0.1812],\n",
       "          [ 0.1543,  0.4768,  0.2196,  ...,  0.3654, -0.3218, -0.0846],\n",
       "          [-0.2767,  0.1374, -0.0758,  ...,  0.2337, -0.9914, -0.4037],\n",
       "          [-0.0211,  0.5166, -0.4387,  ..., -0.0334, -0.4778, -0.4299]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-3.0256e-03, -2.2125e-01,  4.3460e-02,  ..., -3.5130e-03,\n",
       "            1.9566e-01, -9.9811e-02],\n",
       "          [ 3.1152e-01, -4.6965e-02,  3.2391e-01,  ...,  2.3579e-01,\n",
       "           -2.2049e-01,  8.1715e-02],\n",
       "          [ 8.3892e-02, -2.0903e-01, -1.2718e-01,  ..., -8.5115e-02,\n",
       "           -7.6810e-02,  2.4624e-01],\n",
       "          ...,\n",
       "          [-2.3947e-01, -2.4102e-01, -1.5019e-02,  ..., -1.1707e-01,\n",
       "            2.8459e-01,  4.5461e-02],\n",
       "          [ 4.1871e-02, -3.3276e-01,  1.1912e-01,  ...,  2.8474e-01,\n",
       "           -2.3717e-01,  1.5182e-01],\n",
       "          [-4.5359e-03, -2.2551e-01,  4.2745e-02,  ..., -4.0429e-03,\n",
       "            1.9530e-01, -1.0039e-01]],\n",
       "\n",
       "         [[-3.3765e-01,  3.3704e-01, -1.5502e-01,  ...,  8.0318e-02,\n",
       "           -2.3001e-01, -7.1033e-02],\n",
       "          [ 9.8389e-02,  2.6592e-01, -5.1980e-02,  ...,  1.6436e-01,\n",
       "           -1.2493e-01, -2.2464e-01],\n",
       "          [-8.7590e-03,  1.2558e-01,  4.3853e-02,  ..., -5.8422e-02,\n",
       "           -8.8848e-02, -3.0582e-01],\n",
       "          ...,\n",
       "          [-4.8968e-01,  2.0851e-01, -1.2308e-01,  ..., -8.6486e-02,\n",
       "            3.5005e-02,  5.2828e-04],\n",
       "          [-1.1602e-01,  1.1952e-01, -7.8498e-02,  ..., -1.5208e-01,\n",
       "            5.4845e-02, -1.7920e-01],\n",
       "          [-3.3723e-01,  3.3981e-01, -1.5521e-01,  ...,  7.9986e-02,\n",
       "           -2.3162e-01, -7.0929e-02]],\n",
       "\n",
       "         [[-4.7363e-02,  1.6596e-02, -3.5318e-01,  ..., -8.3422e-02,\n",
       "           -2.1343e-01,  2.5302e-01],\n",
       "          [-1.4488e-01, -9.6140e-03, -4.4347e-01,  ..., -4.7053e-02,\n",
       "           -5.3942e-01,  2.2801e-01],\n",
       "          [ 8.5571e-02,  2.1223e-01, -2.5081e-01,  ..., -3.9093e-01,\n",
       "           -1.1513e-01,  1.0205e-01],\n",
       "          ...,\n",
       "          [ 1.8550e-01,  2.0827e-01, -1.9261e-01,  ..., -2.2987e-01,\n",
       "            3.5405e-02,  2.8761e-01],\n",
       "          [ 1.9433e-01, -6.9471e-03, -3.2133e-01,  ..., -1.0944e-01,\n",
       "           -6.4626e-02,  3.0399e-01],\n",
       "          [-4.4624e-02,  1.8075e-02, -3.5367e-01,  ..., -8.4522e-02,\n",
       "           -2.1192e-01,  2.5374e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.4047e-01,  1.7874e-01,  1.1390e-01,  ...,  3.0818e-01,\n",
       "           -1.3902e-01, -1.7404e-01],\n",
       "          [ 1.0014e-01,  8.4280e-02,  1.7098e-02,  ...,  4.5325e-01,\n",
       "            1.7543e-01, -6.3025e-02],\n",
       "          [ 1.6712e-01, -1.3461e-01, -2.0328e-01,  ..., -5.4429e-02,\n",
       "           -1.8413e-01,  2.9975e-02],\n",
       "          ...,\n",
       "          [ 2.7112e-01,  3.0786e-01,  1.7093e-01,  ..., -3.1548e-01,\n",
       "            1.9685e-01, -1.1030e-01],\n",
       "          [ 1.5287e-01,  2.1742e-01,  7.6753e-02,  ...,  1.2452e-01,\n",
       "           -8.4545e-02, -3.9393e-02],\n",
       "          [ 2.3839e-01,  1.8068e-01,  1.1292e-01,  ...,  3.1029e-01,\n",
       "           -1.3728e-01, -1.7410e-01]],\n",
       "\n",
       "         [[-2.3572e-01,  1.1943e-02, -3.4907e-01,  ..., -5.1160e-01,\n",
       "           -5.6100e-02,  2.5712e-01],\n",
       "          [ 1.6779e-01, -1.6738e-01, -5.0371e-02,  ..., -6.9116e-01,\n",
       "           -1.8501e-01,  3.0834e-01],\n",
       "          [-2.5019e-02,  9.2202e-02,  2.0470e-01,  ..., -4.4071e-01,\n",
       "            3.3997e-01, -6.1243e-03],\n",
       "          ...,\n",
       "          [-3.8077e-01,  1.5452e-01, -2.2773e-01,  ..., -1.3332e-01,\n",
       "           -3.3447e-01,  2.0273e-01],\n",
       "          [-1.4025e-01, -1.9435e-01, -2.8190e-01,  ..., -4.1894e-01,\n",
       "           -9.0249e-02,  2.5530e-01],\n",
       "          [-2.3554e-01,  1.2363e-02, -3.4931e-01,  ..., -5.1085e-01,\n",
       "           -5.4987e-02,  2.5784e-01]],\n",
       "\n",
       "         [[-1.9063e-01,  2.6322e-01,  2.2176e-01,  ...,  2.1248e-02,\n",
       "           -4.5951e-01, -2.1527e-01],\n",
       "          [-4.7741e-01,  1.5322e-01,  1.5689e-01,  ...,  2.2884e-01,\n",
       "           -4.0670e-01,  3.5743e-02],\n",
       "          [-2.6818e-01,  2.2251e-01,  4.3521e-02,  ...,  8.2451e-02,\n",
       "           -4.9600e-01, -1.6262e-02],\n",
       "          ...,\n",
       "          [-5.7761e-02, -3.5898e-02,  1.1825e-01,  ..., -2.4580e-01,\n",
       "           -1.3179e-01,  6.8278e-02],\n",
       "          [-1.2168e-01,  2.4762e-01,  5.6946e-02,  ..., -1.3322e-02,\n",
       "           -2.4070e-01,  1.2917e-02],\n",
       "          [-1.8992e-01,  2.6394e-01,  2.2247e-01,  ...,  2.1233e-02,\n",
       "           -4.6075e-01, -2.1629e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 1.0908e-01,  2.0079e-01, -1.9822e-01,  ..., -3.3749e-01,\n",
       "           -1.9685e-01,  5.1244e-02],\n",
       "          [ 1.9670e-01, -2.6843e-01, -1.9126e-01,  ..., -1.4373e-01,\n",
       "            6.3540e-03,  2.2458e-01],\n",
       "          [-1.5412e-01, -4.9134e-01, -2.3447e-01,  ...,  1.2879e-01,\n",
       "           -1.9092e-01,  2.4344e-01],\n",
       "          ...,\n",
       "          [-2.0428e-01, -3.1781e-01, -1.1532e-01,  ..., -5.2646e-01,\n",
       "           -1.9807e-01, -1.7307e-01],\n",
       "          [ 1.3222e-01, -7.8979e-02, -1.2308e-01,  ..., -5.0074e-01,\n",
       "           -1.7340e-02, -9.2319e-02],\n",
       "          [ 1.0730e-01,  2.0134e-01, -1.9997e-01,  ..., -3.3448e-01,\n",
       "           -1.9734e-01,  4.9627e-02]],\n",
       "\n",
       "         [[-3.1371e-01, -1.9406e-01,  5.8923e-02,  ...,  2.0452e-01,\n",
       "           -5.1329e-01,  2.7469e-02],\n",
       "          [-2.0984e-01, -7.6208e-02, -1.3181e-01,  ...,  2.8003e-01,\n",
       "           -6.2247e-01, -2.0087e-01],\n",
       "          [-7.7219e-02,  2.1298e-01,  2.1871e-01,  ...,  2.5799e-01,\n",
       "           -5.5572e-01,  1.9587e-02],\n",
       "          ...,\n",
       "          [-2.2493e-01, -1.7411e-01,  1.1110e-01,  ...,  1.8431e-01,\n",
       "           -3.8268e-01, -4.5880e-01],\n",
       "          [-2.9893e-01, -1.5781e-01, -4.6604e-02,  ...,  1.8729e-01,\n",
       "           -2.8423e-01,  1.1523e-01],\n",
       "          [-3.1656e-01, -1.9546e-01,  5.4005e-02,  ...,  2.0422e-01,\n",
       "           -5.1357e-01,  2.6803e-02]],\n",
       "\n",
       "         [[-2.9047e-01, -1.6730e-01, -7.3307e-02,  ...,  3.9158e-01,\n",
       "            8.0199e-02,  5.1211e-01],\n",
       "          [-4.3172e-01,  1.3399e-01, -1.9717e-01,  ...,  3.3724e-01,\n",
       "            2.7794e-01,  4.4398e-01],\n",
       "          [-2.8094e-01, -2.7623e-01,  7.6602e-02,  ..., -7.8312e-02,\n",
       "            3.5450e-01,  1.1991e-01],\n",
       "          ...,\n",
       "          [-6.4016e-02, -4.7865e-03,  3.3983e-01,  ...,  1.7802e-01,\n",
       "            1.6627e-01, -1.6993e-01],\n",
       "          [-2.9383e-01, -1.7821e-01,  3.5306e-02,  ...,  3.1543e-01,\n",
       "            2.0401e-01,  3.2323e-01],\n",
       "          [-2.9161e-01, -1.7097e-01, -7.1009e-02,  ...,  3.9168e-01,\n",
       "            8.0039e-02,  5.1124e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.4680e-02, -9.6925e-02,  5.2241e-02,  ...,  2.3456e-01,\n",
       "           -3.4136e-01, -4.0834e-02],\n",
       "          [-2.1517e-01, -3.5410e-01,  9.8627e-02,  ...,  4.1852e-01,\n",
       "           -2.8466e-01, -1.2426e-01],\n",
       "          [ 4.0893e-02, -3.3630e-01,  4.2097e-01,  ..., -1.9906e-02,\n",
       "           -2.9970e-01, -5.4391e-03],\n",
       "          ...,\n",
       "          [-5.1683e-02,  2.7790e-01,  7.1595e-02,  ..., -2.6519e-02,\n",
       "           -3.8570e-01,  4.1188e-01],\n",
       "          [-7.8480e-02,  2.4319e-02,  4.0473e-02,  ...,  1.1943e-01,\n",
       "           -4.4514e-01, -2.8787e-01],\n",
       "          [-5.2766e-02, -9.3895e-02,  5.1558e-02,  ...,  2.3658e-01,\n",
       "           -3.3814e-01, -4.2336e-02]],\n",
       "\n",
       "         [[ 1.1662e-01, -2.4622e-01,  1.5394e-01,  ...,  1.6403e-01,\n",
       "            3.2288e-02,  2.1065e-01],\n",
       "          [-2.7464e-01,  1.6506e-02, -2.6806e-01,  ..., -4.1471e-04,\n",
       "            1.8953e-02,  2.2359e-01],\n",
       "          [-5.1309e-02,  7.1081e-02,  4.1231e-01,  ..., -3.4859e-02,\n",
       "            3.2373e-01,  4.3916e-02],\n",
       "          ...,\n",
       "          [ 1.7082e-01,  1.2634e-01, -2.0795e-01,  ...,  4.1023e-02,\n",
       "            7.6420e-02,  2.6367e-01],\n",
       "          [-4.1469e-02, -3.8416e-01,  2.8760e-01,  ...,  1.6147e-01,\n",
       "            2.2531e-01,  2.1863e-01],\n",
       "          [ 1.1691e-01, -2.4559e-01,  1.5645e-01,  ...,  1.6357e-01,\n",
       "            3.0595e-02,  2.1026e-01]],\n",
       "\n",
       "         [[ 3.9132e-01, -5.9355e-02,  1.3709e-01,  ..., -4.6327e-02,\n",
       "           -1.3166e-01,  3.3492e-01],\n",
       "          [ 3.6434e-01,  6.6210e-02,  5.3189e-02,  ...,  1.2611e-01,\n",
       "           -2.5541e-01,  1.3667e-01],\n",
       "          [ 1.8213e-01,  5.0154e-03,  5.2722e-02,  ..., -3.5660e-01,\n",
       "           -1.7527e-01,  5.5356e-01],\n",
       "          ...,\n",
       "          [ 5.5943e-02, -2.5184e-01,  1.3692e-01,  ..., -6.4755e-02,\n",
       "            2.9573e-01,  2.9521e-01],\n",
       "          [ 2.7183e-01, -2.2614e-01, -1.6124e-01,  ...,  1.7288e-01,\n",
       "           -3.2487e-01,  3.0449e-01],\n",
       "          [ 3.9337e-01, -5.7296e-02,  1.3861e-01,  ..., -4.4770e-02,\n",
       "           -1.2833e-01,  3.3442e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.4995, -0.9864, -0.0904,  ...,  0.4426,  0.8353, -0.8452],\n",
       "          [ 1.0032, -1.1263, -0.5307,  ...,  0.6866,  0.2234, -0.5846],\n",
       "          [ 1.0826, -1.1613, -0.2856,  ...,  0.6352,  0.5640, -0.7502],\n",
       "          [ 1.3873, -1.2930, -0.2523,  ...,  0.6886,  0.5190, -0.6160]],\n",
       "\n",
       "         [[ 0.9626, -2.1702, -0.1208,  ..., -0.2852,  0.8235, -0.4676],\n",
       "          [ 0.9713, -1.9348,  0.0412,  ..., -0.0450,  1.0731, -0.5664],\n",
       "          [ 0.9614, -1.5956,  0.2056,  ...,  0.3683,  1.1205, -0.8397],\n",
       "          [ 0.9680, -1.6948,  0.2069,  ...,  0.4903,  1.0531, -0.7690]],\n",
       "\n",
       "         [[ 0.0451, -0.2506, -0.9757,  ...,  0.4651, -2.5396, -0.5876],\n",
       "          [-0.0992, -0.3633, -0.9318,  ...,  1.0242, -2.2180, -1.2210],\n",
       "          [-0.1873, -0.2291, -0.7140,  ...,  1.3463, -1.9108, -1.6695],\n",
       "          [-0.3438,  0.1053, -0.7391,  ...,  1.1888, -1.9629, -1.8607]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2053,  0.6155,  0.1240,  ..., -0.6351,  0.7645, -0.3093],\n",
       "          [ 0.6791,  0.9625,  0.5389,  ..., -0.6981,  0.9177, -0.1420],\n",
       "          [ 0.7821,  1.0715,  0.5262,  ..., -0.8773,  0.8884,  0.0962],\n",
       "          [ 0.9538,  0.9888,  0.3649,  ..., -0.5690,  1.1076,  0.1817]],\n",
       "\n",
       "         [[ 0.0166, -1.8777,  0.2483,  ..., -0.5580, -0.8815,  0.0844],\n",
       "          [ 0.3166, -1.7294,  0.5171,  ..., -0.8958, -1.0704, -0.0400],\n",
       "          [ 0.3701, -1.5515,  0.6429,  ..., -0.8336, -1.3113, -0.1215],\n",
       "          [ 0.3894, -1.5568,  0.9125,  ..., -1.0174, -1.4624, -0.2637]],\n",
       "\n",
       "         [[ 0.9680,  0.5395, -0.0711,  ...,  0.4879, -0.6193,  1.0205],\n",
       "          [ 1.1015,  0.7301, -0.2828,  ...,  0.5376, -0.9129,  0.9271],\n",
       "          [ 1.3020,  0.4106, -0.5284,  ...,  0.8241, -0.7192,  0.6133],\n",
       "          [ 1.4652,  0.3849, -0.4312,  ...,  0.6176, -0.7119,  0.6696]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-5.3657e-02, -1.5099e-01,  6.3258e-02,  ...,  1.8220e-02,\n",
       "            1.1341e-02, -3.0333e-02],\n",
       "          [ 5.7851e-02, -8.8714e-02, -1.8704e-01,  ...,  1.2237e-01,\n",
       "            1.4247e-01, -8.0157e-02],\n",
       "          [ 1.1409e-01, -5.6758e-02, -1.3251e-01,  ...,  1.7895e-01,\n",
       "           -3.7839e-02, -8.8712e-02],\n",
       "          [ 3.4302e-01, -9.3896e-02,  2.5179e-03,  ...,  1.7178e-01,\n",
       "            1.9392e-01, -2.3804e-01]],\n",
       "\n",
       "         [[-1.0832e-01, -3.2427e-02,  1.1201e-01,  ...,  1.8440e-01,\n",
       "           -3.9870e-02, -9.1582e-02],\n",
       "          [-2.6606e-01, -3.7266e-01,  1.3524e-01,  ...,  5.4993e-01,\n",
       "           -1.6459e-02, -8.9971e-02],\n",
       "          [-1.9174e-02, -2.5117e-01,  2.5059e-01,  ...,  5.3606e-01,\n",
       "           -4.5647e-02, -3.0807e-01],\n",
       "          [-4.2808e-02, -2.7361e-01,  1.6752e-01,  ...,  7.4861e-01,\n",
       "           -1.5302e-02, -2.0326e-01]],\n",
       "\n",
       "         [[-1.2906e-01,  1.5864e-01, -6.0737e-01,  ...,  7.2039e-02,\n",
       "           -3.2212e-01, -1.9560e-01],\n",
       "          [-1.5654e-01,  3.0266e-01, -7.5124e-01,  ...,  1.4425e-01,\n",
       "           -4.1782e-01, -3.8032e-01],\n",
       "          [-4.3402e-01,  7.5803e-02, -7.7065e-01,  ..., -1.0786e-02,\n",
       "           -7.5248e-01, -3.3449e-01],\n",
       "          [-5.2800e-01,  1.4632e-01, -5.9818e-01,  ...,  8.8470e-02,\n",
       "           -8.4858e-01, -4.4878e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.7270e-05,  2.6496e-01,  1.2175e-01,  ..., -1.3730e-01,\n",
       "            9.1592e-02,  1.1103e-01],\n",
       "          [ 1.4952e-01,  5.4499e-01,  2.2029e-01,  ..., -1.1600e-01,\n",
       "            6.0675e-01,  3.5537e-01],\n",
       "          [-7.7902e-03,  6.4012e-01,  4.9068e-01,  ..., -2.6121e-01,\n",
       "            6.1885e-01,  2.6099e-01],\n",
       "          [-7.3101e-02,  7.7151e-01,  4.9251e-01,  ..., -1.2092e-01,\n",
       "            6.5975e-01,  4.2904e-01]],\n",
       "\n",
       "         [[ 9.8641e-02, -1.1058e-02, -6.1264e-03,  ..., -1.3459e-01,\n",
       "            2.0737e-01,  2.4483e-01],\n",
       "          [ 6.6520e-02, -1.1328e-01, -1.3905e-01,  ..., -8.6127e-03,\n",
       "           -3.9021e-02,  7.6877e-01],\n",
       "          [ 1.5852e-01,  5.3757e-02, -4.9482e-02,  ..., -2.0519e-01,\n",
       "           -1.3968e-01,  8.6881e-01],\n",
       "          [ 3.5562e-01,  7.7728e-02, -3.3188e-01,  ..., -2.0423e-01,\n",
       "            1.2414e-03,  7.4903e-01]],\n",
       "\n",
       "         [[ 9.1269e-02,  5.3334e-02, -1.5642e-01,  ..., -2.7355e-02,\n",
       "           -1.7516e-02, -1.7358e-01],\n",
       "          [-1.2568e-01,  2.1210e-01, -1.0173e-01,  ..., -2.3218e-01,\n",
       "            1.4098e-01, -2.5811e-01],\n",
       "          [-3.6866e-01,  2.1414e-01, -1.9977e-01,  ..., -3.2064e-01,\n",
       "            8.1601e-02, -4.8141e-01],\n",
       "          [-3.5099e-01,  2.3110e-01, -1.4362e-01,  ..., -3.1777e-01,\n",
       "            1.3074e-02, -4.5551e-01]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.1425,  0.0729,  0.0297,  ..., -0.0695, -0.2719,  0.1530],\n",
       "          [-0.2493,  0.4567,  0.2377,  ...,  0.4657, -0.4622,  0.3743],\n",
       "          [-0.1327,  0.0975,  0.1293,  ...,  0.0767,  0.0189,  0.2759],\n",
       "          ...,\n",
       "          [-0.0397,  0.2333, -0.0206,  ..., -0.1290,  0.0016,  0.2089],\n",
       "          [-0.0587,  0.1407, -0.1108,  ..., -0.0089,  0.0108, -0.0504],\n",
       "          [-0.1427,  0.0728,  0.0303,  ..., -0.0702, -0.2707,  0.1507]],\n",
       "\n",
       "         [[ 0.0324,  0.1773, -0.0385,  ...,  0.3239,  0.1758, -0.1284],\n",
       "          [ 0.0018,  0.2460, -0.2965,  ...,  0.3566, -0.1168, -0.0091],\n",
       "          [ 0.1135,  0.0299, -0.1000,  ...,  0.0817,  0.1934, -0.1481],\n",
       "          ...,\n",
       "          [-0.5031,  0.0417,  0.1869,  ...,  0.5706, -0.1166, -0.4290],\n",
       "          [-0.1227,  0.0958, -0.1422,  ...,  0.3647,  0.1847, -0.1828],\n",
       "          [ 0.0335,  0.1792, -0.0394,  ...,  0.3210,  0.1779, -0.1267]],\n",
       "\n",
       "         [[-0.4057, -0.4218, -0.1837,  ..., -0.0809,  0.0777,  0.1006],\n",
       "          [-0.4568, -0.0942, -0.2761,  ..., -0.2756,  0.0376,  0.4056],\n",
       "          [-0.4300, -0.2269, -0.1730,  ...,  0.2143,  0.0243,  0.6076],\n",
       "          ...,\n",
       "          [-0.3375,  0.2432,  0.0428,  ..., -0.1620,  0.0416,  0.4604],\n",
       "          [-0.3590, -0.4852, -0.2387,  ..., -0.1779,  0.0255,  0.2132],\n",
       "          [-0.4027, -0.4244, -0.1836,  ..., -0.0802,  0.0765,  0.1033]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3782, -0.0273,  0.2908,  ...,  0.3071,  0.1642,  0.0910],\n",
       "          [ 0.6675,  0.0295,  0.2028,  ...,  0.2654, -0.1921,  0.1861],\n",
       "          [ 0.3813, -0.1075,  0.0505,  ...,  0.1096,  0.2305,  0.3769],\n",
       "          ...,\n",
       "          [ 0.0407,  0.1080, -0.1257,  ..., -0.3304,  0.2418, -0.0891],\n",
       "          [-0.0048, -0.0401,  0.1840,  ...,  0.4586,  0.0389,  0.0720],\n",
       "          [ 0.3750, -0.0252,  0.2912,  ...,  0.3057,  0.1637,  0.0923]],\n",
       "\n",
       "         [[ 0.1330, -0.2693,  0.3468,  ..., -0.1057,  0.0398, -0.3133],\n",
       "          [ 0.2686,  0.1355,  0.1222,  ...,  0.1823,  0.1389, -0.2932],\n",
       "          [ 0.1319,  0.0677,  0.3033,  ..., -0.1862,  0.1376, -0.1193],\n",
       "          ...,\n",
       "          [-0.1056, -0.0406,  0.1849,  ...,  0.0084, -0.0230, -0.2031],\n",
       "          [-0.0100, -0.0966,  0.3551,  ..., -0.2224,  0.0921, -0.5587],\n",
       "          [ 0.1349, -0.2712,  0.3471,  ..., -0.1063,  0.0401, -0.3131]],\n",
       "\n",
       "         [[ 0.1006,  0.3567, -0.2135,  ...,  0.1608, -0.2790,  0.3148],\n",
       "          [ 0.4546,  0.2478, -0.1118,  ..., -0.0253, -0.1845,  0.1764],\n",
       "          [ 0.0052,  0.3252, -0.2321,  ..., -0.0124, -0.1687,  0.2884],\n",
       "          ...,\n",
       "          [-0.2316,  0.7035, -0.1916,  ...,  0.1252, -0.5228,  0.2545],\n",
       "          [ 0.2009,  0.3184, -0.0303,  ..., -0.0239, -0.0701,  0.1785],\n",
       "          [ 0.0984,  0.3584, -0.2130,  ...,  0.1604, -0.2784,  0.3140]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[-1.5923e-01,  2.8818e-01,  1.3243e-01,  ...,  3.8172e-02,\n",
       "            3.6441e-01,  5.5358e-02],\n",
       "          [ 1.6802e-01, -1.2197e-01,  4.0942e-01,  ...,  1.7861e-01,\n",
       "           -6.2294e-03, -1.3123e-02],\n",
       "          [-1.2507e-01,  2.3854e-01,  4.1173e-01,  ...,  1.2982e-01,\n",
       "            1.6547e-01,  1.2993e-01],\n",
       "          ...,\n",
       "          [ 1.7048e-01,  7.6876e-02,  2.0553e-01,  ..., -1.0816e-01,\n",
       "            3.3860e-01, -1.4288e-01],\n",
       "          [ 6.4147e-03,  2.7298e-02,  1.5668e-01,  ...,  5.7397e-02,\n",
       "            4.3572e-01,  5.0313e-02],\n",
       "          [-1.5886e-01,  2.8835e-01,  1.3180e-01,  ...,  3.6557e-02,\n",
       "            3.6444e-01,  5.8699e-02]],\n",
       "\n",
       "         [[-4.6024e-01, -2.6099e-01, -1.8611e-01,  ..., -8.3625e-02,\n",
       "            5.1839e-01,  1.5448e-02],\n",
       "          [ 1.9866e-02, -1.9565e-01, -2.6137e-01,  ..., -1.3687e-02,\n",
       "            3.9810e-01, -1.1822e-01],\n",
       "          [-3.3917e-02, -1.1153e-01, -3.0891e-01,  ..., -8.7782e-02,\n",
       "            2.3289e-01,  4.6999e-02],\n",
       "          ...,\n",
       "          [-8.7135e-02, -4.4995e-02, -4.1731e-01,  ...,  2.0541e-01,\n",
       "            7.0298e-01,  1.7023e-01],\n",
       "          [-8.8704e-02, -1.3433e-01, -7.7297e-02,  ..., -1.5968e-01,\n",
       "            4.7180e-01, -1.8562e-01],\n",
       "          [-4.6026e-01, -2.6233e-01, -1.8330e-01,  ..., -8.2612e-02,\n",
       "            5.1922e-01,  1.9174e-02]],\n",
       "\n",
       "         [[-2.6662e-01, -1.9854e-01, -8.4734e-02,  ...,  5.3848e-02,\n",
       "            3.4383e-01, -1.4868e-01],\n",
       "          [-7.5820e-02, -5.3776e-01, -3.4106e-01,  ..., -3.2706e-01,\n",
       "            5.2583e-01, -1.1042e-01],\n",
       "          [ 1.0192e-01, -2.6320e-01, -3.6041e-01,  ..., -2.7981e-01,\n",
       "            2.8319e-01, -5.3888e-01],\n",
       "          ...,\n",
       "          [ 7.0800e-02, -2.4102e-01,  2.3907e-01,  ...,  2.6349e-01,\n",
       "            2.4975e-01, -1.9844e-01],\n",
       "          [-2.1199e-01, -2.0047e-01,  7.3680e-03,  ..., -5.4476e-04,\n",
       "            4.6599e-01, -6.1688e-02],\n",
       "          [-2.6566e-01, -1.9882e-01, -8.2426e-02,  ...,  5.6545e-02,\n",
       "            3.4551e-01, -1.4978e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 9.1463e-03,  5.0394e-01,  3.6759e-01,  ..., -1.1869e-01,\n",
       "            1.5575e-02, -2.1614e-02],\n",
       "          [ 1.9276e-02,  4.4753e-01,  3.8935e-01,  ...,  9.6903e-02,\n",
       "           -2.0080e-01,  8.9376e-02],\n",
       "          [ 2.6596e-01,  1.6196e-01,  4.9064e-01,  ..., -5.4999e-03,\n",
       "            2.6306e-01, -2.4944e-02],\n",
       "          ...,\n",
       "          [ 4.1975e-03, -3.5355e-02,  3.5295e-02,  ...,  2.0138e-02,\n",
       "            2.0296e-01, -1.1754e-01],\n",
       "          [-1.3527e-01,  2.2664e-01,  3.6248e-01,  ..., -2.0341e-01,\n",
       "           -1.2412e-01, -7.3789e-02],\n",
       "          [ 9.0179e-03,  5.0464e-01,  3.6922e-01,  ..., -1.1805e-01,\n",
       "            1.5296e-02, -2.1854e-02]],\n",
       "\n",
       "         [[ 4.9665e-02,  2.1769e-02, -1.3043e-01,  ..., -4.7466e-02,\n",
       "           -9.9166e-02, -7.0173e-02],\n",
       "          [ 1.4370e-01,  5.3487e-02,  9.6631e-02,  ...,  2.2505e-01,\n",
       "           -1.7359e-01,  3.8112e-01],\n",
       "          [-9.4488e-02,  5.3558e-01, -4.7630e-01,  ...,  1.7048e-01,\n",
       "           -7.0340e-01, -9.1035e-02],\n",
       "          ...,\n",
       "          [ 3.7129e-01,  1.1708e-01,  3.5106e-02,  ...,  1.7768e-01,\n",
       "            4.5171e-02, -7.6125e-01],\n",
       "          [-4.9350e-02,  1.5973e-01, -7.0256e-02,  ...,  4.1935e-02,\n",
       "           -2.4180e-01, -3.0286e-01],\n",
       "          [ 5.1684e-02,  2.2656e-02, -1.3046e-01,  ..., -4.8633e-02,\n",
       "           -9.7768e-02, -7.3444e-02]],\n",
       "\n",
       "         [[-1.6333e-01, -1.3282e-01,  1.3851e-01,  ...,  1.1594e-01,\n",
       "            1.6874e-02,  2.3759e-02],\n",
       "          [-2.1959e-01, -1.6871e-01,  4.8856e-01,  ..., -2.3822e-01,\n",
       "            6.4504e-02, -2.7044e-01],\n",
       "          [-1.3824e-02, -1.2848e-01, -1.1295e-01,  ..., -1.6644e-02,\n",
       "            9.8352e-02, -7.1826e-02],\n",
       "          ...,\n",
       "          [ 9.9441e-02, -3.1238e-01,  2.3768e-02,  ...,  2.5638e-01,\n",
       "           -1.9673e-01, -1.3889e-01],\n",
       "          [-1.9060e-01, -5.9555e-02,  3.7166e-02,  ...,  8.2493e-02,\n",
       "           -9.9208e-02,  1.0630e-01],\n",
       "          [-1.6511e-01, -1.3104e-01,  1.3700e-01,  ...,  1.1585e-01,\n",
       "            1.6770e-02,  2.5572e-02]]]], device='cuda:0',\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.1324,  0.5155, -2.2443,  ...,  1.5503,  1.8881,  1.4860],\n",
       "          [-0.1315,  0.4403, -2.3551,  ...,  1.6004,  1.9811,  1.4629],\n",
       "          [-0.1526,  0.5062, -2.2769,  ...,  1.4854,  1.8076,  1.4089],\n",
       "          [-0.1161,  0.4195, -2.1851,  ...,  1.4461,  1.7884,  1.2768]],\n",
       "\n",
       "         [[-1.1521,  0.2674, -1.0927,  ...,  0.7240, -0.0703,  0.7835],\n",
       "          [-1.3245,  0.2196, -1.1601,  ...,  0.5963, -0.1994,  0.7982],\n",
       "          [-1.2953,  0.3048, -1.3278,  ...,  0.6975, -0.2860,  0.9144],\n",
       "          [-1.3730,  0.3723, -1.4092,  ...,  0.6244, -0.3802,  0.9508]],\n",
       "\n",
       "         [[ 1.3281,  0.6041,  1.3719,  ..., -0.5899,  2.0981,  0.7100],\n",
       "          [ 1.2565,  0.8514,  1.3808,  ..., -0.5847,  2.0702,  0.6534],\n",
       "          [ 1.1710,  0.8839,  1.3725,  ..., -0.6113,  1.9286,  0.6718],\n",
       "          [ 1.1012,  0.8282,  1.3364,  ..., -0.6247,  1.9150,  0.6256]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.8899, -1.3228,  2.1489,  ..., -0.0495, -0.3858, -0.8476],\n",
       "          [ 1.1013, -1.5162,  2.3202,  ...,  0.0419, -0.4881, -0.8668],\n",
       "          [ 1.1033, -1.5065,  2.2437,  ...,  0.0522, -0.4637, -0.8770],\n",
       "          [ 1.1518, -1.4907,  2.1985,  ...,  0.0130, -0.4980, -0.8622]],\n",
       "\n",
       "         [[ 0.8786, -1.7811,  1.3200,  ..., -0.3405,  0.6079,  1.0721],\n",
       "          [ 0.9311, -1.7941,  1.3525,  ..., -0.4360,  0.5019,  1.0687],\n",
       "          [ 0.9220, -1.7931,  1.4401,  ..., -0.3626,  0.4419,  0.8757],\n",
       "          [ 0.9406, -1.7565,  1.3817,  ..., -0.3619,  0.4737,  0.8406]],\n",
       "\n",
       "         [[-1.1207, -0.4259, -0.2618,  ..., -0.3379, -0.2965,  1.3111],\n",
       "          [-1.3677, -0.3336, -0.3173,  ..., -0.3621, -0.4619,  1.4255],\n",
       "          [-1.4080, -0.3421, -0.2256,  ..., -0.3378, -0.5928,  1.3768],\n",
       "          [-1.4660, -0.3429, -0.2904,  ..., -0.2967, -0.5657,  1.3040]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 0.1775, -0.0600, -0.0861,  ..., -0.1186, -0.1869,  0.0921],\n",
       "          [ 0.1794, -0.1066, -0.0394,  ..., -0.1528, -0.1312, -0.0892],\n",
       "          [ 0.1345, -0.0178,  0.0042,  ..., -0.1602, -0.1575, -0.2209],\n",
       "          [ 0.1073, -0.0883,  0.0144,  ..., -0.2566, -0.1114, -0.2318]],\n",
       "\n",
       "         [[-0.2821, -0.0905,  0.0087,  ...,  0.0457,  0.0861, -0.0096],\n",
       "          [-0.3145, -0.1748, -0.0697,  ..., -0.0618,  0.1090, -0.0879],\n",
       "          [-0.3562, -0.2202, -0.1170,  ..., -0.0288,  0.1205, -0.1139],\n",
       "          [-0.4063, -0.2452, -0.1265,  ..., -0.0369,  0.1478, -0.0697]],\n",
       "\n",
       "         [[ 0.1068, -0.0168,  0.2994,  ..., -0.0178,  0.0022, -0.0876],\n",
       "          [ 0.1204,  0.0470,  0.2087,  ..., -0.0243,  0.0132, -0.0123],\n",
       "          [ 0.0938,  0.0583,  0.1125,  ...,  0.0072,  0.0994, -0.0428],\n",
       "          [ 0.0639,  0.0311,  0.1135,  ..., -0.0192,  0.1232, -0.0709]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0128, -0.0814,  0.0743,  ..., -0.1438,  0.1097,  0.2474],\n",
       "          [-0.0067, -0.1325, -0.0994,  ...,  0.0105,  0.2444,  0.0653],\n",
       "          [ 0.0061, -0.1433, -0.1862,  ...,  0.1060,  0.2115,  0.0485],\n",
       "          [ 0.0473, -0.2279, -0.1362,  ...,  0.0993,  0.2193,  0.0385]],\n",
       "\n",
       "         [[ 0.1114, -0.0593,  0.0483,  ...,  0.1198, -0.1012,  0.0369],\n",
       "          [ 0.1036, -0.1301,  0.0320,  ...,  0.1248, -0.3731,  0.1041],\n",
       "          [ 0.0667, -0.0803, -0.0465,  ...,  0.1633, -0.3829,  0.0457],\n",
       "          [ 0.0168, -0.0926, -0.0395,  ...,  0.2099, -0.3908,  0.0539]],\n",
       "\n",
       "         [[-0.1008,  0.1912, -0.1208,  ..., -0.1510, -0.0163,  0.1997],\n",
       "          [-0.0763,  0.3578, -0.0274,  ..., -0.1421, -0.0202,  0.1308],\n",
       "          [-0.1203,  0.3395, -0.0669,  ..., -0.0489, -0.0336,  0.1311],\n",
       "          [-0.1139,  0.3661, -0.0533,  ..., -0.0021, -0.0820,  0.1239]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 0.3550,  0.0937, -0.3769,  ...,  0.1464, -0.1714,  0.4501],\n",
       "          [ 0.2672, -0.0866,  0.1736,  ...,  0.3113, -0.3664, -0.0303],\n",
       "          [ 0.1937,  0.0711, -0.3956,  ...,  0.0895, -0.4504,  0.1764],\n",
       "          ...,\n",
       "          [-0.2676, -0.0214, -0.3461,  ...,  0.0036, -0.0404,  0.2952],\n",
       "          [ 0.2727,  0.1226, -0.3091,  ...,  0.0991, -0.0803,  0.5678],\n",
       "          [ 0.3546,  0.0966, -0.3771,  ...,  0.1453, -0.1720,  0.4509]],\n",
       "\n",
       "         [[-0.0345, -0.0858, -0.5659,  ...,  0.1246, -0.3194, -0.3105],\n",
       "          [-0.0746, -0.2093, -0.6664,  ...,  0.1398, -0.2255, -0.1978],\n",
       "          [ 0.1599, -0.3311, -0.6134,  ...,  0.3754, -0.4302, -0.0323],\n",
       "          ...,\n",
       "          [ 0.1384,  0.2254, -0.2887,  ...,  0.4568, -0.3581, -0.2675],\n",
       "          [ 0.1350, -0.0167, -0.4831,  ...,  0.2484, -0.2972, -0.5043],\n",
       "          [-0.0373, -0.0850, -0.5662,  ...,  0.1263, -0.3166, -0.3135]],\n",
       "\n",
       "         [[ 0.0801,  0.4241,  0.0932,  ..., -0.1142,  0.2806, -0.1877],\n",
       "          [-0.0455,  0.4709, -0.1054,  ...,  0.2737, -0.0938, -0.0518],\n",
       "          [ 0.0551,  0.3168,  0.0540,  ..., -0.1971,  0.5638,  0.1970],\n",
       "          ...,\n",
       "          [ 0.2273,  0.1147,  0.3138,  ..., -0.1750,  0.3816, -0.0974],\n",
       "          [-0.1635,  0.1999,  0.2470,  ...,  0.2277,  0.3356, -0.0237],\n",
       "          [ 0.0821,  0.4231,  0.0923,  ..., -0.1162,  0.2809, -0.1875]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0588, -0.0107,  0.2075,  ..., -0.1192,  0.0107, -0.2304],\n",
       "          [ 0.3450, -0.2984,  0.3448,  ..., -0.0983, -0.0730, -0.3539],\n",
       "          [ 0.2483, -0.1598,  0.4659,  ...,  0.1416, -0.0100, -0.2023],\n",
       "          ...,\n",
       "          [-0.1907, -0.2651, -0.1784,  ...,  0.0612,  0.1440, -0.3871],\n",
       "          [ 0.1636, -0.1810,  0.2937,  ..., -0.0319, -0.1201, -0.0874],\n",
       "          [ 0.0567, -0.0111,  0.2076,  ..., -0.1158,  0.0103, -0.2310]],\n",
       "\n",
       "         [[ 0.3446, -0.1355, -0.0746,  ..., -0.0494, -0.2537, -0.1664],\n",
       "          [ 0.3258,  0.0757,  0.1252,  ..., -0.0267, -0.2660, -0.0720],\n",
       "          [ 0.0740, -0.1686, -0.2643,  ..., -0.1942, -0.1427, -0.5634],\n",
       "          ...,\n",
       "          [ 0.0063, -0.2528, -0.0741,  ...,  0.0490, -0.2816,  0.1782],\n",
       "          [ 0.2465, -0.1355,  0.0994,  ...,  0.1179, -0.4956, -0.1211],\n",
       "          [ 0.3457, -0.1332, -0.0734,  ..., -0.0506, -0.2535, -0.1644]],\n",
       "\n",
       "         [[ 0.0818, -0.1293,  0.3032,  ...,  0.1974,  0.3706, -0.2177],\n",
       "          [ 0.0225, -0.0537,  0.0867,  ...,  0.2386, -0.0656, -0.0666],\n",
       "          [-0.1234, -0.0316,  0.0068,  ...,  0.0382,  0.1129, -0.4870],\n",
       "          ...,\n",
       "          [-0.0059, -0.1279,  0.5889,  ...,  0.0896,  0.1095,  0.0501],\n",
       "          [-0.0033, -0.0450,  0.3852,  ..., -0.0055,  0.2162,  0.2093],\n",
       "          [ 0.0813, -0.1301,  0.3051,  ...,  0.1976,  0.3727, -0.2185]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>), tensor([[[[ 0.2059, -0.3377, -0.1028,  ...,  0.3104,  0.3473,  0.0117],\n",
       "          [ 0.0577, -0.3694, -0.0309,  ...,  0.4778,  0.6014,  0.1893],\n",
       "          [ 0.0881, -0.4990, -0.1746,  ..., -0.0397,  0.3522,  0.0437],\n",
       "          ...,\n",
       "          [-0.0768, -0.1850, -0.0160,  ...,  0.4328,  0.2432, -0.1250],\n",
       "          [ 0.3264, -0.2896, -0.1105,  ...,  0.3101,  0.3495,  0.1887],\n",
       "          [ 0.2076, -0.3360, -0.1010,  ...,  0.3092,  0.3472,  0.0088]],\n",
       "\n",
       "         [[ 0.4719, -0.0714, -0.3389,  ..., -0.2209, -0.2324, -0.3723],\n",
       "          [ 0.0634, -0.0020, -0.5692,  ..., -0.3058, -0.2460, -0.3515],\n",
       "          [ 0.7496,  0.0252, -0.5796,  ..., -0.5440,  0.0095,  0.2762],\n",
       "          ...,\n",
       "          [ 0.6183, -0.5157, -0.3027,  ..., -0.0530,  0.2014, -0.1682],\n",
       "          [ 0.6566, -0.1688, -0.5101,  ..., -0.0983, -0.3005, -0.4153],\n",
       "          [ 0.4711, -0.0695, -0.3355,  ..., -0.2213, -0.2345, -0.3711]],\n",
       "\n",
       "         [[-0.0617, -0.0691, -0.0780,  ...,  0.1051,  0.1096,  0.0393],\n",
       "          [-0.2162, -0.2238, -0.2046,  ...,  0.1939, -0.2153,  0.1720],\n",
       "          [ 0.2339, -0.1330,  0.1749,  ...,  0.3516, -0.2451,  0.3433],\n",
       "          ...,\n",
       "          [-0.0934, -0.0559, -0.0087,  ...,  0.3745,  0.0739, -0.0727],\n",
       "          [-0.1220, -0.1707, -0.0642,  ...,  0.2387,  0.2168,  0.3319],\n",
       "          [-0.0623, -0.0673, -0.0778,  ...,  0.1066,  0.1106,  0.0398]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0366, -0.2707, -0.0337,  ...,  0.0940, -0.1776,  0.0493],\n",
       "          [-0.0173, -0.1194,  0.0935,  ...,  0.1433, -0.2319,  0.2357],\n",
       "          [ 0.0976, -0.3243,  0.2231,  ...,  0.3625, -0.3675,  0.1449],\n",
       "          ...,\n",
       "          [ 0.0538, -0.3662,  0.1953,  ...,  0.3959,  0.2360,  0.2273],\n",
       "          [-0.2880, -0.3247, -0.0836,  ...,  0.3010,  0.0843, -0.0278],\n",
       "          [-0.0347, -0.2696, -0.0344,  ...,  0.0944, -0.1761,  0.0486]],\n",
       "\n",
       "         [[ 0.1736, -0.1392,  0.1581,  ..., -0.0663,  0.0456,  0.0610],\n",
       "          [ 0.2923,  0.2522, -0.2043,  ..., -0.1357,  0.1702,  0.2548],\n",
       "          [-0.0387, -0.0931,  0.1554,  ..., -0.1923, -0.2056,  0.2720],\n",
       "          ...,\n",
       "          [ 0.2523, -0.1323,  0.1722,  ..., -0.0973, -0.2244,  0.2892],\n",
       "          [-0.0130, -0.2019, -0.2865,  ...,  0.0370, -0.1499,  0.0489],\n",
       "          [ 0.1734, -0.1395,  0.1559,  ..., -0.0672,  0.0440,  0.0580]],\n",
       "\n",
       "         [[ 0.2867,  0.3324,  0.0037,  ...,  0.1281,  0.0917,  0.1166],\n",
       "          [ 0.3294, -0.0182, -0.3330,  ...,  0.1945,  0.1801,  0.1627],\n",
       "          [-0.2143,  0.0154,  0.1102,  ...,  0.0867, -0.2408,  0.1945],\n",
       "          ...,\n",
       "          [-0.0759,  0.4212,  0.0084,  ...,  0.0837, -0.1079,  0.0132],\n",
       "          [ 0.2665,  0.2321, -0.2631,  ..., -0.0685, -0.1124,  0.5014],\n",
       "          [ 0.2861,  0.3306,  0.0027,  ...,  0.1267,  0.0913,  0.1150]]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.1148,  0.3781, -0.0583,  ...,  0.0342, -0.3667,  0.1216],\n",
       "         [ 0.3092,  0.3377,  0.6219,  ...,  0.0505, -0.4492,  0.0503],\n",
       "         [ 0.5204,  0.5981, -0.5640,  ...,  0.1196, -0.1687, -0.0257],\n",
       "         ...,\n",
       "         [ 0.1314,  0.5522,  0.2636,  ..., -0.7403,  0.5468,  0.3223],\n",
       "         [ 0.0352,  0.3708, -0.1617,  ..., -0.1439,  0.1771,  0.1838],\n",
       "         [ 0.1180,  0.3753, -0.0589,  ...,  0.0330, -0.3659,  0.1240]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "981f108a204f421f158e0977940335d851edffa6dd3586828a3e1aec045160e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('final': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
