{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from functools import partial\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    HfArgumentParser,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sys.path.append('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from dataloader import SumDataset\n",
    "from processor import preprocess_function\n",
    "from rouge import compute_metrics\n",
    "from data_collator import DataCollatorForSeq2SeqWithDocType"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "types = ['magazine']\n",
    "dataset_name = ['metamong1/summarization_' + dt for dt in types]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_dataset = SumDataset(\n",
    "    dataset_name,\n",
    "    'train',\n",
    "    shuffle_seed=1234,\n",
    "    ratio=0.01,\n",
    "    USE_AUTH_TOKEN=True\n",
    ").load_data()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset magazine_summarization (/opt/ml/.cache/huggingface/datasets/metamong1___magazine_summarization/Magizine Summarization/1.0.0/506cb41eb0b96b084eafa5dd5fe3b51ff0d1061256700adf1aa92d3b19762c36)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0ccc6ac48c54b70a411ab6638c81ba9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached shuffled indices for dataset at /opt/ml/.cache/huggingface/datasets/metamong1___magazine_summarization/Magizine Summarization/1.0.0/506cb41eb0b96b084eafa5dd5fe3b51ff0d1061256700adf1aa92d3b19762c36/cache-ec04b3f04ce8063a.arrow\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doc_id', 'title', 'text', 'doc_type', 'file'],\n",
       "    num_rows: 527\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model_checkpoint = 'gogamza/kobart-base-v1'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    use_fast=True,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/envs/final/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokenizer.all_special_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 5, 3, 6]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "for sep_id in tokenizer.all_special_ids :\n",
    "    sep_token = tokenizer.convert_ids_to_tokens(sep_id)\n",
    "    print(f'Special Id : {sep_id} \\t Special Token : {sep_token}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Special Id : 0 \t Special Token : <s>\n",
      "Special Id : 1 \t Special Token : </s>\n",
      "Special Id : 5 \t Special Token : <unk>\n",
      "Special Id : 3 \t Special Token : <pad>\n",
      "Special Id : 6 \t Special Token : <mask>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arguments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from args import DataTrainingArguments"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "data_args = DataTrainingArguments()\n",
    "data_args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataTrainingArguments(dataset_name='paper,news', text_column='text', summary_column='title', overwrite_cache=False, preprocessing_num_workers=4, max_source_length=1024, max_target_length=128, val_max_target_length=None, pad_to_max_length=False, max_train_samples=None, max_eval_samples=None, max_predict_samples=None, num_beams=None, ignore_pad_token_for_loss=True, use_auth_token_path='./use_auth_token.env', relative_sample_ratio=1.0, relative_eval_steps=10)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def preprocess_function(examples, tokenizer, data_args):\n",
    "    bos_token = tokenizer.bos_token\n",
    "    eos_token = tokenizer.eos_token\n",
    "    padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "    inputs = examples['text']\n",
    "    inputs = [bos_token + inp + eos_token for inp in inputs]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
    "    return model_inputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "prep_fn  = partial(preprocess_function, tokenizer=tokenizer, data_args=data_args)\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    prep_fn,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8da872c733a84bd7aca77805b443781f"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "tokenized_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'token_type_ids'],\n",
       "    num_rows: 527\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "    mlm=0.15)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "data_loader = DataLoader(tokenized_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for data in data_loader :\n",
    "    break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "data.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['attention_mask', 'input_ids', 'token_type_ids', 'labels'])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "for key in data.keys() :\n",
    "    print(f'Key : {key} \\t Tensor Shape : {data[key].shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Key : attention_mask \t Tensor Shape : torch.Size([16, 785])\n",
      "Key : input_ids \t Tensor Shape : torch.Size([16, 785])\n",
      "Key : token_type_ids \t Tensor Shape : torch.Size([16, 785])\n",
      "Key : labels \t Tensor Shape : torch.Size([16, 785])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "data['input_ids'][0][:20]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([    0, 14287, 15832, 27607, 17881,     6, 13417, 14501,  9866,  9229,\n",
       "        17908,     6, 17510,   299, 25873, 19632, 10213, 19553, 15533,     6])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "data['labels'][0][:20]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ -100,  -100,  -100, 27607,  -100, 14063,  -100,  -100,  -100,  -100,\n",
       "         -100, 21738,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 14501])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "  1. _torch_collate_batch 를 활용해서 Batch 안에서 Padding을 하고 Tensor로 만든다.\n",
    "  2. tokenizer.get_special_tokens_mask를 활용해서 각 데이터 마다 special token의 위치에 Mask를 취한다.\n",
    "  3. 그 외에 자리에서 torch.bernoulli를 활용해서 Mask이 될 토큰을 선정한다.\n",
    "      * 80% Masking 처리\n",
    "      * 10% Random으로 위치 변경\n",
    "      * 10% 그대로\n",
    "  4. tokenizer에 sepcial token의 위치 이외에서 Masking을 진행한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Infilling\n",
    "  방향\n",
    "  1. 푸아송 분포에 따라서 값을 추출한다.\n",
    "  2. 1번 값의 길이에 해당하는 text span을 선정해서 MASK Token으로 변형한다. \n",
    "      * 값이 0인 경우에는 MASK Token을 추가한다. \n",
    "  3. 그렇게 변형된 입력이 들어오면 모델을 통해서 원래의 입력을 맞춰야 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collator 구현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Libarary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import random\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from transformers.models.bert import BertTokenizer, BertTokenizerFast\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "from transformers.data.data_collator import (\n",
    "        DataCollatorForLanguageModeling,\n",
    "        _torch_collate_batch\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "poisson_nums = np.random.poisson(3, 10)\n",
    "print(poisson_nums)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4 5 4 1 4 3 3 5 3 4]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "\n",
    "@dataclass\n",
    "class DataCollatorForBartPretraining(DataCollatorForLanguageModeling):\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    poisson: int = 3\n",
    "    label_pad_token_id: int = -100\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], (dict, BatchEncoding)):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            ## input_ids를 배치안에서 최대 길이로 padding을 진행한다.\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        batch[\"input_ids\"], batch[\"labels\"] = self.torch_infilling(\n",
    "            batch[\"input_ids\"], poisson_num=self.poisson\n",
    "        )\n",
    "\n",
    "        # label에서 pad_token_id로 된 부분은 loss 계산에서 제외하기 위해서 label_pad_token_id로 변경한다.\n",
    "        batch[\"labels\"] = torch.where(batch[\"labels\"] == tokenizer.pad_token_id, \n",
    "            self.label_pad_token_id, batch[\"labels\"])\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def torch_infilling(self, inputs: Any, poisson_num: Optional[int] = None) -> Tuple[Any, Any]:\n",
    "        import torch\n",
    "        labels = inputs.clone() # label은 기존 문장과 동일\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        mask_token_id = tokenizer.mask_token_id\n",
    "        batch_size, seq_size = inputs.shape\n",
    "\n",
    "        poisson_value_list = np.random.poisson(poisson_num, batch_size) # batch size에 맞게 possion 값 생성\n",
    "        input_list = []\n",
    "        max_size = 0\n",
    "        for i, poisson in enumerate(poisson_value_list) :\n",
    "            input_arr = list(inputs[i])\n",
    "            poisson = poisson_value_list[i] # 해당 문장에 대한 poisson 값\n",
    "            pad_ids = np.where(np.array(input_arr) == pad_token_id)[0] # pad의 첫번째 부분 파악\n",
    "            pad_size = pad_ids[0] if len(pad_ids) > 0 else 0 \n",
    "\n",
    "            sen_size = seq_size - pad_size # sequence size - padding size : 기존의 input size\n",
    "            infilling_start = np.random.randint(sen_size-poisson) # poisson value의 길이에 맞는 span의 시작점 선정\n",
    "            infilling_end = infilling_start + poisson\n",
    "\n",
    "            input_arr = input_arr[:infilling_start] + [mask_token_id] + input_arr[infilling_end:] # poisson value에 해당되는 길이의 span 자체를 mask 처리\n",
    "            max_size = max(max_size, len(input_arr)) # 문장의 길이 파악\n",
    "            input_list.append(input_arr)\n",
    "\n",
    "        input_infilling = []\n",
    "        # 각각의 문장의 길이가 다르기 때문에 Batch 중에서 가장 긴 길이에 맞춰서 padding을 진행한다.\n",
    "        for input_ids in input_list :\n",
    "            if self.tokenizer.padding_side == 'right' :\n",
    "                input_ids = input_ids + [pad_token_id] * (max_size - len(input_ids))\n",
    "            else :\n",
    "                input_ids = [pad_token_id] * (max_size - len(input_ids)) + input_ids\n",
    "            input_infilling.append(torch.tensor(input_ids))\n",
    "            \n",
    "        input_infilling = torch.stack(input_infilling, dim=0)\n",
    "        return input_infilling, labels\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "data_collator = DataCollatorForBartPretraining(tokenizer=tokenizer,\n",
    "    poisson=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "data_loader = DataLoader(tokenized_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "for data in data_loader :\n",
    "    break\n",
    "\n",
    "data.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['attention_mask', 'input_ids', 'token_type_ids', 'labels'])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "for key in data.keys() :\n",
    "    print(f'Key : {key} \\t Tensor Shape : {data[key].shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Key : attention_mask \t Tensor Shape : torch.Size([16, 785])\n",
      "Key : input_ids \t Tensor Shape : torch.Size([16, 786])\n",
      "Key : token_type_ids \t Tensor Shape : torch.Size([16, 785])\n",
      "Key : labels \t Tensor Shape : torch.Size([16, 785])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Case 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "list(data['input_ids'][0]).index(6)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "print('After Infilling')\n",
    "print(tokenizer.decode(data['input_ids'][0][160:180]))\n",
    "print('\\nBefore Infilling')\n",
    "print(tokenizer.decode(data['labels'][0][160:180]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After Infilling\n",
      "직면하자 애완견을 방패막이로 삼았다. \"개인적으로 받은 것은<mask>반려견\n",
      "\n",
      "Before Infilling\n",
      "직면하자 애완견을 방패막이로 삼았다. \"개인적으로 받은 것은 체커스(반\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Case2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "list(data['input_ids'][11]).index(6)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "print('After Infilling')\n",
    "print(tokenizer.decode(data['input_ids'][11][150:180]))\n",
    "print('\\nBefore Infilling')\n",
    "print(tokenizer.decode(data['labels'][11][150:180]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After Infilling\n",
      "제는 각 지방자치단체가 도시공원 등 도시<mask> 녹지를 20 년 이상 개발하지 않으면 공원지정 효력을 자동 해제해야 하는 제도다. 지금까지 지자\n",
      "\n",
      "Before Infilling\n",
      "제는 각 지방자치단체가 도시공원 등 도시계획시설로 정한 녹지를 20 년 이상 개발하지 않으면 공원지정 효력을 자동 해제해야 하는 제도\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Case 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "list(data['input_ids'][5]).index(6)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "print('After Infilling')\n",
    "print(tokenizer.decode(data['input_ids'][5][150:180]))\n",
    "print('\\nBefore Infilling')\n",
    "print(tokenizer.decode(data['labels'][5][150:180]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After Infilling\n",
      "주장에 대해 7월 9일 페이스북을 통해 \"G20이<mask> 새벽 1시 반이 되어서야 숙소로 돌아올 수 있었습니다. 함께 동행한 청와대\n",
      "\n",
      "Before Infilling\n",
      "주장에 대해 7월 9일 페이스북을 통해 \"G20이 있던 첫째 날 대통령은 새벽 1시 반이 되어서야 숙소로 돌아올 수 있었습니다. 함께\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('final': conda)"
  },
  "interpreter": {
   "hash": "4d949d837314f0bd127b310a251a9c5e144fbaeaec32e29e415d9adebd0cb2d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}